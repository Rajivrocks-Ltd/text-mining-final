{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-24T16:57:04.145061Z",
     "start_time": "2024-12-24T16:57:04.140741Z"
    }
   },
   "source": [
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, RobertaTokenizerFast, get_scheduler\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import Dataset, DatasetDict, Features, Sequence, Value, ClassLabel\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "3d65dfb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:33.977678Z",
     "start_time": "2024-12-24T16:56:33.974230Z"
    }
   },
   "source": [
    "# Reduce VRAM usage by reducing fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "6fcf0ba874e901ac",
   "metadata": {},
   "source": [
    "## Combine all .txt and .ann files and combine them per medicine"
   ]
  },
  {
   "cell_type": "code",
   "id": "aa71104cf543fea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:35.816817Z",
     "start_time": "2024-12-24T16:56:33.983244Z"
    }
   },
   "source": [
    "# Base folders containing annotation and text files\n",
    "annotations_folder = 'annotations/'\n",
    "original_texts_folder = 'originaltexts/'\n",
    "output_folder = 'output_datasets/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Group files by medicine\n",
    "file_groups = {}\n",
    "for file_name in os.listdir(annotations_folder):\n",
    "    if file_name.endswith('.ann'):\n",
    "        base_name = '.'.join(file_name.split('.')[:-1])\n",
    "        medicine = base_name.rsplit('.', 1)[0]\n",
    "        file_groups.setdefault(medicine, []).append(file_name)\n",
    "\n",
    "# Process each group\n",
    "for medicine, ann_files in file_groups.items():\n",
    "    combined_output = []\n",
    "\n",
    "    for ann_file in ann_files:\n",
    "        txt_file = ann_file.replace('.ann', '.txt')\n",
    "        txt_path = os.path.join(original_texts_folder, txt_file)\n",
    "        ann_path = os.path.join(annotations_folder, ann_file)\n",
    "\n",
    "        # Ensure the corresponding .txt file exists\n",
    "        if not os.path.exists(txt_path):\n",
    "            raise FileNotFoundError(f\"Text file not found for annotation file {ann_file}\")\n",
    "\n",
    "        # Read the content of the .ann and .txt files\n",
    "        with open(ann_path, 'r') as ann_f:\n",
    "            ann_lines = ann_f.readlines()\n",
    "\n",
    "        with open(txt_path, 'r') as txt_f:\n",
    "            txt_content = txt_f.read()\n",
    "\n",
    "        # Parse annotations and filter out AnnotatorNotes\n",
    "        annotations = []\n",
    "        for line in ann_lines:\n",
    "            if line.startswith('T'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 3:\n",
    "                    tag_info, word = parts[1], parts[2]\n",
    "                    tag_parts = tag_info.split()\n",
    "                    if len(tag_parts) >= 3:\n",
    "                        tag = tag_parts[0]\n",
    "                        try:\n",
    "                            start_idx = int(tag_parts[1])\n",
    "                            end_idx = int(tag_parts[2])\n",
    "                        except ValueError:\n",
    "                            if ';' in tag_parts[2]:  # Handle ranges like '742;763'\n",
    "                                start_idx = int(tag_parts[1])\n",
    "                                end_idx = int(tag_parts[2].split(';')[-1])\n",
    "                            else:\n",
    "                                raise ValueError(f\"Unexpected annotation format: {tag_parts}\")\n",
    "                        annotations.append((start_idx, end_idx, tag, word))\n",
    "\n",
    "        # Sort annotations by start index\n",
    "        annotations.sort(key=lambda x: x[0])\n",
    "\n",
    "        # Generate output format\n",
    "        output = []\n",
    "        current_idx = 0\n",
    "        for start_idx, end_idx, tag, word in annotations:\n",
    "            # Add text between the last annotation and the current annotation as \"O\"\n",
    "            if current_idx < start_idx:\n",
    "                intervening_text = txt_content[current_idx:start_idx]\n",
    "                for token in re.findall(r\"\\w+(?:'\\w+)?|[.,!?]\", intervening_text):\n",
    "                    output.append(f\"{token} O\")\n",
    "\n",
    "            # Add the annotated word with its tag\n",
    "            for i, token in enumerate(word.split()):\n",
    "                tag_prefix = 'B-' if i == 0 else 'I-'\n",
    "                output.append(f\"{token} {tag_prefix}{tag}\")\n",
    "\n",
    "            current_idx = end_idx\n",
    "\n",
    "        # Add remaining text as \"O\"\n",
    "        if current_idx < len(txt_content):\n",
    "            remaining_text = txt_content[current_idx:]\n",
    "            for token in re.findall(r\"\\w+(?:'\\w+)?|[.,!?]\", remaining_text):\n",
    "                output.append(f\"{token} O\")\n",
    "\n",
    "        # Add to combined output with a newline separator\n",
    "        combined_output.extend(output)\n",
    "        combined_output.append('')  # Empty line between posts\n",
    "\n",
    "    # Write combined output to file\n",
    "    combined_output_text = '\\n'.join(combined_output).strip()\n",
    "    output_file = os.path.join(output_folder, f\"{medicine}_combined_output.txt\")\n",
    "    with open(output_file, 'w') as out_f:\n",
    "        out_f.write(combined_output_text)\n",
    "\n",
    "    print(f\"Combined output saved for {medicine} in {output_file}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined output saved for ARTHROTEC in output_datasets/ARTHROTEC_combined_output.txt\n",
      "Combined output saved for CAMBIA in output_datasets/CAMBIA_combined_output.txt\n",
      "Combined output saved for CATAFLAM in output_datasets/CATAFLAM_combined_output.txt\n",
      "Combined output saved for DICLOFENAC-POTASSIUM in output_datasets/DICLOFENAC-POTASSIUM_combined_output.txt\n",
      "Combined output saved for DICLOFENAC-SODIUM in output_datasets/DICLOFENAC-SODIUM_combined_output.txt\n",
      "Combined output saved for FLECTOR in output_datasets/FLECTOR_combined_output.txt\n",
      "Combined output saved for LIPITOR in output_datasets/LIPITOR_combined_output.txt\n",
      "Combined output saved for PENNSAID in output_datasets/PENNSAID_combined_output.txt\n",
      "Combined output saved for SOLARAZE in output_datasets/SOLARAZE_combined_output.txt\n",
      "Combined output saved for VOLTAREN-XR in output_datasets/VOLTAREN-XR_combined_output.txt\n",
      "Combined output saved for VOLTAREN in output_datasets/VOLTAREN_combined_output.txt\n",
      "Combined output saved for ZIPSOR in output_datasets/ZIPSOR_combined_output.txt\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "806f757861e7d38d",
   "metadata": {},
   "source": [
    "## Combine all the medicine files into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "f940e15d9430fc7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:35.980494Z",
     "start_time": "2024-12-24T16:56:35.963990Z"
    }
   },
   "source": [
    "# Folder containing all combined output files\n",
    "output_datasets_folder = 'output_datasets/'\n",
    "final_output_file = 'final_dataset.txt'\n",
    "\n",
    "# Ensure the folder exists\n",
    "if not os.path.exists(output_datasets_folder):\n",
    "    raise FileNotFoundError(f\"The folder {output_datasets_folder} does not exist.\")\n",
    "\n",
    "# List all files in the folder\n",
    "output_files = [f for f in os.listdir(output_datasets_folder) if f.endswith('_combined_output.txt')]\n",
    "\n",
    "# Combine all files into a single final dataset\n",
    "final_dataset = []\n",
    "for file_name in output_files:\n",
    "    file_path = os.path.join(output_datasets_folder, file_name)\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read().strip()  # Read and strip any trailing spaces or newlines\n",
    "        final_dataset.append(content)\n",
    "\n",
    "    # Add an empty line to separate posts from different files\n",
    "    final_dataset.append('')\n",
    "\n",
    "# Write the combined dataset to the final output file\n",
    "with open(final_output_file, 'w') as f:\n",
    "    f.write('\\n'.join(final_dataset).strip())  # Ensure no extra trailing newline\n",
    "\n",
    "print(f\"Final dataset saved to {final_output_file}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset saved to final_dataset.txt\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "3269e1b5930cd4c4",
   "metadata": {},
   "source": [
    "## Read the final dataset into the Iob dataset format"
   ]
  },
  {
   "cell_type": "code",
   "id": "435d59ec1801f10d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.014412Z",
     "start_time": "2024-12-24T16:56:36.007621Z"
    }
   },
   "source": [
    "def read_iob_file(file_path):\n",
    "    \"\"\"Reads an IOB file from filepath and returns sentences with tokens and tags.\"\"\"\n",
    "    sentences = []\n",
    "    sentence_tokens = []\n",
    "    sentence_labels = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # If line is not empty\n",
    "                token, tag = line.split()\n",
    "                sentence_tokens.append(token)\n",
    "                sentence_labels.append(tag)\n",
    "\n",
    "            else:\n",
    "                # End of a sentence\n",
    "                if sentence_tokens:\n",
    "                    sentences.append({\"tokens\": sentence_tokens, \"ner_tags\": sentence_labels})\n",
    "                    sentence_tokens = []\n",
    "                    sentence_labels = []\n",
    "        # Add the last sentence if file doesn't end with a newline\n",
    "        if sentence_tokens:\n",
    "            sentences.append({\"tokens\": sentence_tokens, \"ner_tags\": sentence_labels})\n",
    "    return sentences\n",
    "\n",
    "def create_dataset_from_final_file(final_file_path):\n",
    "    \"\"\"Create a dataset from a single IOB file and return it as a DatasetDict.\"\"\"\n",
    "\n",
    "    if not os.path.exists(final_file_path):\n",
    "        raise FileNotFoundError(f\"The file {final_file_path} does not exist.\")\n",
    "\n",
    "    # Parse the file\n",
    "    data = read_iob_file(final_file_path)\n",
    "\n",
    "    # Define the label names and ClassLabel feature\n",
    "    unique_labels = sorted(set(tag for d in data for tag in d[\"ner_tags\"]))\n",
    "    label_feature = ClassLabel(names=unique_labels)\n",
    "\n",
    "    # Define the Features schema for Hugging Face datasets\n",
    "    features = Features({\n",
    "        'tokens': Sequence(Value(\"string\")),\n",
    "        'ner_tags': Sequence(label_feature)\n",
    "    })\n",
    "\n",
    "    # Convert data into a Dataset\n",
    "    dataset = Dataset.from_list(data).cast(features)\n",
    "\n",
    "    # Create a DatasetDict\n",
    "    dataset_dict = DatasetDict({\"full_data\": dataset})\n",
    "\n",
    "    return dataset_dict\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "35b8e6d4d262ac49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.508707Z",
     "start_time": "2024-12-24T16:56:36.044426Z"
    }
   },
   "source": [
    "final_dataset_path = \"final_dataset.txt\"\n",
    "dataset_dict = create_dataset_from_final_file(final_dataset_path)\n",
    "dataset = dataset_dict['full_data']"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 1248/1248 [00:00<00:00, 6870.58 examples/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "a85774695bc51613",
   "metadata": {},
   "source": [
    "## Tokenize and align labels, also add datacollator"
   ]
  },
  {
   "cell_type": "code",
   "id": "5b8c9593f60bdb7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.519853Z",
     "start_time": "2024-12-24T16:56:36.516810Z"
    }
   },
   "source": [
    "# def align_labels_with_tokens(labels, word_ids):\n",
    "#     new_labels = []\n",
    "#     current_word = None\n",
    "#\n",
    "#     for word_id in word_ids:\n",
    "#         if word_id != current_word:\n",
    "#             current_word = word_id\n",
    "#             label = -100 if word_id is None else labels[word_id]\n",
    "#             new_labels.append(label)\n",
    "#\n",
    "#         elif word_id is None:\n",
    "#             # Special token\n",
    "#             new_labels.append(-100)\n",
    "#\n",
    "#         else:\n",
    "#             # Same word as previous token\n",
    "#             label = labels[word_id]\n",
    "#\n",
    "#             # If the label is B-XXX we change it to I-XXX\n",
    "#             if label % 2 == 1:\n",
    "#                 label += 1\n",
    "#             new_labels.append(label)\n",
    "#\n",
    "#     return new_labels\n",
    "#\n",
    "#\n",
    "# def tokenize_and_align_labels(examples):\n",
    "#     tokenized_inputs = tokenizer(\n",
    "#         examples[\"tokens\"], truncation=True,\n",
    "#         is_split_into_words=True\n",
    "#     )\n",
    "#     all_labels = examples[\"ner_tags\"]\n",
    "#     new_labels = []\n",
    "#     for i, labels in enumerate(all_labels):\n",
    "#         word_ids = tokenized_inputs.word_ids(i)\n",
    "#         new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "#\n",
    "#     tokenized_inputs[\"labels\"] = new_labels\n",
    "#     return tokenized_inputs"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "1798cebc48c3299c",
   "metadata": {},
   "source": [
    "## Dataset generators"
   ]
  },
  {
   "cell_type": "code",
   "id": "b4b79e14d8ca0efa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.543588Z",
     "start_time": "2024-12-24T16:56:36.539561Z"
    }
   },
   "source": [
    "def generate_train_datasets(dataset_, number_of_samples, number_of_splits):\n",
    "    \"\"\"\n",
    "    Generates train datasets by sampling from the given dataset based on the number of samples and splits.\n",
    "\n",
    "    Args:\n",
    "        dataset_ (Dataset): The base dataset to sample from.\n",
    "        number_of_samples (int): Number of samples per dataset.\n",
    "        number_of_splits (int): Number of datasets to generate (different seeds).\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, Dataset, List[int]]]: List of generated datasets with their names and indices.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "\n",
    "    for seed in range(number_of_splits):\n",
    "        # Set the random seed for reproducibility\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Shuffle and sample from the dataset\n",
    "        indices = list(range(len(dataset_)))\n",
    "        random.shuffle(indices)\n",
    "        sampled_indices = indices[:number_of_samples]\n",
    "\n",
    "        sampled_dataset = dataset_.select(sampled_indices)\n",
    "\n",
    "        # Add the dataset with its name and indices\n",
    "        datasets.append((f\"train_dataset_{number_of_samples}_{seed}\", sampled_dataset, sampled_indices))\n",
    "\n",
    "    return datasets"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "513adc9d9f4ee109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.560393Z",
     "start_time": "2024-12-24T16:56:36.555449Z"
    }
   },
   "source": [
    "def generate_validation_datasets(dataset_, train_indices, number_of_samples, number_of_splits):\n",
    "    \"\"\"\n",
    "    Generates validation datasets by sampling from the given dataset, ensuring no overlap with training data.\n",
    "\n",
    "    Args:\n",
    "        dataset_ (Dataset): The base dataset to sample from.\n",
    "        train_indices (List[int]): Indices of the training dataset to exclude from sampling.\n",
    "        number_of_samples (int): Number of samples per validation dataset.\n",
    "        number_of_splits (int): Number of validation datasets to generate (different seeds).\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, Dataset, List[int]]]: List of generated validation datasets with names and indices.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    all_indices = set(range(len(dataset_)))\n",
    "    available_indices = list(all_indices - set(train_indices))  # Exclude training indices\n",
    "\n",
    "    for seed in range(number_of_splits):\n",
    "        # Set the random seed for reproducibility\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Shuffle and sample from the remaining indices\n",
    "        random.shuffle(available_indices)\n",
    "        sampled_indices = available_indices[:int(number_of_samples / 5)]\n",
    "\n",
    "        sampled_dataset = dataset_.select(sampled_indices)\n",
    "\n",
    "        # Add the dataset with its name and indices\n",
    "        datasets.append((f\"val_dataset_{number_of_samples/5}_{seed}\", sampled_dataset, sampled_indices))\n",
    "\n",
    "    return datasets"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "58e5dae8d7e7bcf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.571276Z",
     "start_time": "2024-12-24T16:56:36.567043Z"
    }
   },
   "source": [
    "def generate_test_datasets(dataset_, train_indices, val_indices, number_of_samples, number_of_splits):\n",
    "    \"\"\"\n",
    "    Generates test datasets by sampling from the given dataset, ensuring no overlap with training or validation data.\n",
    "\n",
    "    Args:\n",
    "        dataset_ (Dataset): The base dataset to sample from.\n",
    "        train_indices (List[int]): Indices of the training dataset to exclude from sampling.\n",
    "        val_indices (List[int]): Indices of the validation dataset to exclude from sampling.\n",
    "        number_of_samples (int): Number of samples per test dataset.\n",
    "        number_of_splits (int): Number of test datasets to generate (different seeds).\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, Dataset]]: List of generated test datasets with names.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    all_indices = set(range(len(dataset_)))\n",
    "    available_indices = list(all_indices - set(train_indices) - set(val_indices))  # Exclude train and val indices\n",
    "\n",
    "    for seed in range(number_of_splits):\n",
    "        sampled_indices = available_indices[:]\n",
    "\n",
    "        sampled_dataset = dataset_.select(sampled_indices)\n",
    "\n",
    "        # Add the dataset with its name\n",
    "        datasets.append((f\"test_dataset_{number_of_samples}_{seed}\", sampled_dataset))\n",
    "\n",
    "    return datasets"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "87bfb3346f6372cc",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "id": "6f8557b1a1dc6dd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.581336Z",
     "start_time": "2024-12-24T16:56:36.578214Z"
    }
   },
   "source": [
    "# Step 1: Generate Train Dataset\n",
    "# train_datasets = generate_train_datasets(dataset, number_of_samples=30, number_of_splits=1)\n",
    "# train_name, train_dataset, train_indices = train_datasets[0]\n",
    "# print(f\"{train_name}: {len(train_dataset)} samples\")\n",
    "#\n",
    "# # Step 2: Generate Validation Dataset\n",
    "# val_datasets = generate_validation_datasets(dataset, train_indices, number_of_samples=30, number_of_splits=1)\n",
    "# val_name, val_dataset, val_indices = val_datasets[0]\n",
    "# print(f\"{val_name}: {len(val_dataset)} samples\")\n",
    "#\n",
    "# # Step 3: Generate Test Dataset\n",
    "# test_datasets = generate_test_datasets(dataset, train_indices, val_indices, number_of_samples=30, number_of_splits=1)\n",
    "# test_name, test_dataset = test_datasets[0]\n",
    "# print(f\"{test_name}: {len(test_dataset)} samples\")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "b7084bbcd75432d9",
   "metadata": {},
   "source": [
    "# Prepping for training"
   ]
  },
  {
   "cell_type": "code",
   "id": "bc367cf3648c3146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.598393Z",
     "start_time": "2024-12-24T16:56:36.595331Z"
    }
   },
   "source": [
    "label_names = dataset.features[\"ner_tags\"].feature.names\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "1f56f43a627b4076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.615406Z",
     "start_time": "2024-12-24T16:56:36.610884Z"
    }
   },
   "source": [
    "def create_dataset_given_model(tokenizer_):\n",
    "    data_collator_ = DataCollatorForTokenClassification(tokenizer=tokenizer_)\n",
    "\n",
    "    # Tokenize and align labels\n",
    "    tokenized_dataset_ = dataset.map(tokenize_and_align_labels, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    # Step 1: Generate Train Dataset\n",
    "    train_datasets = generate_train_datasets(tokenized_dataset_, number_of_samples=100, number_of_splits=1)\n",
    "    train_name, train_dataset, train_indices = train_datasets[0]\n",
    "    print(f\"{train_name}: {len(train_dataset)} samples\")\n",
    "\n",
    "    # Step 2: Generate Validation Dataset\n",
    "    val_datasets = generate_validation_datasets(tokenized_dataset_, train_indices, number_of_samples=100, number_of_splits=1)\n",
    "    val_name, val_dataset, val_indices = val_datasets[0]\n",
    "    print(f\"{val_name}: {len(val_dataset)} samples\")\n",
    "\n",
    "    # Step 3: Generate Test Dataset\n",
    "    test_datasets = generate_test_datasets(tokenized_dataset_, train_indices, val_indices, number_of_samples=100, number_of_splits=1)\n",
    "    test_name, test_dataset = test_datasets[0]\n",
    "    print(f\"{test_name}: {len(test_dataset)} samples\")\n",
    "\n",
    "    return data_collator_, train_dataset, val_dataset, test_dataset\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "2a875af61f42f00a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.626889Z",
     "start_time": "2024-12-24T16:56:36.622730Z"
    }
   },
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Flatten predictions and labels, removing ignored indices\n",
    "    true_labels = [label for label_seq in labels for label in label_seq if label != -100]\n",
    "    true_predictions = [pred for pred_seq, label_seq in zip(predictions, labels)\n",
    "                        for pred, label in zip(pred_seq, label_seq) if label != -100]\n",
    "    return true_labels, true_predictions\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "e30995779e545bcf",
   "metadata": {},
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "fa33f20959c42e46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.655480Z",
     "start_time": "2024-12-24T16:56:36.640507Z"
    }
   },
   "source": [
    "def iterate_and_finetune_with_torch(\n",
    "    dataset,\n",
    "    file_name,\n",
    "    models,\n",
    "    start_size=5,\n",
    "    end_size=500,\n",
    "    step_size=5,\n",
    "    k_splits=5,\n",
    "    batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.0,\n",
    "    num_epochs=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune models with varying dataset sizes and k-fold splits, saving results to Excel.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (DatasetDict): Dataset for training, validation, and testing.\n",
    "    - file_name (str): Excel file to save results.\n",
    "    - models (dict): Dictionary of model names and their sizes.\n",
    "    - start_size (int): Starting size for training datasets.\n",
    "    - end_size (int): Maximum size for training datasets.\n",
    "    - step_size (int): Step size for increasing dataset sizes.\n",
    "    - k_splits (int): Number of k-fold splits.\n",
    "    - batch_size (int): Training batch size.\n",
    "    - learning_rate (float): Learning rate for fine-tuning.\n",
    "    - weight_decay (float): Weight decay for optimizer.\n",
    "    - num_epochs (int): Number of training epochs.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Check or create the results file\n",
    "    if os.path.exists(file_name):\n",
    "        results_df = pd.read_excel(file_name)\n",
    "    else:\n",
    "        results_df = pd.DataFrame(columns=[\"Train Size\", \"K-Fold\", \"Test F1\", \"Model\"])\n",
    "\n",
    "    for train_size in range(start_size, end_size + 1, step_size):\n",
    "        for split in range(k_splits):\n",
    "            for size, model_name in models.items():\n",
    "                print(f\"\\nFine-tuning {model_name} ({size}) with Train Size {train_size}, Split {split + 1}...\")\n",
    "\n",
    "                # Initialize tokenizer\n",
    "                if size == \"large\":\n",
    "                    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-large\", add_prefix_space=True)\n",
    "                else:\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "                data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "                # Generate datasets\n",
    "                train_datasets = generate_train_datasets(\n",
    "                    dataset, number_of_samples=train_size, number_of_splits=k_splits\n",
    "                )\n",
    "                _, train_dataset, train_indices = train_datasets[split]\n",
    "\n",
    "                val_datasets = generate_validation_datasets(\n",
    "                    dataset, train_indices=train_indices, number_of_samples=train_size, number_of_splits=k_splits\n",
    "                )\n",
    "                _, val_dataset, val_indices = val_datasets[split]\n",
    "\n",
    "                test_datasets = generate_test_datasets(\n",
    "                    dataset, train_indices=train_indices, val_indices=val_indices,\n",
    "                    number_of_samples=train_size, number_of_splits=k_splits\n",
    "                )\n",
    "                _, test_dataset = test_datasets[split]\n",
    "\n",
    "                def align_labels_with_tokens(labels_, word_ids):\n",
    "                    new_labels = []\n",
    "                    current_word = None\n",
    "\n",
    "                    for word_id in word_ids:\n",
    "                        if word_id != current_word:\n",
    "                            current_word = word_id\n",
    "                            label = -100 if word_id is None else labels_[word_id]\n",
    "                            new_labels.append(label)\n",
    "\n",
    "                        elif word_id is None:\n",
    "                            # Special token\n",
    "                            new_labels.append(-100)\n",
    "\n",
    "                        else:\n",
    "                            # Same word as previous token\n",
    "                            label = labels_[word_id]\n",
    "\n",
    "                            # If the label is B-XXX we change it to I-XXX\n",
    "                            if label % 2 == 1:\n",
    "                                label += 1\n",
    "                            new_labels.append(label)\n",
    "\n",
    "                    return new_labels\n",
    "\n",
    "\n",
    "                def tokenize_and_align_labels(examples):\n",
    "                    tokenized_inputs = tokenizer(\n",
    "                        examples[\"tokens\"], truncation=True,\n",
    "                        is_split_into_words=True\n",
    "                    )\n",
    "                    all_labels = examples[\"ner_tags\"]\n",
    "                    new_labels = []\n",
    "                    for i, labels_ in enumerate(all_labels):\n",
    "                        word_ids = tokenized_inputs.word_ids(i)\n",
    "                        new_labels.append(align_labels_with_tokens(labels_, word_ids))\n",
    "\n",
    "                    tokenized_inputs[\"labels\"] = new_labels\n",
    "                    return tokenized_inputs\n",
    "\n",
    "                # Tokenize datasets\n",
    "                tokenized_train = train_dataset.map(tokenize_and_align_labels, batched=True,  remove_columns=dataset.column_names)\n",
    "                tokenized_val = val_dataset.map(tokenize_and_align_labels, batched=True,  remove_columns=dataset.column_names)\n",
    "                tokenized_test = test_dataset.map(tokenize_and_align_labels, batched=True,  remove_columns=dataset.column_names)\n",
    "\n",
    "                train_dataloader = DataLoader(tokenized_train, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "                val_dataloader = DataLoader(tokenized_val, batch_size=batch_size, collate_fn=data_collator)\n",
    "                test_dataloader = DataLoader(tokenized_test, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "                # Initialize the model for token classification\n",
    "                model = AutoModelForTokenClassification.from_pretrained(\n",
    "                    model_name, id2label=id2label, label2id=label2id\n",
    "                )\n",
    "\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "                num_training_steps = num_epochs * len(train_dataloader)\n",
    "                lr_scheduler = get_scheduler(\n",
    "                    \"linear\",\n",
    "                    optimizer=optimizer,\n",
    "                    num_warmup_steps=0,\n",
    "                    num_training_steps=num_training_steps\n",
    "                )\n",
    "\n",
    "                accelerator = Accelerator()\n",
    "                model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "                    model, optimizer, train_dataloader, val_dataloader\n",
    "                )\n",
    "\n",
    "                # Training loop\n",
    "                for epoch in range(num_epochs):\n",
    "                    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "                    model.train()\n",
    "                    total_loss = 0\n",
    "                    progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\")\n",
    "                    for batch in progress_bar:\n",
    "                        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                        outputs = model(**batch)\n",
    "                        loss = outputs.loss\n",
    "                        total_loss += loss.item()\n",
    "\n",
    "                        accelerator.backward(loss)\n",
    "                        optimizer.step()\n",
    "                        lr_scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "                    print(f\"Epoch {epoch + 1} Loss: {total_loss:.4f}\")\n",
    "\n",
    "                # Validation loop\n",
    "                model.eval()\n",
    "                val_predictions, val_labels = [], []\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_dataloader:\n",
    "                        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                        outputs = model(**batch)\n",
    "                        logits = outputs.logits\n",
    "                        predictions = logits.argmax(dim=-1)\n",
    "                        labels = batch[\"labels\"]\n",
    "\n",
    "                        predictions = accelerator.gather(predictions)\n",
    "                        labels = accelerator.gather(labels)\n",
    "\n",
    "                        flat_labels, flat_predictions = postprocess(predictions, labels)\n",
    "                        val_labels.extend(flat_labels)\n",
    "                        val_predictions.extend(flat_predictions)\n",
    "\n",
    "                # Test loop\n",
    "                test_predictions, test_labels = [], []\n",
    "                with torch.no_grad():\n",
    "                    for batch in test_dataloader:\n",
    "                        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                        outputs = model(**batch)\n",
    "                        logits = outputs.logits\n",
    "                        predictions = logits.argmax(dim=-1)\n",
    "                        labels = batch[\"labels\"]\n",
    "\n",
    "                        predictions = accelerator.gather(predictions)\n",
    "                        labels = accelerator.gather(labels)\n",
    "\n",
    "                        flat_labels, flat_predictions = postprocess(predictions, labels)\n",
    "                        test_labels.extend(flat_labels)\n",
    "                        test_predictions.extend(flat_predictions)\n",
    "\n",
    "                # Calculate test metrics\n",
    "                precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                    test_labels, test_predictions, average=\"micro\"\n",
    "                )\n",
    "                print(f\"Test Metrics: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "                # Append results\n",
    "                new_row = pd.DataFrame(\n",
    "                    [{\"Train Size\": train_size, \"K-Fold\": split + 1, \"Test F1\": f1, \"Model\": model_name}]\n",
    "                )\n",
    "                results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "                results_df.to_excel(file_name, index=False)\n",
    "\n",
    "                # Cleanup\n",
    "                del model, train_dataloader, val_dataloader, test_dataloader\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Results saved to {file_name}\")\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "1c4a8133",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T17:01:23.350446Z",
     "start_time": "2024-12-24T16:59:35.016781Z"
    }
   },
   "source": [
    "# Define the models and their corresponding sizes\n",
    "models = {\n",
    "    \"small\": \"bert-base-cased\",\n",
    "    \"medium\": \"bert-large-cased\",\n",
    "    \"large\": \"roberta-large\"\n",
    "}\n",
    "\n",
    "iterate_and_finetune_with_torch(dataset=dataset, file_name='Experiments_full_labeled.xlsx', models=models, start_size=5, end_size=30, step_size=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 5, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 973.70 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 331.23 examples/s]\n",
      "Map: 100%|██████████| 1242/1242 [00:00<00:00, 4988.81 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s, loss=2.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.4089\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  4.86it/s, loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.8562\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s, loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.5074\n",
      "Test Metrics: Precision=0.7765, Recall=0.7765, F1=0.7765\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 5, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yjeth\\AppData\\Local\\Temp\\ipykernel_26244\\521506702.py:201: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1134.64 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 330.42 examples/s]\n",
      "Map: 100%|██████████| 1242/1242 [00:00<00:00, 5180.30 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it, loss=2.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.3787\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:04<00:00,  4.56s/it, loss=1.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.4191\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:09<00:00,  9.02s/it, loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.1486\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 8\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Define the models and their corresponding sizes\u001B[39;00m\n\u001B[0;32m      2\u001B[0m models \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msmall\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbert-base-cased\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmedium\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbert-large-cased\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlarge\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mroberta-large\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      6\u001B[0m }\n\u001B[1;32m----> 8\u001B[0m \u001B[43miterate_and_finetune_with_torch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mExperiments_full_labeled.xlsx\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[15], line 187\u001B[0m, in \u001B[0;36miterate_and_finetune_with_torch\u001B[1;34m(dataset, file_name, models, start_size, end_size, step_size, k_splits, batch_size, learning_rate, weight_decay, num_epochs)\u001B[0m\n\u001B[0;32m    184\u001B[0m predictions \u001B[38;5;241m=\u001B[39m accelerator\u001B[38;5;241m.\u001B[39mgather(predictions)\n\u001B[0;32m    185\u001B[0m labels \u001B[38;5;241m=\u001B[39m accelerator\u001B[38;5;241m.\u001B[39mgather(labels)\n\u001B[1;32m--> 187\u001B[0m flat_labels, flat_predictions \u001B[38;5;241m=\u001B[39m \u001B[43mpostprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    188\u001B[0m test_labels\u001B[38;5;241m.\u001B[39mextend(flat_labels)\n\u001B[0;32m    189\u001B[0m test_predictions\u001B[38;5;241m.\u001B[39mextend(flat_predictions)\n",
      "Cell \u001B[1;32mIn[14], line 2\u001B[0m, in \u001B[0;36mpostprocess\u001B[1;34m(predictions, labels)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpostprocess\u001B[39m(predictions, labels):\n\u001B[1;32m----> 2\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m \u001B[43mpredictions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mclone()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m      3\u001B[0m     labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mclone()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;66;03m# Flatten predictions and labels, removing ignored indices\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
