{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:57:04.145061Z",
     "start_time": "2024-12-24T16:57:04.140741Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/res/anaconda3/envs/pyenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, RobertaTokenizerFast, get_scheduler\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import Dataset, DatasetDict, Features, Sequence, Value, ClassLabel\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d65dfb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:33.977678Z",
     "start_time": "2024-12-24T16:56:33.974230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reduce VRAM usage by reducing fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf0ba874e901ac",
   "metadata": {},
   "source": [
    "## Combine all .txt and .ann files and combine them per medicine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa71104cf543fea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:35.816817Z",
     "start_time": "2024-12-24T16:56:33.983244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined output saved for LIPITOR in output_datasets/LIPITOR_combined_output.txt\n",
      "Combined output saved for VOLTAREN-XR in output_datasets/VOLTAREN-XR_combined_output.txt\n",
      "Combined output saved for ARTHROTEC in output_datasets/ARTHROTEC_combined_output.txt\n",
      "Combined output saved for VOLTAREN in output_datasets/VOLTAREN_combined_output.txt\n",
      "Combined output saved for DICLOFENAC-SODIUM in output_datasets/DICLOFENAC-SODIUM_combined_output.txt\n",
      "Combined output saved for ZIPSOR in output_datasets/ZIPSOR_combined_output.txt\n",
      "Combined output saved for FLECTOR in output_datasets/FLECTOR_combined_output.txt\n",
      "Combined output saved for PENNSAID in output_datasets/PENNSAID_combined_output.txt\n",
      "Combined output saved for CATAFLAM in output_datasets/CATAFLAM_combined_output.txt\n",
      "Combined output saved for SOLARAZE in output_datasets/SOLARAZE_combined_output.txt\n",
      "Combined output saved for DICLOFENAC-POTASSIUM in output_datasets/DICLOFENAC-POTASSIUM_combined_output.txt\n",
      "Combined output saved for CAMBIA in output_datasets/CAMBIA_combined_output.txt\n"
     ]
    }
   ],
   "source": [
    "# Base folders containing annotation and text files\n",
    "annotations_folder = 'annotations/'\n",
    "original_texts_folder = 'originaltexts/'\n",
    "output_folder = 'output_datasets/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Group files by medicine\n",
    "file_groups = {}\n",
    "for file_name in os.listdir(annotations_folder):\n",
    "    if file_name.endswith('.ann'):\n",
    "        base_name = '.'.join(file_name.split('.')[:-1])\n",
    "        medicine = base_name.rsplit('.', 1)[0]\n",
    "        file_groups.setdefault(medicine, []).append(file_name)\n",
    "\n",
    "# Process each group\n",
    "for medicine, ann_files in file_groups.items():\n",
    "    combined_output = []\n",
    "\n",
    "    for ann_file in ann_files:\n",
    "        txt_file = ann_file.replace('.ann', '.txt')\n",
    "        txt_path = os.path.join(original_texts_folder, txt_file)\n",
    "        ann_path = os.path.join(annotations_folder, ann_file)\n",
    "\n",
    "        # Ensure the corresponding .txt file exists\n",
    "        if not os.path.exists(txt_path):\n",
    "            raise FileNotFoundError(f\"Text file not found for annotation file {ann_file}\")\n",
    "\n",
    "        # Read the content of the .ann and .txt files\n",
    "        with open(ann_path, 'r') as ann_f:\n",
    "            ann_lines = ann_f.readlines()\n",
    "\n",
    "        with open(txt_path, 'r') as txt_f:\n",
    "            txt_content = txt_f.read()\n",
    "\n",
    "        # Parse annotations and filter out AnnotatorNotes\n",
    "        annotations = []\n",
    "        for line in ann_lines:\n",
    "            if line.startswith('T'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 3:\n",
    "                    tag_info, word = parts[1], parts[2]\n",
    "                    tag_parts = tag_info.split()\n",
    "                    if len(tag_parts) >= 3:\n",
    "                        tag = tag_parts[0]\n",
    "                        try:\n",
    "                            start_idx = int(tag_parts[1])\n",
    "                            end_idx = int(tag_parts[2])\n",
    "                        except ValueError:\n",
    "                            if ';' in tag_parts[2]:  # Handle ranges like '742;763'\n",
    "                                start_idx = int(tag_parts[1])\n",
    "                                end_idx = int(tag_parts[2].split(';')[-1])\n",
    "                            else:\n",
    "                                raise ValueError(f\"Unexpected annotation format: {tag_parts}\")\n",
    "                        annotations.append((start_idx, end_idx, tag, word))\n",
    "\n",
    "        # Sort annotations by start index\n",
    "        annotations.sort(key=lambda x: x[0])\n",
    "\n",
    "        # Generate output format\n",
    "        output = []\n",
    "        current_idx = 0\n",
    "        for start_idx, end_idx, tag, word in annotations:\n",
    "            # Add text between the last annotation and the current annotation as \"O\"\n",
    "            if current_idx < start_idx:\n",
    "                intervening_text = txt_content[current_idx:start_idx]\n",
    "                for token in re.findall(r\"\\w+(?:'\\w+)?|[.,!?]\", intervening_text):\n",
    "                    output.append(f\"{token} O\")\n",
    "\n",
    "            # Add the annotated word with its tag\n",
    "            for i, token in enumerate(word.split()):\n",
    "                tag_prefix = 'B-' if i == 0 else 'I-'\n",
    "                output.append(f\"{token} {tag_prefix}{tag}\")\n",
    "\n",
    "            current_idx = end_idx\n",
    "\n",
    "        # Add remaining text as \"O\"\n",
    "        if current_idx < len(txt_content):\n",
    "            remaining_text = txt_content[current_idx:]\n",
    "            for token in re.findall(r\"\\w+(?:'\\w+)?|[.,!?]\", remaining_text):\n",
    "                output.append(f\"{token} O\")\n",
    "\n",
    "        # Add to combined output with a newline separator\n",
    "        combined_output.extend(output)\n",
    "        combined_output.append('')  # Empty line between posts\n",
    "\n",
    "    # Write combined output to file\n",
    "    combined_output_text = '\\n'.join(combined_output).strip()\n",
    "    output_file = os.path.join(output_folder, f\"{medicine}_combined_output.txt\")\n",
    "    with open(output_file, 'w') as out_f:\n",
    "        out_f.write(combined_output_text)\n",
    "\n",
    "    print(f\"Combined output saved for {medicine} in {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f757861e7d38d",
   "metadata": {},
   "source": [
    "## Combine all the medicine files into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f940e15d9430fc7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:35.980494Z",
     "start_time": "2024-12-24T16:56:35.963990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset saved to final_dataset.txt\n"
     ]
    }
   ],
   "source": [
    "# Folder containing all combined output files\n",
    "output_datasets_folder = 'output_datasets/'\n",
    "final_output_file = 'final_dataset.txt'\n",
    "\n",
    "# Ensure the folder exists\n",
    "if not os.path.exists(output_datasets_folder):\n",
    "    raise FileNotFoundError(f\"The folder {output_datasets_folder} does not exist.\")\n",
    "\n",
    "# List all files in the folder\n",
    "output_files = [f for f in os.listdir(output_datasets_folder) if f.endswith('_combined_output.txt')]\n",
    "\n",
    "# Combine all files into a single final dataset\n",
    "final_dataset = []\n",
    "for file_name in output_files:\n",
    "    file_path = os.path.join(output_datasets_folder, file_name)\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read().strip()  # Read and strip any trailing spaces or newlines\n",
    "        final_dataset.append(content)\n",
    "\n",
    "    # Add an empty line to separate posts from different files\n",
    "    final_dataset.append('')\n",
    "\n",
    "# Write the combined dataset to the final output file\n",
    "with open(final_output_file, 'w') as f:\n",
    "    f.write('\\n'.join(final_dataset).strip())  # Ensure no extra trailing newline\n",
    "\n",
    "print(f\"Final dataset saved to {final_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269e1b5930cd4c4",
   "metadata": {},
   "source": [
    "## Read the final dataset into the Iob dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "435d59ec1801f10d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.014412Z",
     "start_time": "2024-12-24T16:56:36.007621Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_iob_file(file_path):\n",
    "    \"\"\"Reads an IOB file from filepath and returns sentences with tokens and tags.\"\"\"\n",
    "    sentences = []\n",
    "    sentence_tokens = []\n",
    "    sentence_labels = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # If line is not empty\n",
    "                token, tag = line.split()\n",
    "                sentence_tokens.append(token)\n",
    "                sentence_labels.append(tag)\n",
    "\n",
    "            else:\n",
    "                # End of a sentence\n",
    "                if sentence_tokens:\n",
    "                    sentences.append({\"tokens\": sentence_tokens, \"ner_tags\": sentence_labels})\n",
    "                    sentence_tokens = []\n",
    "                    sentence_labels = []\n",
    "        # Add the last sentence if file doesn't end with a newline\n",
    "        if sentence_tokens:\n",
    "            sentences.append({\"tokens\": sentence_tokens, \"ner_tags\": sentence_labels})\n",
    "    return sentences\n",
    "\n",
    "def create_dataset_from_final_file(final_file_path):\n",
    "    \"\"\"Create a dataset from a single IOB file and return it as a DatasetDict.\"\"\"\n",
    "\n",
    "    if not os.path.exists(final_file_path):\n",
    "        raise FileNotFoundError(f\"The file {final_file_path} does not exist.\")\n",
    "\n",
    "    # Parse the file\n",
    "    data = read_iob_file(final_file_path)\n",
    "\n",
    "    # Define the label names and ClassLabel feature\n",
    "    unique_labels = sorted(set(tag for d in data for tag in d[\"ner_tags\"]))\n",
    "    label_feature = ClassLabel(names=unique_labels)\n",
    "\n",
    "    # Define the Features schema for Hugging Face datasets\n",
    "    features = Features({\n",
    "        'tokens': Sequence(Value(\"string\")),\n",
    "        'ner_tags': Sequence(label_feature)\n",
    "    })\n",
    "\n",
    "    # Convert data into a Dataset\n",
    "    dataset = Dataset.from_list(data).cast(features)\n",
    "\n",
    "    # Create a DatasetDict\n",
    "    dataset_dict = DatasetDict({\"full_data\": dataset})\n",
    "\n",
    "    return dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b8e6d4d262ac49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.508707Z",
     "start_time": "2024-12-24T16:56:36.044426Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 1248/1248 [00:00<00:00, 13490.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "final_dataset_path = \"final_dataset.txt\"\n",
    "dataset_dict = create_dataset_from_final_file(final_dataset_path)\n",
    "dataset = dataset_dict['full_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1798cebc48c3299c",
   "metadata": {},
   "source": [
    "## Dataset generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4b79e14d8ca0efa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.543588Z",
     "start_time": "2024-12-24T16:56:36.539561Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_train_datasets(dataset_, number_of_samples, number_of_splits):\n",
    "    \"\"\"\n",
    "    Generates train datasets by sampling from the given dataset based on the number of samples and splits.\n",
    "\n",
    "    Args:\n",
    "        dataset_ (Dataset): The base dataset to sample from.\n",
    "        number_of_samples (int): Number of samples per dataset.\n",
    "        number_of_splits (int): Number of datasets to generate (different seeds).\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, Dataset, List[int]]]: List of generated datasets with their names and indices.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "\n",
    "    for seed in range(number_of_splits):\n",
    "        # Set the random seed for reproducibility\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Shuffle and sample from the dataset\n",
    "        indices = list(range(len(dataset_)))\n",
    "        random.shuffle(indices)\n",
    "        sampled_indices = indices[:number_of_samples]\n",
    "\n",
    "        sampled_dataset = dataset_.select(sampled_indices)\n",
    "\n",
    "        # Add the dataset with its name and indices\n",
    "        datasets.append((f\"train_dataset_{number_of_samples}_{seed}\", sampled_dataset, sampled_indices))\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "513adc9d9f4ee109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.560393Z",
     "start_time": "2024-12-24T16:56:36.555449Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_validation_datasets(dataset_, train_indices, number_of_samples, number_of_splits):\n",
    "    \"\"\"\n",
    "    Generates validation datasets by sampling from the given dataset, ensuring no overlap with training data.\n",
    "\n",
    "    Args:\n",
    "        dataset_ (Dataset): The base dataset to sample from.\n",
    "        train_indices (List[int]): Indices of the training dataset to exclude from sampling.\n",
    "        number_of_samples (int): Number of samples per validation dataset.\n",
    "        number_of_splits (int): Number of validation datasets to generate (different seeds).\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, Dataset, List[int]]]: List of generated validation datasets with names and indices.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    all_indices = set(range(len(dataset_)))\n",
    "    available_indices = list(all_indices - set(train_indices))  # Exclude training indices\n",
    "\n",
    "    for seed in range(number_of_splits):\n",
    "        # Set the random seed for reproducibility\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Shuffle and sample from the remaining indices\n",
    "        random.shuffle(available_indices)\n",
    "        sampled_indices = available_indices[:int(number_of_samples / 5)]\n",
    "\n",
    "        sampled_dataset = dataset_.select(sampled_indices)\n",
    "\n",
    "        # Add the dataset with its name and indices\n",
    "        datasets.append((f\"val_dataset_{number_of_samples/5}_{seed}\", sampled_dataset, sampled_indices))\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58e5dae8d7e7bcf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.571276Z",
     "start_time": "2024-12-24T16:56:36.567043Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_test_datasets(dataset_, train_indices, val_indices, number_of_samples, number_of_splits):\n",
    "    \"\"\"\n",
    "    Generates test datasets by sampling from the given dataset, ensuring no overlap with training or validation data.\n",
    "\n",
    "    Args:\n",
    "        dataset_ (Dataset): The base dataset to sample from.\n",
    "        train_indices (List[int]): Indices of the training dataset to exclude from sampling.\n",
    "        val_indices (List[int]): Indices of the validation dataset to exclude from sampling.\n",
    "        number_of_samples (int): Number of samples per test dataset.\n",
    "        number_of_splits (int): Number of test datasets to generate (different seeds).\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, Dataset]]: List of generated test datasets with names.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    all_indices = set(range(len(dataset_)))\n",
    "    available_indices = list(all_indices - set(train_indices) - set(val_indices))  # Exclude train and val indices\n",
    "\n",
    "    for seed in range(number_of_splits):\n",
    "        sampled_indices = available_indices[:]\n",
    "\n",
    "        sampled_dataset = dataset_.select(sampled_indices)\n",
    "\n",
    "        # Add the dataset with its name\n",
    "        datasets.append((f\"test_dataset_{number_of_samples}_{seed}\", sampled_dataset))\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bfb3346f6372cc",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f8557b1a1dc6dd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.581336Z",
     "start_time": "2024-12-24T16:56:36.578214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Generate Train Dataset\n",
    "# train_datasets = generate_train_datasets(dataset, number_of_samples=30, number_of_splits=1)\n",
    "# train_name, train_dataset, train_indices = train_datasets[0]\n",
    "# print(f\"{train_name}: {len(train_dataset)} samples\")\n",
    "#\n",
    "# # Step 2: Generate Validation Dataset\n",
    "# val_datasets = generate_validation_datasets(dataset, train_indices, number_of_samples=30, number_of_splits=1)\n",
    "# val_name, val_dataset, val_indices = val_datasets[0]\n",
    "# print(f\"{val_name}: {len(val_dataset)} samples\")\n",
    "#\n",
    "# # Step 3: Generate Test Dataset\n",
    "# test_datasets = generate_test_datasets(dataset, train_indices, val_indices, number_of_samples=30, number_of_splits=1)\n",
    "# test_name, test_dataset = test_datasets[0]\n",
    "# print(f\"{test_name}: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7084bbcd75432d9",
   "metadata": {},
   "source": [
    "# Prepping for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc367cf3648c3146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.598393Z",
     "start_time": "2024-12-24T16:56:36.595331Z"
    }
   },
   "outputs": [],
   "source": [
    "label_names = dataset.features[\"ner_tags\"].feature.names\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a875af61f42f00a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.626889Z",
     "start_time": "2024-12-24T16:56:36.622730Z"
    }
   },
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Flatten predictions and labels, removing ignored indices\n",
    "    true_labels = [label for label_seq in labels for label in label_seq if label != -100]\n",
    "    true_predictions = [pred for pred_seq, label_seq in zip(predictions, labels)\n",
    "                        for pred, label in zip(pred_seq, label_seq) if label != -100]\n",
    "    return true_labels, true_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30995779e545bcf",
   "metadata": {},
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa33f20959c42e46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:56:36.655480Z",
     "start_time": "2024-12-24T16:56:36.640507Z"
    }
   },
   "outputs": [],
   "source": [
    "def iterate_and_finetune_with_torch(\n",
    "    dataset,\n",
    "    file_name,\n",
    "    models,\n",
    "    start_size=5,\n",
    "    end_size=500,\n",
    "    step_size=5,\n",
    "    k_splits=5,\n",
    "    batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.0,\n",
    "    num_epochs=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune models with varying dataset sizes and k-fold splits, saving results to Excel.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (DatasetDict): Dataset for training, validation, and testing.\n",
    "    - file_name (str): Excel file to save results.\n",
    "    - models (dict): Dictionary of model names and their sizes.\n",
    "    - start_size (int): Starting size for training datasets.\n",
    "    - end_size (int): Maximum size for training datasets.\n",
    "    - step_size (int): Step size for increasing dataset sizes.\n",
    "    - k_splits (int): Number of k-fold splits.\n",
    "    - batch_size (int): Training batch size.\n",
    "    - learning_rate (float): Learning rate for fine-tuning.\n",
    "    - weight_decay (float): Weight decay for optimizer.\n",
    "    - num_epochs (int): Number of training epochs.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Check or create the results file\n",
    "    if os.path.exists(file_name):\n",
    "        results_df = pd.read_excel(file_name)\n",
    "    else:\n",
    "        results_df = pd.DataFrame(columns=[\"Train Size\", \"K-Fold\", \"Test F1\", \"Model\"])\n",
    "\n",
    "    for train_size in range(start_size, end_size + 1, step_size):\n",
    "        for split in range(k_splits):\n",
    "            for size, model_name in models.items():\n",
    "                print(f\"\\nFine-tuning {model_name} ({size}) with Train Size {train_size}, Split {split + 1}...\")\n",
    "\n",
    "                # Initialize tokenizer\n",
    "                if size == \"large\":\n",
    "                    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-large\", add_prefix_space=True)\n",
    "                else:\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "                data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "                # Generate datasets\n",
    "                train_datasets = generate_train_datasets(\n",
    "                    dataset, number_of_samples=train_size, number_of_splits=k_splits\n",
    "                )\n",
    "                _, train_dataset, train_indices = train_datasets[split]\n",
    "\n",
    "                val_datasets = generate_validation_datasets(\n",
    "                    dataset, train_indices=train_indices, number_of_samples=train_size, number_of_splits=k_splits\n",
    "                )\n",
    "                _, val_dataset, val_indices = val_datasets[split]\n",
    "\n",
    "                test_datasets = generate_test_datasets(\n",
    "                    dataset, train_indices=train_indices, val_indices=val_indices,\n",
    "                    number_of_samples=train_size, number_of_splits=k_splits\n",
    "                )\n",
    "                _, test_dataset = test_datasets[split]\n",
    "\n",
    "                def align_labels_with_tokens(labels_, word_ids):\n",
    "                    new_labels = []\n",
    "                    current_word = None\n",
    "\n",
    "                    for word_id in word_ids:\n",
    "                        if word_id != current_word:\n",
    "                            current_word = word_id\n",
    "                            label = -100 if word_id is None else labels_[word_id]\n",
    "                            new_labels.append(label)\n",
    "\n",
    "                        elif word_id is None:\n",
    "                            # Special token\n",
    "                            new_labels.append(-100)\n",
    "\n",
    "                        else:\n",
    "                            # Same word as previous token\n",
    "                            label = labels_[word_id]\n",
    "\n",
    "                            # If the label is B-XXX we change it to I-XXX\n",
    "                            if label % 2 == 1:\n",
    "                                label += 1\n",
    "                            new_labels.append(label)\n",
    "\n",
    "                    return new_labels\n",
    "\n",
    "\n",
    "                def tokenize_and_align_labels(examples):\n",
    "                    tokenized_inputs = tokenizer(\n",
    "                        examples[\"tokens\"], truncation=True,\n",
    "                        is_split_into_words=True\n",
    "                    )\n",
    "                    all_labels = examples[\"ner_tags\"]\n",
    "                    new_labels = []\n",
    "                    for i, labels_ in enumerate(all_labels):\n",
    "                        word_ids = tokenized_inputs.word_ids(i)\n",
    "                        new_labels.append(align_labels_with_tokens(labels_, word_ids))\n",
    "\n",
    "                    tokenized_inputs[\"labels\"] = new_labels\n",
    "                    return tokenized_inputs\n",
    "\n",
    "                # Tokenize datasets\n",
    "                tokenized_train = train_dataset.map(tokenize_and_align_labels, batched=True,  remove_columns=dataset.column_names)\n",
    "                tokenized_val = val_dataset.map(tokenize_and_align_labels, batched=True,  remove_columns=dataset.column_names)\n",
    "                tokenized_test = test_dataset.map(tokenize_and_align_labels, batched=True,  remove_columns=dataset.column_names)\n",
    "\n",
    "                train_dataloader = DataLoader(tokenized_train, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "                val_dataloader = DataLoader(tokenized_val, batch_size=batch_size, collate_fn=data_collator)\n",
    "                test_dataloader = DataLoader(tokenized_test, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "                # Initialize the model for token classification\n",
    "                model = AutoModelForTokenClassification.from_pretrained(\n",
    "                    model_name, id2label=id2label, label2id=label2id\n",
    "                )\n",
    "                \n",
    "                #Optimize GPU ram at cost of some speed\n",
    "                model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "                num_training_steps = num_epochs * len(train_dataloader)\n",
    "                lr_scheduler = get_scheduler(\n",
    "                    \"linear\",\n",
    "                    optimizer=optimizer,\n",
    "                    num_warmup_steps=0,\n",
    "                    num_training_steps=num_training_steps\n",
    "                )\n",
    "\n",
    "                accelerator = Accelerator()\n",
    "                model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "                    model, optimizer, train_dataloader, val_dataloader\n",
    "                )\n",
    "\n",
    "\n",
    "                # Training loop\n",
    "                for epoch in range(num_epochs):\n",
    "                    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "                    model.train()\n",
    "                    total_loss = 0\n",
    "                    progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\")\n",
    "                    for batch in progress_bar:\n",
    "                        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                        outputs = model(**batch)\n",
    "                        loss = outputs.loss\n",
    "                        total_loss += loss.item()\n",
    "\n",
    "                        accelerator.backward(loss)\n",
    "                        optimizer.step()\n",
    "                        lr_scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "                    print(f\"Epoch {epoch + 1} Loss: {total_loss:.4f}\")\n",
    "\n",
    "                # Validation loop\n",
    "                model.eval()\n",
    "                val_predictions, val_labels = [], []\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_dataloader:\n",
    "                        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                        outputs = model(**batch)\n",
    "                        logits = outputs.logits\n",
    "                        predictions = logits.argmax(dim=-1)\n",
    "                        labels = batch[\"labels\"]\n",
    "\n",
    "                        predictions = accelerator.gather(predictions)\n",
    "                        labels = accelerator.gather(labels)\n",
    "\n",
    "                        flat_labels, flat_predictions = postprocess(predictions, labels)\n",
    "                        val_labels.extend(flat_labels)\n",
    "                        val_predictions.extend(flat_predictions)\n",
    "\n",
    "                # Test loop\n",
    "                test_predictions, test_labels = [], []\n",
    "                with torch.no_grad():\n",
    "                    for batch in test_dataloader:\n",
    "                        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                        outputs = model(**batch)\n",
    "                        logits = outputs.logits\n",
    "                        predictions = logits.argmax(dim=-1)\n",
    "                        labels = batch[\"labels\"]\n",
    "\n",
    "                        predictions = accelerator.gather(predictions)\n",
    "                        labels = accelerator.gather(labels)\n",
    "\n",
    "                        flat_labels, flat_predictions = postprocess(predictions, labels)\n",
    "                        test_labels.extend(flat_labels)\n",
    "                        test_predictions.extend(flat_predictions)\n",
    "\n",
    "                # Calculate test metrics\n",
    "                precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                    test_labels, test_predictions, average=\"micro\"\n",
    "                )\n",
    "                print(f\"Test Metrics: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "                # Append results\n",
    "                new_row = pd.DataFrame(\n",
    "                    [{\"Train Size\": train_size, \"K-Fold\": split + 1, \"Test F1\": f1, \"Model\": model_name}]\n",
    "                )\n",
    "                results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "                results_df.to_excel(file_name, index=False)\n",
    "\n",
    "                # Cleanup\n",
    "                del model, train_dataloader, val_dataloader, test_dataloader, predictions, labels, test_labels\n",
    "                del test_predictions, logits, loss, outputs, precision, recall, f1, new_row \n",
    "                del val_dataset, train_dataset, test_dataset, val_datasets, train_datasets, test_datasets\n",
    "                del optimizer, tokenized_train, tokenized_test, tokenized_val, tokenizer\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Results saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c4a8133",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T17:01:23.350446Z",
     "start_time": "2024-12-24T16:59:35.016781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 500, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8939.91 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7813.53 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9405.15 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:10<00:00,  6.28it/s, loss=0.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.1593\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:09<00:00,  6.65it/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.8825\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:09<00:00,  6.50it/s, loss=0.224] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.2111\n",
      "Test Metrics: Precision=0.9100, Recall=0.9100, F1=0.9100\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 500, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8948.88 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7603.47 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 8907.67 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:29<00:00,  2.10it/s, loss=0.104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.7254\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:30<00:00,  2.09it/s, loss=0.0812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.0859\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:30<00:00,  2.05it/s, loss=0.129] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.9905\n",
      "Test Metrics: Precision=0.9186, Recall=0.9186, F1=0.9186\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9798.86 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7946.62 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9891.87 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:27<00:00,  2.27it/s, loss=0.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.2582\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:27<00:00,  2.29it/s, loss=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.7184\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:29<00:00,  2.16it/s, loss=0.135] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6425\n",
      "Test Metrics: Precision=0.9333, Recall=0.9333, F1=0.9333\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 500, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8494.76 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7516.54 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 8710.30 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:10<00:00,  6.03it/s, loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 34.2931\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:10<00:00,  6.09it/s, loss=0.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.6747\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:10<00:00,  6.17it/s, loss=0.131] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.1127\n",
      "Test Metrics: Precision=0.9124, Recall=0.9124, F1=0.9124\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 500, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9597.99 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7925.59 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9305.55 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:31<00:00,  2.02it/s, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.3671\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:31<00:00,  1.99it/s, loss=0.194] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.7689\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:32<00:00,  1.96it/s, loss=0.115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9727\n",
      "Test Metrics: Precision=0.9185, Recall=0.9185, F1=0.9185\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9417.49 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7752.16 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9058.52 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:29<00:00,  2.12it/s, loss=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.2547\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:29<00:00,  2.17it/s, loss=0.149] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.0209\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:30<00:00,  2.10it/s, loss=0.168] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.1675\n",
      "Test Metrics: Precision=0.9321, Recall=0.9321, F1=0.9321\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 500, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8740.35 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8430.25 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9654.13 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:11<00:00,  5.64it/s, loss=0.218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.5508\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:11<00:00,  5.55it/s, loss=0.346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.7075\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:11<00:00,  5.58it/s, loss=0.127] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.3906\n",
      "Test Metrics: Precision=0.9049, Recall=0.9049, F1=0.9049\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 500, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8403.57 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7911.24 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 8945.82 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:33<00:00,  1.89it/s, loss=0.689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 35.2110\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:33<00:00,  1.86it/s, loss=0.477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.7552\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:33<00:00,  1.86it/s, loss=0.0731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.1147\n",
      "Test Metrics: Precision=0.9121, Recall=0.9121, F1=0.9121\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9014.31 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8377.55 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9834.85 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:31<00:00,  2.03it/s, loss=0.174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.7020\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:31<00:00,  2.03it/s, loss=0.12] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1986\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:30<00:00,  2.05it/s, loss=0.322] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5771\n",
      "Test Metrics: Precision=0.9332, Recall=0.9332, F1=0.9332\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 500, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8581.35 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7118.28 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 8811.53 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:10<00:00,  5.83it/s, loss=0.496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 38.6916\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:10<00:00,  5.92it/s, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 19.2435\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:10<00:00,  5.99it/s, loss=0.0951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.7200\n",
      "Test Metrics: Precision=0.9067, Recall=0.9067, F1=0.9067\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 500, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9451.53 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7857.44 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9566.33 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:32<00:00,  1.96it/s, loss=0.189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.6605\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:31<00:00,  1.99it/s, loss=0.101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.3580\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:31<00:00,  2.02it/s, loss=0.153] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.1601\n",
      "Test Metrics: Precision=0.9202, Recall=0.9202, F1=0.9202\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8747.87 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7148.61 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 8990.20 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:30<00:00,  2.04it/s, loss=0.236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.7229\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:30<00:00,  2.08it/s, loss=0.229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.6414\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:29<00:00,  2.11it/s, loss=0.172] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.1909\n",
      "Test Metrics: Precision=0.9307, Recall=0.9307, F1=0.9307\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 500, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9327.93 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8841.28 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9165.74 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:11<00:00,  5.69it/s, loss=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 37.4265\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:11<00:00,  5.65it/s, loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.7155\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:11<00:00,  5.49it/s, loss=0.174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.1346\n",
      "Test Metrics: Precision=0.9126, Recall=0.9126, F1=0.9126\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 500, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8639.39 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8495.14 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 8636.40 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:33<00:00,  1.88it/s, loss=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.1481\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:33<00:00,  1.90it/s, loss=0.272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.7950\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:33<00:00,  1.87it/s, loss=0.0425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.0362\n",
      "Test Metrics: Precision=0.9199, Recall=0.9199, F1=0.9199\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9192.87 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8667.53 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9249.34 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:31<00:00,  2.00it/s, loss=0.462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.3731\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:31<00:00,  2.01it/s, loss=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.1399\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:31<00:00,  1.99it/s, loss=0.0931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.5362\n",
      "Test Metrics: Precision=0.9319, Recall=0.9319, F1=0.9319\n",
      "Results saved to Experiments_full_labeled.xlsx\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"small\": \"bert-base-cased\",\n",
    "    \"medium\": \"bert-large-cased\",\n",
    "    \"large\": \"roberta-large\"\n",
    "}\n",
    "\n",
    "iterate_and_finetune_with_torch(dataset=dataset, file_name='Experiments_full_labeled.xlsx', models=models, start_size=500, end_size=500, step_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c32a0d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 230, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 230/230 [00:00<00:00, 6874.63 examples/s]\n",
      "Map: 100%|██████████| 46/46 [00:00<00:00, 6216.39 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:00<00:00, 9899.70 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 29/29 [00:12<00:00,  2.32it/s, loss=0.451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.1539\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 29/29 [00:12<00:00,  2.29it/s, loss=0.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.8948\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 29/29 [00:12<00:00,  2.27it/s, loss=0.193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.3007\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 29/29 [00:13<00:00,  2.23it/s, loss=0.119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.8220\n",
      "Test Metrics: Precision=0.9248, Recall=0.9248, F1=0.9248\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 230, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 230/230 [00:00<00:00, 8573.65 examples/s]\n",
      "Map: 100%|██████████| 46/46 [00:00<00:00, 4918.62 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:00<00:00, 9930.03 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 29/29 [00:13<00:00,  2.15it/s, loss=0.318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.1513\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 29/29 [00:13<00:00,  2.13it/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.9576\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 29/29 [00:13<00:00,  2.09it/s, loss=0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4727\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 29/29 [00:13<00:00,  2.18it/s, loss=0.115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.7431\n",
      "Test Metrics: Precision=0.9282, Recall=0.9282, F1=0.9282\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 230, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 230/230 [00:00<00:00, 8432.68 examples/s]\n",
      "Map: 100%|██████████| 46/46 [00:00<00:00, 6335.81 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:00<00:00, 9513.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 29/29 [00:14<00:00,  2.05it/s, loss=0.38] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.8315\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 29/29 [00:14<00:00,  2.02it/s, loss=0.323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8092\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 29/29 [00:14<00:00,  2.00it/s, loss=0.12] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0276\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 29/29 [00:14<00:00,  1.97it/s, loss=0.146] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6522\n",
      "Test Metrics: Precision=0.9239, Recall=0.9239, F1=0.9239\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 230, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 230/230 [00:00<00:00, 8787.40 examples/s]\n",
      "Map: 100%|██████████| 46/46 [00:00<00:00, 6120.35 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:00<00:00, 9395.49 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 29/29 [00:14<00:00,  1.97it/s, loss=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.8701\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 29/29 [00:14<00:00,  2.01it/s, loss=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.5991\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 29/29 [00:14<00:00,  2.02it/s, loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1273\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 29/29 [00:14<00:00,  2.07it/s, loss=0.188] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7711\n",
      "Test Metrics: Precision=0.9200, Recall=0.9200, F1=0.9200\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 230, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 230/230 [00:00<00:00, 8430.76 examples/s]\n",
      "Map: 100%|██████████| 46/46 [00:00<00:00, 6957.48 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:00<00:00, 9712.97 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 29/29 [00:14<00:00,  1.95it/s, loss=0.419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.0435\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 29/29 [00:15<00:00,  1.93it/s, loss=0.229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.6151\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 29/29 [00:14<00:00,  1.96it/s, loss=0.222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3708\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 29/29 [00:15<00:00,  1.86it/s, loss=0.105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6083\n",
      "Test Metrics: Precision=0.9271, Recall=0.9271, F1=0.9271\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 235, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 235/235 [00:00<00:00, 8586.95 examples/s]\n",
      "Map: 100%|██████████| 47/47 [00:00<00:00, 6393.34 examples/s]\n",
      "Map: 100%|██████████| 966/966 [00:00<00:00, 9591.91 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:13<00:00,  2.19it/s, loss=0.498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.1261\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:13<00:00,  2.23it/s, loss=0.42] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.1006\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:13<00:00,  2.18it/s, loss=0.175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.9448\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:13<00:00,  2.17it/s, loss=0.185] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1362\n",
      "Test Metrics: Precision=0.9243, Recall=0.9243, F1=0.9243\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 235, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 235/235 [00:00<00:00, 8759.88 examples/s]\n",
      "Map: 100%|██████████| 47/47 [00:00<00:00, 6145.79 examples/s]\n",
      "Map: 100%|██████████| 966/966 [00:00<00:00, 9372.92 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:14<00:00,  2.09it/s, loss=0.7]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.1078\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:14<00:00,  2.09it/s, loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.4120\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:14<00:00,  2.07it/s, loss=0.278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4679\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:13<00:00,  2.16it/s, loss=0.0684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8451\n",
      "Test Metrics: Precision=0.9213, Recall=0.9213, F1=0.9213\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 235, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 235/235 [00:00<00:00, 8569.78 examples/s]\n",
      "Map: 100%|██████████| 47/47 [00:00<00:00, 6385.68 examples/s]\n",
      "Map: 100%|██████████| 966/966 [00:00<00:00, 9448.37 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:14<00:00,  2.09it/s, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.9471\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:14<00:00,  2.00it/s, loss=0.437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.8872\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:14<00:00,  2.08it/s, loss=0.108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0900\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:14<00:00,  2.02it/s, loss=0.34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8306\n",
      "Test Metrics: Precision=0.9255, Recall=0.9255, F1=0.9255\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 235, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 235/235 [00:00<00:00, 8616.52 examples/s]\n",
      "Map: 100%|██████████| 47/47 [00:00<00:00, 6553.16 examples/s]\n",
      "Map: 100%|██████████| 966/966 [00:00<00:00, 9375.87 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:14<00:00,  2.04it/s, loss=0.394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.6685\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:14<00:00,  2.10it/s, loss=0.158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.7430\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:14<00:00,  2.01it/s, loss=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.1439\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:14<00:00,  2.09it/s, loss=0.144] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.3838\n",
      "Test Metrics: Precision=0.9237, Recall=0.9237, F1=0.9237\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 235, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 235/235 [00:00<00:00, 8275.50 examples/s]\n",
      "Map: 100%|██████████| 47/47 [00:00<00:00, 6139.09 examples/s]\n",
      "Map: 100%|██████████| 966/966 [00:00<00:00, 9601.00 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:15<00:00,  1.93it/s, loss=0.334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.3138\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:15<00:00,  1.99it/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.7090\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:15<00:00,  1.96it/s, loss=0.453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.4535\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:15<00:00,  1.99it/s, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.5384\n",
      "Test Metrics: Precision=0.9283, Recall=0.9283, F1=0.9283\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 240, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 240/240 [00:00<00:00, 8775.38 examples/s]\n",
      "Map: 100%|██████████| 48/48 [00:00<00:00, 6887.91 examples/s]\n",
      "Map: 100%|██████████| 960/960 [00:00<00:00, 9576.45 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:13<00:00,  2.14it/s, loss=0.499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.5754\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:13<00:00,  2.15it/s, loss=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.7865\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:13<00:00,  2.14it/s, loss=0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.1842\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:14<00:00,  2.14it/s, loss=0.198] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.9005\n",
      "Test Metrics: Precision=0.9265, Recall=0.9265, F1=0.9265\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 240, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 240/240 [00:00<00:00, 8467.57 examples/s]\n",
      "Map: 100%|██████████| 48/48 [00:00<00:00, 5717.39 examples/s]\n",
      "Map: 100%|██████████| 960/960 [00:00<00:00, 9492.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:14<00:00,  2.07it/s, loss=0.508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.9972\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:14<00:00,  2.04it/s, loss=0.268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.9490\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:14<00:00,  2.09it/s, loss=0.222] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.6022\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:14<00:00,  2.08it/s, loss=0.169] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1501\n",
      "Test Metrics: Precision=0.9244, Recall=0.9244, F1=0.9244\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 240, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 240/240 [00:00<00:00, 8516.14 examples/s]\n",
      "Map: 100%|██████████| 48/48 [00:00<00:00, 5055.66 examples/s]\n",
      "Map: 100%|██████████| 960/960 [00:00<00:00, 9847.18 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:14<00:00,  2.00it/s, loss=0.352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.5331\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:14<00:00,  2.07it/s, loss=0.386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.6900\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:14<00:00,  2.04it/s, loss=0.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.7747\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:13<00:00,  2.15it/s, loss=0.0993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1071\n",
      "Test Metrics: Precision=0.9251, Recall=0.9251, F1=0.9251\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 240, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 240/240 [00:00<00:00, 8562.64 examples/s]\n",
      "Map: 100%|██████████| 48/48 [00:00<00:00, 5837.75 examples/s]\n",
      "Map: 100%|██████████| 960/960 [00:00<00:00, 9106.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:14<00:00,  2.05it/s, loss=0.36] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.5935\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:14<00:00,  2.06it/s, loss=0.321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.1743\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:14<00:00,  2.01it/s, loss=0.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3395\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:14<00:00,  2.02it/s, loss=0.138] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7071\n",
      "Test Metrics: Precision=0.9222, Recall=0.9222, F1=0.9222\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 240, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 240/240 [00:00<00:00, 8430.90 examples/s]\n",
      "Map: 100%|██████████| 48/48 [00:00<00:00, 6472.07 examples/s]\n",
      "Map: 100%|██████████| 960/960 [00:00<00:00, 9557.74 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:16<00:00,  1.87it/s, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.6783\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:16<00:00,  1.85it/s, loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.5087\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:14<00:00,  2.00it/s, loss=0.423] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5644\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:15<00:00,  1.89it/s, loss=0.163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7057\n",
      "Test Metrics: Precision=0.9287, Recall=0.9287, F1=0.9287\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 245, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 245/245 [00:00<00:00, 8560.37 examples/s]\n",
      "Map: 100%|██████████| 49/49 [00:00<00:00, 6264.54 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 9416.29 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 31/31 [00:13<00:00,  2.27it/s, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.8038\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 31/31 [00:13<00:00,  2.23it/s, loss=0.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.1451\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 31/31 [00:14<00:00,  2.15it/s, loss=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8509\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 31/31 [00:13<00:00,  2.21it/s, loss=0.392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9377\n",
      "Test Metrics: Precision=0.9192, Recall=0.9192, F1=0.9192\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 245, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 245/245 [00:00<00:00, 8498.08 examples/s]\n",
      "Map: 100%|██████████| 49/49 [00:00<00:00, 5765.29 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 9592.01 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 31/31 [00:14<00:00,  2.08it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.2771\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 31/31 [00:15<00:00,  2.02it/s, loss=0.543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8214\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 31/31 [00:14<00:00,  2.14it/s, loss=0.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9346\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 31/31 [00:14<00:00,  2.12it/s, loss=0.172] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1403\n",
      "Test Metrics: Precision=0.9255, Recall=0.9255, F1=0.9255\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 245, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 245/245 [00:00<00:00, 8501.80 examples/s]\n",
      "Map: 100%|██████████| 49/49 [00:00<00:00, 5856.30 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 9541.51 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 31/31 [00:15<00:00,  2.01it/s, loss=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.8881\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 31/31 [00:15<00:00,  2.01it/s, loss=0.115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.4028\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 31/31 [00:14<00:00,  2.08it/s, loss=0.341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.9755\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 31/31 [00:15<00:00,  2.02it/s, loss=0.146] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.3427\n",
      "Test Metrics: Precision=0.9254, Recall=0.9254, F1=0.9254\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 245, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 245/245 [00:00<00:00, 8532.51 examples/s]\n",
      "Map: 100%|██████████| 49/49 [00:00<00:00, 6094.93 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 9170.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 31/31 [00:15<00:00,  1.99it/s, loss=0.549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.6033\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 31/31 [00:14<00:00,  2.07it/s, loss=0.286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.8133\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 31/31 [00:14<00:00,  2.07it/s, loss=0.577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.0665\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 31/31 [00:15<00:00,  2.05it/s, loss=0.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.4310\n",
      "Test Metrics: Precision=0.9206, Recall=0.9206, F1=0.9206\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 245, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 245/245 [00:00<00:00, 8660.15 examples/s]\n",
      "Map: 100%|██████████| 49/49 [00:00<00:00, 6444.88 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 9841.38 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 31/31 [00:15<00:00,  2.05it/s, loss=0.79] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.2890\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 31/31 [00:16<00:00,  1.94it/s, loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.0579\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 31/31 [00:17<00:00,  1.80it/s, loss=0.102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8550\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 31/31 [00:16<00:00,  1.91it/s, loss=0.146] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5020\n",
      "Test Metrics: Precision=0.9257, Recall=0.9257, F1=0.9257\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 250, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 250/250 [00:00<00:00, 8717.07 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6823.33 examples/s]\n",
      "Map: 100%|██████████| 948/948 [00:00<00:00, 9478.54 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:14<00:00,  2.22it/s, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.5523\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:14<00:00,  2.24it/s, loss=0.255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.8245\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:14<00:00,  2.24it/s, loss=0.0934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.5945\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:14<00:00,  2.22it/s, loss=0.12]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.0216\n",
      "Test Metrics: Precision=0.9270, Recall=0.9270, F1=0.9270\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 250, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 250/250 [00:00<00:00, 9006.91 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6867.35 examples/s]\n",
      "Map: 100%|██████████| 948/948 [00:00<00:00, 9860.14 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:15<00:00,  2.10it/s, loss=0.452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.7303\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:15<00:00,  2.11it/s, loss=0.36] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.1378\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:15<00:00,  2.11it/s, loss=0.278] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4081\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:15<00:00,  2.12it/s, loss=0.0742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3393\n",
      "Test Metrics: Precision=0.9271, Recall=0.9271, F1=0.9271\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 250, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 250/250 [00:00<00:00, 1665.55 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6865.55 examples/s]\n",
      "Map: 100%|██████████| 948/948 [00:00<00:00, 9577.47 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:15<00:00,  2.05it/s, loss=0.387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.0284\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.8851\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:15<00:00,  2.07it/s, loss=0.174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.2143\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:14<00:00,  2.15it/s, loss=0.0188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.4198\n",
      "Test Metrics: Precision=0.9264, Recall=0.9264, F1=0.9264\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 250, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 250/250 [00:00<00:00, 8530.56 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6852.09 examples/s]\n",
      "Map: 100%|██████████| 948/948 [00:00<00:00, 9273.83 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:15<00:00,  2.11it/s, loss=0.406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.5455\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:15<00:00,  2.02it/s, loss=0.131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.3530\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:15<00:00,  2.08it/s, loss=0.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.4025\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:15<00:00,  2.12it/s, loss=0.159] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9099\n",
      "Test Metrics: Precision=0.9243, Recall=0.9243, F1=0.9243\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 250, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 250/250 [00:00<00:00, 8495.65 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6334.28 examples/s]\n",
      "Map: 100%|██████████| 948/948 [00:00<00:00, 9484.26 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s, loss=0.485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.1574\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:16<00:00,  1.92it/s, loss=0.388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.3883\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:16<00:00,  1.95it/s, loss=0.337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2573\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s, loss=0.348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5437\n",
      "Test Metrics: Precision=0.9284, Recall=0.9284, F1=0.9284\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 255, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 255/255 [00:00<00:00, 8372.59 examples/s]\n",
      "Map: 100%|██████████| 51/51 [00:00<00:00, 6476.41 examples/s]\n",
      "Map: 100%|██████████| 942/942 [00:00<00:00, 9300.01 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:14<00:00,  2.14it/s, loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.0105\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:14<00:00,  2.24it/s, loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.5978\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:14<00:00,  2.21it/s, loss=0.227] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.5710\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, loss=0.248] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.8954\n",
      "Test Metrics: Precision=0.9283, Recall=0.9283, F1=0.9283\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 255, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 255/255 [00:00<00:00, 8568.51 examples/s]\n",
      "Map: 100%|██████████| 51/51 [00:00<00:00, 6914.36 examples/s]\n",
      "Map: 100%|██████████| 942/942 [00:00<00:00, 9361.53 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:15<00:00,  2.12it/s, loss=0.515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.4079\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:15<00:00,  2.04it/s, loss=0.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.0287\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:15<00:00,  2.01it/s, loss=0.283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.4587\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:15<00:00,  2.09it/s, loss=0.0823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.3724\n",
      "Test Metrics: Precision=0.9284, Recall=0.9284, F1=0.9284\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 255, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 255/255 [00:00<00:00, 8504.94 examples/s]\n",
      "Map: 100%|██████████| 51/51 [00:00<00:00, 5926.29 examples/s]\n",
      "Map: 100%|██████████| 942/942 [00:00<00:00, 9756.34 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:15<00:00,  2.05it/s, loss=0.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.9595\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:15<00:00,  2.08it/s, loss=0.146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.1751\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:14<00:00,  2.15it/s, loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1850\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:15<00:00,  2.08it/s, loss=0.205] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6459\n",
      "Test Metrics: Precision=0.9248, Recall=0.9248, F1=0.9248\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 255, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 255/255 [00:00<00:00, 7645.25 examples/s]\n",
      "Map: 100%|██████████| 51/51 [00:00<00:00, 6479.16 examples/s]\n",
      "Map: 100%|██████████| 942/942 [00:00<00:00, 9608.73 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:16<00:00,  2.00it/s, loss=0.448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.8800\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:16<00:00,  1.99it/s, loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.8402\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:15<00:00,  2.06it/s, loss=0.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.9538\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:15<00:00,  2.01it/s, loss=0.0931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7068\n",
      "Test Metrics: Precision=0.9247, Recall=0.9247, F1=0.9247\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 255, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 255/255 [00:00<00:00, 8455.92 examples/s]\n",
      "Map: 100%|██████████| 51/51 [00:00<00:00, 7018.03 examples/s]\n",
      "Map: 100%|██████████| 942/942 [00:00<00:00, 9511.15 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:16<00:00,  1.89it/s, loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.1630\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:16<00:00,  1.89it/s, loss=0.147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.3563\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s, loss=0.157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.0647\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:17<00:00,  1.86it/s, loss=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2350\n",
      "Test Metrics: Precision=0.9295, Recall=0.9295, F1=0.9295\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 260, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 260/260 [00:00<00:00, 8575.35 examples/s]\n",
      "Map: 100%|██████████| 52/52 [00:00<00:00, 6426.92 examples/s]\n",
      "Map: 100%|██████████| 936/936 [00:00<00:00, 9609.18 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 33/33 [00:15<00:00,  2.18it/s, loss=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.9363\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 33/33 [00:14<00:00,  2.21it/s, loss=0.173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.0395\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 33/33 [00:14<00:00,  2.24it/s, loss=0.182] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.4619\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 33/33 [00:14<00:00,  2.23it/s, loss=0.117] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7953\n",
      "Test Metrics: Precision=0.9268, Recall=0.9268, F1=0.9268\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 260, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 260/260 [00:00<00:00, 8480.06 examples/s]\n",
      "Map: 100%|██████████| 52/52 [00:00<00:00, 5997.96 examples/s]\n",
      "Map: 100%|██████████| 936/936 [00:00<00:00, 9314.22 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 33/33 [00:15<00:00,  2.15it/s, loss=0.39] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.1132\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 33/33 [00:15<00:00,  2.18it/s, loss=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.7619\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 33/33 [00:15<00:00,  2.12it/s, loss=0.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6595\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 33/33 [00:15<00:00,  2.14it/s, loss=0.208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.0686\n",
      "Test Metrics: Precision=0.9239, Recall=0.9239, F1=0.9239\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 260, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 260/260 [00:00<00:00, 8204.76 examples/s]\n",
      "Map: 100%|██████████| 52/52 [00:00<00:00, 6534.75 examples/s]\n",
      "Map: 100%|██████████| 936/936 [00:00<00:00, 8977.50 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 33/33 [00:15<00:00,  2.12it/s, loss=0.514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.7551\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 33/33 [00:15<00:00,  2.12it/s, loss=0.141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.0836\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 33/33 [00:15<00:00,  2.18it/s, loss=0.155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7494\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 33/33 [00:15<00:00,  2.08it/s, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.2181\n",
      "Test Metrics: Precision=0.9085, Recall=0.9085, F1=0.9085\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 260, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 260/260 [00:00<00:00, 8863.41 examples/s]\n",
      "Map: 100%|██████████| 52/52 [00:00<00:00, 6670.25 examples/s]\n",
      "Map: 100%|██████████| 936/936 [00:00<00:00, 9678.71 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 33/33 [00:16<00:00,  2.04it/s, loss=0.511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.8083\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 33/33 [00:15<00:00,  2.07it/s, loss=0.232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.0707\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 33/33 [00:15<00:00,  2.14it/s, loss=0.114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0101\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 33/33 [00:15<00:00,  2.10it/s, loss=0.106] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.5965\n",
      "Test Metrics: Precision=0.9247, Recall=0.9247, F1=0.9247\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 260, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 260/260 [00:00<00:00, 8693.07 examples/s]\n",
      "Map: 100%|██████████| 52/52 [00:00<00:00, 6460.42 examples/s]\n",
      "Map: 100%|██████████| 936/936 [00:00<00:00, 9495.50 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 33/33 [00:16<00:00,  2.06it/s, loss=0.473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.0830\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 33/33 [00:17<00:00,  1.92it/s, loss=0.286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5676\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 33/33 [00:17<00:00,  1.94it/s, loss=0.582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.7814\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 33/33 [00:16<00:00,  1.99it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.4149\n",
      "Test Metrics: Precision=0.9126, Recall=0.9126, F1=0.9126\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 265, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 265/265 [00:00<00:00, 8651.95 examples/s]\n",
      "Map: 100%|██████████| 53/53 [00:00<00:00, 6452.21 examples/s]\n",
      "Map: 100%|██████████| 930/930 [00:00<00:00, 9516.51 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:14<00:00,  2.27it/s, loss=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.4514\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:15<00:00,  2.25it/s, loss=0.122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.2424\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:15<00:00,  2.23it/s, loss=0.263] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.2803\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:15<00:00,  2.24it/s, loss=0.248] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6598\n",
      "Test Metrics: Precision=0.9265, Recall=0.9265, F1=0.9265\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 265, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 265/265 [00:00<00:00, 8491.79 examples/s]\n",
      "Map: 100%|██████████| 53/53 [00:00<00:00, 6792.08 examples/s]\n",
      "Map: 100%|██████████| 930/930 [00:00<00:00, 9480.86 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:15<00:00,  2.18it/s, loss=0.0397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.1293\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:15<00:00,  2.21it/s, loss=0.429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.4283\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:15<00:00,  2.17it/s, loss=0.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5608\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:16<00:00,  2.12it/s, loss=0.0353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6154\n",
      "Test Metrics: Precision=0.9270, Recall=0.9270, F1=0.9270\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 265, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 265/265 [00:00<00:00, 8294.64 examples/s]\n",
      "Map: 100%|██████████| 53/53 [00:00<00:00, 6608.54 examples/s]\n",
      "Map: 100%|██████████| 930/930 [00:00<00:00, 4105.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:16<00:00,  2.12it/s, loss=0.00811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.5057\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:15<00:00,  2.15it/s, loss=0.0984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.1916\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:16<00:00,  2.07it/s, loss=0.225] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.2687\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:16<00:00,  2.12it/s, loss=0.166] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.4993\n",
      "Test Metrics: Precision=0.9292, Recall=0.9292, F1=0.9292\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 265, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 265/265 [00:00<00:00, 8867.03 examples/s]\n",
      "Map: 100%|██████████| 53/53 [00:00<00:00, 5630.37 examples/s]\n",
      "Map: 100%|██████████| 930/930 [00:00<00:00, 9405.33 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:16<00:00,  2.11it/s, loss=0.751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.3249\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:16<00:00,  2.09it/s, loss=0.218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.1850\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:16<00:00,  2.05it/s, loss=0.102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.5189\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:16<00:00,  2.08it/s, loss=0.145] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7354\n",
      "Test Metrics: Precision=0.9237, Recall=0.9237, F1=0.9237\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 265, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 265/265 [00:00<00:00, 8379.19 examples/s]\n",
      "Map: 100%|██████████| 53/53 [00:00<00:00, 6967.06 examples/s]\n",
      "Map: 100%|██████████| 930/930 [00:00<00:00, 9548.03 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:17<00:00,  1.96it/s, loss=0.528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.0294\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:17<00:00,  1.95it/s, loss=0.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.3291\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:16<00:00,  2.10it/s, loss=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4496\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:16<00:00,  2.01it/s, loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3007\n",
      "Test Metrics: Precision=0.9244, Recall=0.9244, F1=0.9244\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 270, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 270/270 [00:00<00:00, 8428.69 examples/s]\n",
      "Map: 100%|██████████| 54/54 [00:00<00:00, 5451.34 examples/s]\n",
      "Map: 100%|██████████| 924/924 [00:00<00:00, 9632.32 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:15<00:00,  2.17it/s, loss=0.287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.1883\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:15<00:00,  2.21it/s, loss=0.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.8082\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:15<00:00,  2.23it/s, loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.8049\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:14<00:00,  2.34it/s, loss=0.174] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.3031\n",
      "Test Metrics: Precision=0.9253, Recall=0.9253, F1=0.9253\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 270, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 270/270 [00:00<00:00, 8570.30 examples/s]\n",
      "Map: 100%|██████████| 54/54 [00:00<00:00, 6563.10 examples/s]\n",
      "Map: 100%|██████████| 924/924 [00:00<00:00, 9531.74 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:15<00:00,  2.17it/s, loss=0.317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.8125\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:15<00:00,  2.18it/s, loss=0.265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.9952\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:16<00:00,  2.12it/s, loss=0.139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.0581\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:15<00:00,  2.21it/s, loss=0.211] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1422\n",
      "Test Metrics: Precision=0.9273, Recall=0.9273, F1=0.9273\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 270, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 270/270 [00:00<00:00, 8733.28 examples/s]\n",
      "Map: 100%|██████████| 54/54 [00:00<00:00, 7206.94 examples/s]\n",
      "Map: 100%|██████████| 924/924 [00:00<00:00, 9111.21 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:15<00:00,  2.15it/s, loss=0.372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.6780\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:16<00:00,  2.09it/s, loss=0.334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.0196\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:16<00:00,  2.11it/s, loss=0.331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1751\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:16<00:00,  2.12it/s, loss=0.178] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3733\n",
      "Test Metrics: Precision=0.9269, Recall=0.9269, F1=0.9269\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 270, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 270/270 [00:00<00:00, 8116.26 examples/s]\n",
      "Map: 100%|██████████| 54/54 [00:00<00:00, 6461.43 examples/s]\n",
      "Map: 100%|██████████| 924/924 [00:00<00:00, 9139.08 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:17<00:00,  1.99it/s, loss=0.318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.8864\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:16<00:00,  2.00it/s, loss=0.34] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.2760\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:15<00:00,  2.14it/s, loss=0.124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.4526\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:16<00:00,  2.07it/s, loss=0.119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7550\n",
      "Test Metrics: Precision=0.9277, Recall=0.9277, F1=0.9277\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 270, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 270/270 [00:00<00:00, 8425.87 examples/s]\n",
      "Map: 100%|██████████| 54/54 [00:00<00:00, 6058.54 examples/s]\n",
      "Map: 100%|██████████| 924/924 [00:00<00:00, 9492.82 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:17<00:00,  1.96it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.8119\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:17<00:00,  1.97it/s, loss=0.206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.3248\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:17<00:00,  1.92it/s, loss=0.353] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3223\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:17<00:00,  1.90it/s, loss=0.214] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1728\n",
      "Test Metrics: Precision=0.9274, Recall=0.9274, F1=0.9274\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 275, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 275/275 [00:00<00:00, 8273.92 examples/s]\n",
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6739.31 examples/s]\n",
      "Map: 100%|██████████| 918/918 [00:00<00:00, 9194.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:15<00:00,  2.27it/s, loss=0.434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.0104\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:15<00:00,  2.27it/s, loss=0.0784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.4791\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:14<00:00,  2.35it/s, loss=0.274] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.7364\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:14<00:00,  2.37it/s, loss=0.0967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.9159\n",
      "Test Metrics: Precision=0.9283, Recall=0.9283, F1=0.9283\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 275, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 275/275 [00:00<00:00, 8474.21 examples/s]\n",
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6149.35 examples/s]\n",
      "Map: 100%|██████████| 918/918 [00:00<00:00, 9528.66 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:16<00:00,  2.17it/s, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.2879\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:15<00:00,  2.30it/s, loss=0.233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.9535\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:15<00:00,  2.20it/s, loss=0.28]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5624\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:15<00:00,  2.19it/s, loss=0.191] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0362\n",
      "Test Metrics: Precision=0.9301, Recall=0.9301, F1=0.9301\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 275, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 275/275 [00:00<00:00, 8800.54 examples/s]\n",
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6846.73 examples/s]\n",
      "Map: 100%|██████████| 918/918 [00:00<00:00, 9384.85 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:15<00:00,  2.20it/s, loss=0.256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.5696\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:16<00:00,  2.09it/s, loss=0.198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.7105\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:15<00:00,  2.24it/s, loss=0.192] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.6354\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:16<00:00,  2.16it/s, loss=0.198] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7492\n",
      "Test Metrics: Precision=0.9273, Recall=0.9273, F1=0.9273\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 275, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 275/275 [00:00<00:00, 8254.32 examples/s]\n",
      "Map: 100%|██████████| 55/55 [00:00<00:00, 7966.53 examples/s]\n",
      "Map: 100%|██████████| 918/918 [00:00<00:00, 9202.21 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:16<00:00,  2.09it/s, loss=0.514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.7032\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:16<00:00,  2.14it/s, loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.6590\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:16<00:00,  2.12it/s, loss=0.214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.5700\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:16<00:00,  2.13it/s, loss=0.0549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.1691\n",
      "Test Metrics: Precision=0.9295, Recall=0.9295, F1=0.9295\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 275, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 275/275 [00:00<00:00, 8311.90 examples/s]\n",
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6038.45 examples/s]\n",
      "Map: 100%|██████████| 918/918 [00:00<00:00, 9652.69 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:17<00:00,  1.97it/s, loss=0.323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.3138\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:17<00:00,  2.00it/s, loss=0.403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.2565\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:17<00:00,  1.97it/s, loss=0.146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9521\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:17<00:00,  1.99it/s, loss=0.134] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3382\n",
      "Test Metrics: Precision=0.9276, Recall=0.9276, F1=0.9276\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 280, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 280/280 [00:00<00:00, 8883.62 examples/s]\n",
      "Map: 100%|██████████| 56/56 [00:00<00:00, 7158.82 examples/s]\n",
      "Map: 100%|██████████| 912/912 [00:00<00:00, 9552.84 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:15<00:00,  2.23it/s, loss=0.512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.1759\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:15<00:00,  2.26it/s, loss=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.5067\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:16<00:00,  2.14it/s, loss=0.139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5867\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:15<00:00,  2.24it/s, loss=0.121] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7814\n",
      "Test Metrics: Precision=0.9269, Recall=0.9269, F1=0.9269\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 280, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 280/280 [00:00<00:00, 8691.57 examples/s]\n",
      "Map: 100%|██████████| 56/56 [00:00<00:00, 6746.16 examples/s]\n",
      "Map: 100%|██████████| 912/912 [00:00<00:00, 9632.27 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:16<00:00,  2.13it/s, loss=0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.0616\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:15<00:00,  2.19it/s, loss=0.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.7897\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:16<00:00,  2.11it/s, loss=0.147] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.4911\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:16<00:00,  2.15it/s, loss=0.136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8376\n",
      "Test Metrics: Precision=0.9262, Recall=0.9262, F1=0.9262\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 280, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 280/280 [00:00<00:00, 9206.40 examples/s]\n",
      "Map: 100%|██████████| 56/56 [00:00<00:00, 5751.81 examples/s]\n",
      "Map: 100%|██████████| 912/912 [00:00<00:00, 9660.78 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:16<00:00,  2.10it/s, loss=0.547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.5519\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:16<00:00,  2.13it/s, loss=0.134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.7245\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:16<00:00,  2.16it/s, loss=0.215] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.2609\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:16<00:00,  2.17it/s, loss=0.115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6606\n",
      "Test Metrics: Precision=0.9283, Recall=0.9283, F1=0.9283\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 280, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 280/280 [00:00<00:00, 8748.09 examples/s]\n",
      "Map: 100%|██████████| 56/56 [00:00<00:00, 6428.58 examples/s]\n",
      "Map: 100%|██████████| 912/912 [00:00<00:00, 9656.22 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:17<00:00,  2.05it/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.0976\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:16<00:00,  2.09it/s, loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.8600\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:17<00:00,  2.04it/s, loss=0.251] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.9682\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:16<00:00,  2.06it/s, loss=0.175] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2513\n",
      "Test Metrics: Precision=0.9248, Recall=0.9248, F1=0.9248\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 280, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 280/280 [00:00<00:00, 8609.44 examples/s]\n",
      "Map: 100%|██████████| 56/56 [00:00<00:00, 6366.72 examples/s]\n",
      "Map: 100%|██████████| 912/912 [00:00<00:00, 9445.30 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:17<00:00,  2.02it/s, loss=0.358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.0264\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:18<00:00,  1.94it/s, loss=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.5920\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:17<00:00,  1.95it/s, loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.2421\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:18<00:00,  1.85it/s, loss=0.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 8.6473\n",
      "Test Metrics: Precision=0.9152, Recall=0.9152, F1=0.9152\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 285, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 285/285 [00:00<00:00, 8529.87 examples/s]\n",
      "Map: 100%|██████████| 57/57 [00:00<00:00, 7707.63 examples/s]\n",
      "Map: 100%|██████████| 906/906 [00:00<00:00, 9438.68 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 36/36 [00:16<00:00,  2.23it/s, loss=0.356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.9975\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 36/36 [00:16<00:00,  2.17it/s, loss=0.16] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.6446\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 36/36 [00:15<00:00,  2.26it/s, loss=0.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1828\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 36/36 [00:16<00:00,  2.15it/s, loss=0.129] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5756\n",
      "Test Metrics: Precision=0.9275, Recall=0.9275, F1=0.9275\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 285, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 285/285 [00:00<00:00, 8893.58 examples/s]\n",
      "Map: 100%|██████████| 57/57 [00:00<00:00, 6418.65 examples/s]\n",
      "Map: 100%|██████████| 906/906 [00:00<00:00, 9560.11 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 36/36 [00:16<00:00,  2.17it/s, loss=0.317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.9437\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 36/36 [00:17<00:00,  2.11it/s, loss=0.297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.1476\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 36/36 [00:16<00:00,  2.21it/s, loss=0.169] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3996\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 36/36 [00:16<00:00,  2.17it/s, loss=0.234] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6664\n",
      "Test Metrics: Precision=0.9290, Recall=0.9290, F1=0.9290\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 285, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 285/285 [00:00<00:00, 8689.98 examples/s]\n",
      "Map: 100%|██████████| 57/57 [00:00<00:00, 4882.97 examples/s]\n",
      "Map: 100%|██████████| 906/906 [00:00<00:00, 4129.10 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 36/36 [00:16<00:00,  2.17it/s, loss=0.39] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.1807\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 36/36 [00:16<00:00,  2.16it/s, loss=0.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.9518\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 36/36 [00:16<00:00,  2.16it/s, loss=0.085] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3224\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 36/36 [00:16<00:00,  2.21it/s, loss=0.0903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.4874\n",
      "Test Metrics: Precision=0.9289, Recall=0.9289, F1=0.9289\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 285, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 285/285 [00:00<00:00, 8805.01 examples/s]\n",
      "Map: 100%|██████████| 57/57 [00:00<00:00, 6483.93 examples/s]\n",
      "Map: 100%|██████████| 906/906 [00:00<00:00, 9476.43 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 36/36 [00:17<00:00,  2.06it/s, loss=0.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.0585\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 36/36 [00:17<00:00,  2.11it/s, loss=0.175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.3799\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 36/36 [00:17<00:00,  2.09it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2925\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 36/36 [00:16<00:00,  2.12it/s, loss=0.319] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1697\n",
      "Test Metrics: Precision=0.9260, Recall=0.9260, F1=0.9260\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 285, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 285/285 [00:00<00:00, 8571.59 examples/s]\n",
      "Map: 100%|██████████| 57/57 [00:00<00:00, 6899.52 examples/s]\n",
      "Map: 100%|██████████| 906/906 [00:00<00:00, 9529.71 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 36/36 [00:18<00:00,  1.99it/s, loss=0.613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.3881\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 36/36 [00:18<00:00,  1.94it/s, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.1177\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 36/36 [00:17<00:00,  2.05it/s, loss=0.153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8807\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 36/36 [00:17<00:00,  2.07it/s, loss=0.104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2507\n",
      "Test Metrics: Precision=0.9318, Recall=0.9318, F1=0.9318\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 290, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 290/290 [00:00<00:00, 8772.35 examples/s]\n",
      "Map: 100%|██████████| 58/58 [00:00<00:00, 6730.94 examples/s]\n",
      "Map: 100%|██████████| 900/900 [00:00<00:00, 9516.67 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:16<00:00,  2.29it/s, loss=0.126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.1431\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:16<00:00,  2.29it/s, loss=0.268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.6394\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:16<00:00,  2.21it/s, loss=0.186] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.4963\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:16<00:00,  2.19it/s, loss=0.108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6612\n",
      "Test Metrics: Precision=0.9284, Recall=0.9284, F1=0.9284\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 290, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 290/290 [00:00<00:00, 8913.07 examples/s]\n",
      "Map: 100%|██████████| 58/58 [00:00<00:00, 6417.37 examples/s]\n",
      "Map: 100%|██████████| 900/900 [00:00<00:00, 9401.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:16<00:00,  2.24it/s, loss=0.753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.8883\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:16<00:00,  2.22it/s, loss=0.16] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8474\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:16<00:00,  2.19it/s, loss=0.136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2995\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:16<00:00,  2.26it/s, loss=0.125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2578\n",
      "Test Metrics: Precision=0.9267, Recall=0.9267, F1=0.9267\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 290, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 290/290 [00:00<00:00, 8829.02 examples/s]\n",
      "Map: 100%|██████████| 58/58 [00:00<00:00, 6162.16 examples/s]\n",
      "Map: 100%|██████████| 900/900 [00:00<00:00, 9545.67 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:17<00:00,  2.18it/s, loss=0.0689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.6717\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:17<00:00,  2.14it/s, loss=0.17] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.7218\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:16<00:00,  2.29it/s, loss=0.372] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.7187\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:17<00:00,  2.16it/s, loss=0.0383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5915\n",
      "Test Metrics: Precision=0.9286, Recall=0.9286, F1=0.9286\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 290, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 290/290 [00:00<00:00, 8579.67 examples/s]\n",
      "Map: 100%|██████████| 58/58 [00:00<00:00, 7286.58 examples/s]\n",
      "Map: 100%|██████████| 900/900 [00:00<00:00, 9244.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:17<00:00,  2.08it/s, loss=0.0874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.4424\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:16<00:00,  2.19it/s, loss=0.126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.9689\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:17<00:00,  2.06it/s, loss=0.0111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.7277\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:18<00:00,  2.04it/s, loss=0.12]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0644\n",
      "Test Metrics: Precision=0.9264, Recall=0.9264, F1=0.9264\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 290, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 290/290 [00:00<00:00, 8538.53 examples/s]\n",
      "Map: 100%|██████████| 58/58 [00:00<00:00, 7830.48 examples/s]\n",
      "Map: 100%|██████████| 900/900 [00:00<00:00, 9494.82 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:18<00:00,  1.97it/s, loss=0.287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.1442\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:17<00:00,  2.11it/s, loss=0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.5971\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:18<00:00,  2.04it/s, loss=0.169] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.6855\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:17<00:00,  2.07it/s, loss=0.118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.5760\n",
      "Test Metrics: Precision=0.9319, Recall=0.9319, F1=0.9319\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 295, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 295/295 [00:00<00:00, 8746.10 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 5705.35 examples/s]\n",
      "Map: 100%|██████████| 894/894 [00:00<00:00, 9651.11 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:16<00:00,  2.24it/s, loss=0.285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.3030\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:16<00:00,  2.18it/s, loss=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.2021\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:16<00:00,  2.24it/s, loss=0.158] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.2823\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:16<00:00,  2.19it/s, loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.6452\n",
      "Test Metrics: Precision=0.9235, Recall=0.9235, F1=0.9235\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 295, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 295/295 [00:00<00:00, 8910.49 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 7247.02 examples/s]\n",
      "Map: 100%|██████████| 894/894 [00:00<00:00, 9490.98 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:17<00:00,  2.14it/s, loss=0.285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.3343\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:17<00:00,  2.13it/s, loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.6277\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:17<00:00,  2.17it/s, loss=0.123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3815\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:17<00:00,  2.15it/s, loss=0.0883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2465\n",
      "Test Metrics: Precision=0.9285, Recall=0.9285, F1=0.9285\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 295, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 295/295 [00:00<00:00, 8638.69 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 7689.27 examples/s]\n",
      "Map: 100%|██████████| 894/894 [00:00<00:00, 9367.69 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:17<00:00,  2.18it/s, loss=0.387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.6696\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:17<00:00,  2.09it/s, loss=0.138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.4714\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:17<00:00,  2.08it/s, loss=0.193] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5189\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:16<00:00,  2.19it/s, loss=0.0744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.4321\n",
      "Test Metrics: Precision=0.9314, Recall=0.9314, F1=0.9314\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 295, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 295/295 [00:00<00:00, 9165.60 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 6688.58 examples/s]\n",
      "Map: 100%|██████████| 894/894 [00:00<00:00, 9580.91 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:17<00:00,  2.06it/s, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.7787\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:17<00:00,  2.15it/s, loss=0.153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.5044\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:17<00:00,  2.06it/s, loss=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5520\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:17<00:00,  2.07it/s, loss=0.0961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8320\n",
      "Test Metrics: Precision=0.9289, Recall=0.9289, F1=0.9289\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 295, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 295/295 [00:00<00:00, 8407.08 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 7048.05 examples/s]\n",
      "Map: 100%|██████████| 894/894 [00:00<00:00, 9457.28 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:18<00:00,  1.99it/s, loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.8685\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:18<00:00,  2.02it/s, loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.4585\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:18<00:00,  2.05it/s, loss=0.169] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1355\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:18<00:00,  2.00it/s, loss=0.119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0948\n",
      "Test Metrics: Precision=0.9327, Recall=0.9327, F1=0.9327\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 300, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 8479.51 examples/s]\n",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 5879.45 examples/s]\n",
      "Map: 100%|██████████| 888/888 [00:00<00:00, 9704.10 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 38/38 [00:16<00:00,  2.26it/s, loss=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.7505\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 38/38 [00:17<00:00,  2.23it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.6942\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 38/38 [00:16<00:00,  2.29it/s, loss=0.128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.9370\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 38/38 [00:16<00:00,  2.31it/s, loss=0.204] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9397\n",
      "Test Metrics: Precision=0.9290, Recall=0.9290, F1=0.9290\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 300, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 8792.41 examples/s]\n",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 7375.46 examples/s]\n",
      "Map: 100%|██████████| 888/888 [00:00<00:00, 9236.54 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 38/38 [00:17<00:00,  2.23it/s, loss=0.313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.1125\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 38/38 [00:16<00:00,  2.26it/s, loss=0.314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.4505\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 38/38 [00:17<00:00,  2.18it/s, loss=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2861\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 38/38 [00:17<00:00,  2.12it/s, loss=0.0862]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2517\n",
      "Test Metrics: Precision=0.9305, Recall=0.9305, F1=0.9305\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 300, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 8727.28 examples/s]\n",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 7047.28 examples/s]\n",
      "Map: 100%|██████████| 888/888 [00:00<00:00, 4024.92 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 38/38 [00:17<00:00,  2.16it/s, loss=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.2014\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 38/38 [00:17<00:00,  2.16it/s, loss=0.348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.1551\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 38/38 [00:17<00:00,  2.22it/s, loss=0.149] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.9815\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 38/38 [00:17<00:00,  2.16it/s, loss=0.075] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1798\n",
      "Test Metrics: Precision=0.9292, Recall=0.9292, F1=0.9292\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 300, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 9049.07 examples/s]\n",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 6145.35 examples/s]\n",
      "Map: 100%|██████████| 888/888 [00:00<00:00, 9549.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 38/38 [00:18<00:00,  2.09it/s, loss=0.468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.0947\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 38/38 [00:18<00:00,  2.09it/s, loss=0.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.4687\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 38/38 [00:18<00:00,  2.06it/s, loss=0.236] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3891\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 38/38 [00:17<00:00,  2.14it/s, loss=0.108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6627\n",
      "Test Metrics: Precision=0.9295, Recall=0.9295, F1=0.9295\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 300, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 8756.86 examples/s]\n",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 6669.09 examples/s]\n",
      "Map: 100%|██████████| 888/888 [00:00<00:00, 9339.58 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 38/38 [00:18<00:00,  2.06it/s, loss=0.498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.6316\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 38/38 [00:19<00:00,  2.00it/s, loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.3195\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 38/38 [00:19<00:00,  1.98it/s, loss=0.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3163\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 38/38 [00:18<00:00,  2.03it/s, loss=0.099] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0997\n",
      "Test Metrics: Precision=0.9340, Recall=0.9340, F1=0.9340\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 305, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 305/305 [00:00<00:00, 8574.66 examples/s]\n",
      "Map: 100%|██████████| 61/61 [00:00<00:00, 6951.38 examples/s]\n",
      "Map: 100%|██████████| 882/882 [00:00<00:00, 9831.55 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:17<00:00,  2.22it/s, loss=0.218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.6968\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:17<00:00,  2.23it/s, loss=0.445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.0382\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:17<00:00,  2.21it/s, loss=0.0488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1271\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:16<00:00,  2.30it/s, loss=0.0168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3351\n",
      "Test Metrics: Precision=0.9304, Recall=0.9304, F1=0.9304\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 305, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 305/305 [00:00<00:00, 9044.88 examples/s]\n",
      "Map: 100%|██████████| 61/61 [00:00<00:00, 6849.77 examples/s]\n",
      "Map: 100%|██████████| 882/882 [00:00<00:00, 9360.32 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:16<00:00,  2.31it/s, loss=1.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.2218\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:16<00:00,  2.32it/s, loss=0.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8107\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:17<00:00,  2.18it/s, loss=0.189] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1151\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:17<00:00,  2.21it/s, loss=0.0942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1937\n",
      "Test Metrics: Precision=0.9298, Recall=0.9298, F1=0.9298\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 305, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 305/305 [00:00<00:00, 8477.83 examples/s]\n",
      "Map: 100%|██████████| 61/61 [00:00<00:00, 8183.35 examples/s]\n",
      "Map: 100%|██████████| 882/882 [00:00<00:00, 9222.59 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:18<00:00,  2.10it/s, loss=1.12] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.3485\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:18<00:00,  2.13it/s, loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1381\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:17<00:00,  2.20it/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.7579\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:17<00:00,  2.18it/s, loss=0.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 8.1715\n",
      "Test Metrics: Precision=0.9224, Recall=0.9224, F1=0.9224\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 305, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 305/305 [00:00<00:00, 8829.14 examples/s]\n",
      "Map: 100%|██████████| 61/61 [00:00<00:00, 7534.16 examples/s]\n",
      "Map: 100%|██████████| 882/882 [00:00<00:00, 9463.64 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:18<00:00,  2.11it/s, loss=0.373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.6794\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:18<00:00,  2.09it/s, loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.9214\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:18<00:00,  2.15it/s, loss=0.117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8911\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:18<00:00,  2.10it/s, loss=0.0562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8111\n",
      "Test Metrics: Precision=0.9273, Recall=0.9273, F1=0.9273\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 305, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 305/305 [00:00<00:00, 8407.52 examples/s]\n",
      "Map: 100%|██████████| 61/61 [00:00<00:00, 7912.80 examples/s]\n",
      "Map: 100%|██████████| 882/882 [00:00<00:00, 9273.04 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:19<00:00,  1.96it/s, loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.3619\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:19<00:00,  1.99it/s, loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.7104\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:19<00:00,  2.00it/s, loss=0.527] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6345\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:19<00:00,  1.97it/s, loss=0.0141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6196\n",
      "Test Metrics: Precision=0.9309, Recall=0.9309, F1=0.9309\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 310, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 310/310 [00:00<00:00, 8741.78 examples/s]\n",
      "Map: 100%|██████████| 62/62 [00:00<00:00, 7075.14 examples/s]\n",
      "Map: 100%|██████████| 876/876 [00:00<00:00, 9205.39 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:17<00:00,  2.20it/s, loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.2455\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:16<00:00,  2.29it/s, loss=0.315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.2215\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:17<00:00,  2.18it/s, loss=0.242] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0329\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:17<00:00,  2.17it/s, loss=0.108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9661\n",
      "Test Metrics: Precision=0.9296, Recall=0.9296, F1=0.9296\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 310, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 310/310 [00:00<00:00, 8903.64 examples/s]\n",
      "Map: 100%|██████████| 62/62 [00:00<00:00, 6660.52 examples/s]\n",
      "Map: 100%|██████████| 876/876 [00:00<00:00, 9207.76 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:17<00:00,  2.20it/s, loss=0.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.8647\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:17<00:00,  2.29it/s, loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.1925\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:16<00:00,  2.32it/s, loss=0.122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6662\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:17<00:00,  2.24it/s, loss=0.0933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7965\n",
      "Test Metrics: Precision=0.9315, Recall=0.9315, F1=0.9315\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 310, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 310/310 [00:00<00:00, 8787.80 examples/s]\n",
      "Map: 100%|██████████| 62/62 [00:00<00:00, 6082.82 examples/s]\n",
      "Map: 100%|██████████| 876/876 [00:00<00:00, 9390.42 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:18<00:00,  2.11it/s, loss=0.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.3316\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:18<00:00,  2.14it/s, loss=0.282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.1631\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:17<00:00,  2.21it/s, loss=0.118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.7044\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:18<00:00,  2.12it/s, loss=0.131] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.4574\n",
      "Test Metrics: Precision=0.9318, Recall=0.9318, F1=0.9318\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 310, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 310/310 [00:00<00:00, 8728.86 examples/s]\n",
      "Map: 100%|██████████| 62/62 [00:00<00:00, 6301.72 examples/s]\n",
      "Map: 100%|██████████| 876/876 [00:00<00:00, 9434.74 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:18<00:00,  2.13it/s, loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.1185\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:18<00:00,  2.11it/s, loss=0.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.9535\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:18<00:00,  2.06it/s, loss=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.7067\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:19<00:00,  2.04it/s, loss=0.109] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9508\n",
      "Test Metrics: Precision=0.9274, Recall=0.9274, F1=0.9274\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 310, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 310/310 [00:00<00:00, 8760.51 examples/s]\n",
      "Map: 100%|██████████| 62/62 [00:00<00:00, 7300.38 examples/s]\n",
      "Map: 100%|██████████| 876/876 [00:00<00:00, 9477.19 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:20<00:00,  1.92it/s, loss=0.497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.2757\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:19<00:00,  1.96it/s, loss=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.6114\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:19<00:00,  1.96it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8607\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:19<00:00,  1.96it/s, loss=0.0904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9675\n",
      "Test Metrics: Precision=0.9304, Recall=0.9304, F1=0.9304\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 315, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 315/315 [00:00<00:00, 8921.64 examples/s]\n",
      "Map: 100%|██████████| 63/63 [00:00<00:00, 7281.78 examples/s]\n",
      "Map: 100%|██████████| 870/870 [00:00<00:00, 9548.90 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:17<00:00,  2.24it/s, loss=0.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.7787\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:18<00:00,  2.21it/s, loss=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.3084\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:17<00:00,  2.23it/s, loss=0.12]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5307\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:17<00:00,  2.23it/s, loss=0.167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.2791\n",
      "Test Metrics: Precision=0.9276, Recall=0.9276, F1=0.9276\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 315, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 315/315 [00:00<00:00, 8907.87 examples/s]\n",
      "Map: 100%|██████████| 63/63 [00:00<00:00, 6489.86 examples/s]\n",
      "Map: 100%|██████████| 870/870 [00:00<00:00, 9473.68 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:17<00:00,  2.25it/s, loss=0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.9842\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:18<00:00,  2.20it/s, loss=0.411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.9918\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:18<00:00,  2.22it/s, loss=0.247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1539\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:18<00:00,  2.22it/s, loss=0.128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9879\n",
      "Test Metrics: Precision=0.9279, Recall=0.9279, F1=0.9279\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 315, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 315/315 [00:00<00:00, 8866.56 examples/s]\n",
      "Map: 100%|██████████| 63/63 [00:00<00:00, 5855.89 examples/s]\n",
      "Map: 100%|██████████| 870/870 [00:00<00:00, 9538.07 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:18<00:00,  2.20it/s, loss=0.317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.9740\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:18<00:00,  2.21it/s, loss=0.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.6951\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:18<00:00,  2.15it/s, loss=0.1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8741\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:18<00:00,  2.13it/s, loss=0.0883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0766\n",
      "Test Metrics: Precision=0.9309, Recall=0.9309, F1=0.9309\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 315, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 315/315 [00:00<00:00, 8808.10 examples/s]\n",
      "Map: 100%|██████████| 63/63 [00:00<00:00, 6786.55 examples/s]\n",
      "Map: 100%|██████████| 870/870 [00:00<00:00, 9240.68 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:18<00:00,  2.11it/s, loss=0.281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.3676\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:18<00:00,  2.12it/s, loss=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8361\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:18<00:00,  2.13it/s, loss=0.16]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4251\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:19<00:00,  2.10it/s, loss=0.112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8150\n",
      "Test Metrics: Precision=0.9287, Recall=0.9287, F1=0.9287\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 315, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 315/315 [00:00<00:00, 8803.81 examples/s]\n",
      "Map: 100%|██████████| 63/63 [00:00<00:00, 7289.21 examples/s]\n",
      "Map: 100%|██████████| 870/870 [00:00<00:00, 9450.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:19<00:00,  2.10it/s, loss=0.499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.2784\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:20<00:00,  1.96it/s, loss=0.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8815\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:19<00:00,  2.00it/s, loss=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0004\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:20<00:00,  1.96it/s, loss=0.126] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5435\n",
      "Test Metrics: Precision=0.9300, Recall=0.9300, F1=0.9300\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 320, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 320/320 [00:00<00:00, 8855.11 examples/s]\n",
      "Map: 100%|██████████| 64/64 [00:00<00:00, 6212.06 examples/s]\n",
      "Map: 100%|██████████| 864/864 [00:00<00:00, 4107.21 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:17<00:00,  2.26it/s, loss=0.411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.5455\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:18<00:00,  2.14it/s, loss=0.522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5412\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:17<00:00,  2.23it/s, loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.4509\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:17<00:00,  2.23it/s, loss=0.152] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.0495\n",
      "Test Metrics: Precision=0.9271, Recall=0.9271, F1=0.9271\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 320, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 320/320 [00:00<00:00, 9084.85 examples/s]\n",
      "Map: 100%|██████████| 64/64 [00:00<00:00, 8073.73 examples/s]\n",
      "Map: 100%|██████████| 864/864 [00:00<00:00, 9423.74 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:18<00:00,  2.17it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.9014\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:18<00:00,  2.17it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.2555\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:17<00:00,  2.31it/s, loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4905\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:18<00:00,  2.19it/s, loss=0.186] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0957\n",
      "Test Metrics: Precision=0.9288, Recall=0.9288, F1=0.9288\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 320, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 320/320 [00:00<00:00, 9089.40 examples/s]\n",
      "Map: 100%|██████████| 64/64 [00:00<00:00, 7467.53 examples/s]\n",
      "Map: 100%|██████████| 864/864 [00:00<00:00, 9613.48 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:19<00:00,  2.08it/s, loss=0.236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.5892\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:18<00:00,  2.17it/s, loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.1623\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:18<00:00,  2.14it/s, loss=0.103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.6820\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:18<00:00,  2.12it/s, loss=0.0813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6331\n",
      "Test Metrics: Precision=0.9302, Recall=0.9302, F1=0.9302\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 320, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 320/320 [00:00<00:00, 8834.65 examples/s]\n",
      "Map: 100%|██████████| 64/64 [00:00<00:00, 7047.95 examples/s]\n",
      "Map: 100%|██████████| 864/864 [00:00<00:00, 9229.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:19<00:00,  2.07it/s, loss=0.329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.0009\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:18<00:00,  2.16it/s, loss=0.137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.1806\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:19<00:00,  2.10it/s, loss=0.125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.0829\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:19<00:00,  2.05it/s, loss=0.145] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.4025\n",
      "Test Metrics: Precision=0.9296, Recall=0.9296, F1=0.9296\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 320, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 320/320 [00:00<00:00, 8683.30 examples/s]\n",
      "Map: 100%|██████████| 64/64 [00:00<00:00, 7256.39 examples/s]\n",
      "Map: 100%|██████████| 864/864 [00:00<00:00, 9629.83 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:20<00:00,  1.99it/s, loss=0.475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.9367\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:20<00:00,  1.95it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.3418\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:20<00:00,  1.96it/s, loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2550\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:20<00:00,  2.00it/s, loss=0.0697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.0433\n",
      "Test Metrics: Precision=0.9288, Recall=0.9288, F1=0.9288\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 325, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 325/325 [00:00<00:00, 8657.44 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6950.23 examples/s]\n",
      "Map: 100%|██████████| 858/858 [00:00<00:00, 9391.70 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 41/41 [00:18<00:00,  2.27it/s, loss=0.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.8862\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 41/41 [00:18<00:00,  2.25it/s, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.5236\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 41/41 [00:18<00:00,  2.24it/s, loss=0.202] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.0890\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 41/41 [00:17<00:00,  2.29it/s, loss=0.0418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9055\n",
      "Test Metrics: Precision=0.9332, Recall=0.9332, F1=0.9332\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 325, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 325/325 [00:00<00:00, 9101.19 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 7670.20 examples/s]\n",
      "Map: 100%|██████████| 858/858 [00:00<00:00, 9192.07 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 41/41 [00:18<00:00,  2.21it/s, loss=0.546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.8754\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 41/41 [00:18<00:00,  2.18it/s, loss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.6548\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 41/41 [00:18<00:00,  2.26it/s, loss=0.0901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.9528\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 41/41 [00:18<00:00,  2.16it/s, loss=0.152] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0024\n",
      "Test Metrics: Precision=0.9317, Recall=0.9317, F1=0.9317\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 325, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 325/325 [00:00<00:00, 9128.55 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6591.79 examples/s]\n",
      "Map: 100%|██████████| 858/858 [00:00<00:00, 9570.41 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 41/41 [00:19<00:00,  2.11it/s, loss=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.5951\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 41/41 [00:19<00:00,  2.14it/s, loss=0.192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.5573\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 41/41 [00:19<00:00,  2.12it/s, loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.5234\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 41/41 [00:19<00:00,  2.14it/s, loss=0.146] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5951\n",
      "Test Metrics: Precision=0.9299, Recall=0.9299, F1=0.9299\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 325, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 325/325 [00:00<00:00, 8521.76 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 8343.94 examples/s]\n",
      "Map: 100%|██████████| 858/858 [00:00<00:00, 9162.60 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 41/41 [00:19<00:00,  2.05it/s, loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.9734\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 41/41 [00:19<00:00,  2.12it/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.5907\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 41/41 [00:19<00:00,  2.12it/s, loss=0.157] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.5336\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 41/41 [00:18<00:00,  2.20it/s, loss=0.086] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5491\n",
      "Test Metrics: Precision=0.9256, Recall=0.9256, F1=0.9256\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 325, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 325/325 [00:00<00:00, 8723.87 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6017.52 examples/s]\n",
      "Map: 100%|██████████| 858/858 [00:00<00:00, 9727.91 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 41/41 [00:20<00:00,  1.99it/s, loss=0.545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.6448\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 41/41 [00:19<00:00,  2.06it/s, loss=0.435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.0209\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 41/41 [00:21<00:00,  1.94it/s, loss=0.159] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4728\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 41/41 [00:19<00:00,  2.07it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3252\n",
      "Test Metrics: Precision=0.9283, Recall=0.9283, F1=0.9283\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 330, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 330/330 [00:00<00:00, 8543.53 examples/s]\n",
      "Map: 100%|██████████| 66/66 [00:00<00:00, 7946.95 examples/s]\n",
      "Map: 100%|██████████| 852/852 [00:00<00:00, 9392.38 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:19<00:00,  2.16it/s, loss=0.506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.1919\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:19<00:00,  2.21it/s, loss=0.0842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.5469\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:19<00:00,  2.14it/s, loss=0.0656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3335\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:18<00:00,  2.21it/s, loss=0.0547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.4023\n",
      "Test Metrics: Precision=0.9332, Recall=0.9332, F1=0.9332\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 330, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 330/330 [00:00<00:00, 9079.12 examples/s]\n",
      "Map: 100%|██████████| 66/66 [00:00<00:00, 7378.83 examples/s]\n",
      "Map: 100%|██████████| 852/852 [00:00<00:00, 3830.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s, loss=0.17] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.1574\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:18<00:00,  2.26it/s, loss=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.1026\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:19<00:00,  2.20it/s, loss=0.145] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8568\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:19<00:00,  2.20it/s, loss=0.0415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9481\n",
      "Test Metrics: Precision=0.9310, Recall=0.9310, F1=0.9310\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 330, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 330/330 [00:00<00:00, 9027.13 examples/s]\n",
      "Map: 100%|██████████| 66/66 [00:00<00:00, 6896.81 examples/s]\n",
      "Map: 100%|██████████| 852/852 [00:00<00:00, 9368.30 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s, loss=0.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.5909\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:19<00:00,  2.14it/s, loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.7004\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s, loss=0.34] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1813\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s, loss=0.064] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9995\n",
      "Test Metrics: Precision=0.9307, Recall=0.9307, F1=0.9307\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 330, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 330/330 [00:00<00:00, 8629.61 examples/s]\n",
      "Map: 100%|██████████| 66/66 [00:00<00:00, 7828.07 examples/s]\n",
      "Map: 100%|██████████| 852/852 [00:00<00:00, 9439.47 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:20<00:00,  2.07it/s, loss=0.0403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.6114\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:20<00:00,  2.09it/s, loss=1.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9588\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:19<00:00,  2.14it/s, loss=0.152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4670\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:19<00:00,  2.16it/s, loss=0.0853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.2113\n",
      "Test Metrics: Precision=0.9272, Recall=0.9272, F1=0.9272\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 330, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 330/330 [00:00<00:00, 8523.64 examples/s]\n",
      "Map: 100%|██████████| 66/66 [00:00<00:00, 7090.78 examples/s]\n",
      "Map: 100%|██████████| 852/852 [00:00<00:00, 9554.45 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:21<00:00,  1.95it/s, loss=0.308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.1571\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:21<00:00,  1.94it/s, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.7748\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:21<00:00,  1.97it/s, loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9912\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:21<00:00,  1.94it/s, loss=0.0254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.5463\n",
      "Test Metrics: Precision=0.9278, Recall=0.9278, F1=0.9278\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 335, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 335/335 [00:00<00:00, 8582.02 examples/s]\n",
      "Map: 100%|██████████| 67/67 [00:00<00:00, 7129.37 examples/s]\n",
      "Map: 100%|██████████| 846/846 [00:00<00:00, 9367.50 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:19<00:00,  2.15it/s, loss=0.283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.2619\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s, loss=0.196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.2163\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:19<00:00,  2.17it/s, loss=0.101] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.5784\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:19<00:00,  2.21it/s, loss=0.0509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6596\n",
      "Test Metrics: Precision=0.9315, Recall=0.9315, F1=0.9315\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 335, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 335/335 [00:00<00:00, 9311.29 examples/s]\n",
      "Map: 100%|██████████| 67/67 [00:00<00:00, 7049.25 examples/s]\n",
      "Map: 100%|██████████| 846/846 [00:00<00:00, 9375.37 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:19<00:00,  2.20it/s, loss=0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.9739\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:19<00:00,  2.17it/s, loss=0.334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.6396\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s, loss=0.203] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.6249\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:19<00:00,  2.18it/s, loss=0.129] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.5217\n",
      "Test Metrics: Precision=0.9339, Recall=0.9339, F1=0.9339\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 335, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 335/335 [00:00<00:00, 8834.50 examples/s]\n",
      "Map: 100%|██████████| 67/67 [00:00<00:00, 7670.13 examples/s]\n",
      "Map: 100%|██████████| 846/846 [00:00<00:00, 9258.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:19<00:00,  2.14it/s, loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.2472\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:19<00:00,  2.18it/s, loss=0.206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.1594\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:19<00:00,  2.20it/s, loss=0.131] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4279\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:19<00:00,  2.17it/s, loss=0.215] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9828\n",
      "Test Metrics: Precision=0.9293, Recall=0.9293, F1=0.9293\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 335, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 335/335 [00:00<00:00, 9076.41 examples/s]\n",
      "Map: 100%|██████████| 67/67 [00:00<00:00, 7615.26 examples/s]\n",
      "Map: 100%|██████████| 846/846 [00:00<00:00, 9284.95 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:20<00:00,  2.08it/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.9100\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:20<00:00,  2.07it/s, loss=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.3890\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:19<00:00,  2.13it/s, loss=0.18]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.9010\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:20<00:00,  2.06it/s, loss=0.0716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9969\n",
      "Test Metrics: Precision=0.9288, Recall=0.9288, F1=0.9288\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 335, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 335/335 [00:00<00:00, 8586.22 examples/s]\n",
      "Map: 100%|██████████| 67/67 [00:00<00:00, 7576.66 examples/s]\n",
      "Map: 100%|██████████| 846/846 [00:00<00:00, 9559.76 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:22<00:00,  1.91it/s, loss=0.47] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.0465\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:21<00:00,  1.97it/s, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8186\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:21<00:00,  1.95it/s, loss=0.124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5063\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:20<00:00,  2.01it/s, loss=0.136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1327\n",
      "Test Metrics: Precision=0.9315, Recall=0.9315, F1=0.9315\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 340, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 8664.86 examples/s]\n",
      "Map: 100%|██████████| 68/68 [00:00<00:00, 7244.05 examples/s]\n",
      "Map: 100%|██████████| 840/840 [00:00<00:00, 9228.32 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 43/43 [00:19<00:00,  2.25it/s, loss=0.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.5375\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 43/43 [00:19<00:00,  2.21it/s, loss=0.273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.5025\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 43/43 [00:19<00:00,  2.22it/s, loss=0.159] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9596\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 43/43 [00:20<00:00,  2.15it/s, loss=0.159] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8652\n",
      "Test Metrics: Precision=0.9292, Recall=0.9292, F1=0.9292\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 340, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 9172.06 examples/s]\n",
      "Map: 100%|██████████| 68/68 [00:00<00:00, 6980.24 examples/s]\n",
      "Map: 100%|██████████| 840/840 [00:00<00:00, 9226.72 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 43/43 [00:19<00:00,  2.19it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.1301\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 43/43 [00:19<00:00,  2.21it/s, loss=0.328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8835\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 43/43 [00:19<00:00,  2.19it/s, loss=0.106] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.5255\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 43/43 [00:19<00:00,  2.17it/s, loss=0.238] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9497\n",
      "Test Metrics: Precision=0.9313, Recall=0.9313, F1=0.9313\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 340, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 8728.56 examples/s]\n",
      "Map: 100%|██████████| 68/68 [00:00<00:00, 7166.15 examples/s]\n",
      "Map: 100%|██████████| 840/840 [00:00<00:00, 9332.28 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 43/43 [00:19<00:00,  2.17it/s, loss=0.408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.3584\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 43/43 [00:19<00:00,  2.18it/s, loss=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8212\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 43/43 [00:19<00:00,  2.18it/s, loss=0.128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1586\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 43/43 [00:19<00:00,  2.17it/s, loss=0.0845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9536\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 340, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 8748.43 examples/s]\n",
      "Map: 100%|██████████| 68/68 [00:00<00:00, 6605.36 examples/s]\n",
      "Map: 100%|██████████| 840/840 [00:00<00:00, 9304.01 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 43/43 [00:19<00:00,  2.15it/s, loss=0.515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.5867\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 43/43 [00:20<00:00,  2.12it/s, loss=0.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.4086\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 43/43 [00:21<00:00,  2.03it/s, loss=0.173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.5080\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 43/43 [00:20<00:00,  2.12it/s, loss=0.166] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.2052\n",
      "Test Metrics: Precision=0.9236, Recall=0.9236, F1=0.9236\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 340, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 8336.63 examples/s]\n",
      "Map: 100%|██████████| 68/68 [00:00<00:00, 8341.75 examples/s]\n",
      "Map: 100%|██████████| 840/840 [00:00<00:00, 9584.84 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 43/43 [00:21<00:00,  2.03it/s, loss=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.8933\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 43/43 [00:22<00:00,  1.95it/s, loss=0.196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.5281\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 43/43 [00:21<00:00,  2.01it/s, loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5481\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 43/43 [00:22<00:00,  1.94it/s, loss=0.0977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.2695\n",
      "Test Metrics: Precision=0.9328, Recall=0.9328, F1=0.9328\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 345, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 345/345 [00:00<00:00, 8701.88 examples/s]\n",
      "Map: 100%|██████████| 69/69 [00:00<00:00, 6880.65 examples/s]\n",
      "Map: 100%|██████████| 834/834 [00:00<00:00, 9528.74 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:19<00:00,  2.28it/s, loss=0.129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.8312\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:19<00:00,  2.25it/s, loss=0.652] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.3217\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:19<00:00,  2.24it/s, loss=0.0678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.6523\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:20<00:00,  2.17it/s, loss=0.0843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.5174\n",
      "Test Metrics: Precision=0.9306, Recall=0.9306, F1=0.9306\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 345, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 345/345 [00:00<00:00, 9004.18 examples/s]\n",
      "Map: 100%|██████████| 69/69 [00:00<00:00, 6164.55 examples/s]\n",
      "Map: 100%|██████████| 834/834 [00:00<00:00, 8880.35 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:19<00:00,  2.25it/s, loss=0.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.3270\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:19<00:00,  2.21it/s, loss=0.385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.3111\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:20<00:00,  2.16it/s, loss=0.00394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1548\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:20<00:00,  2.17it/s, loss=0.0972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8885\n",
      "Test Metrics: Precision=0.9295, Recall=0.9295, F1=0.9295\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 345, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 345/345 [00:00<00:00, 8860.45 examples/s]\n",
      "Map: 100%|██████████| 69/69 [00:00<00:00, 6914.18 examples/s]\n",
      "Map: 100%|██████████| 834/834 [00:00<00:00, 9217.76 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:19<00:00,  2.20it/s, loss=0.152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.3876\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:19<00:00,  2.20it/s, loss=0.0598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.3099\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:20<00:00,  2.19it/s, loss=0.0445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1267\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:20<00:00,  2.20it/s, loss=0.0294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7745\n",
      "Test Metrics: Precision=0.9338, Recall=0.9338, F1=0.9338\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 345, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 345/345 [00:00<00:00, 8893.50 examples/s]\n",
      "Map: 100%|██████████| 69/69 [00:00<00:00, 7567.38 examples/s]\n",
      "Map: 100%|██████████| 834/834 [00:00<00:00, 9079.92 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:20<00:00,  2.13it/s, loss=0.283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.2465\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:20<00:00,  2.15it/s, loss=0.0873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.5608\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:20<00:00,  2.15it/s, loss=0.533] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5909\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.0973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8665\n",
      "Test Metrics: Precision=0.9289, Recall=0.9289, F1=0.9289\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 345, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 345/345 [00:00<00:00, 8672.15 examples/s]\n",
      "Map: 100%|██████████| 69/69 [00:00<00:00, 8092.13 examples/s]\n",
      "Map: 100%|██████████| 834/834 [00:00<00:00, 9239.29 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:21<00:00,  2.05it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.0863\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:21<00:00,  2.04it/s, loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.6638\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:21<00:00,  2.03it/s, loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.7433\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:22<00:00,  1.97it/s, loss=0.0337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2890\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 350, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 350/350 [00:00<00:00, 8711.07 examples/s]\n",
      "Map: 100%|██████████| 70/70 [00:00<00:00, 6832.54 examples/s]\n",
      "Map: 100%|██████████| 828/828 [00:00<00:00, 9240.50 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:19<00:00,  2.23it/s, loss=0.331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.6860\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:20<00:00,  2.15it/s, loss=0.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.7540\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:19<00:00,  2.26it/s, loss=0.21]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1351\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:20<00:00,  2.18it/s, loss=0.112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0522\n",
      "Test Metrics: Precision=0.9316, Recall=0.9316, F1=0.9316\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 350, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 350/350 [00:00<00:00, 9034.89 examples/s]\n",
      "Map: 100%|██████████| 70/70 [00:00<00:00, 7562.37 examples/s]\n",
      "Map: 100%|██████████| 828/828 [00:00<00:00, 9361.10 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:20<00:00,  2.13it/s, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.9517\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:20<00:00,  2.16it/s, loss=0.194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.8888\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:19<00:00,  2.20it/s, loss=0.252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9324\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:21<00:00,  2.08it/s, loss=0.125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.6188\n",
      "Test Metrics: Precision=0.9282, Recall=0.9282, F1=0.9282\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 350, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 350/350 [00:00<00:00, 8988.75 examples/s]\n",
      "Map: 100%|██████████| 70/70 [00:00<00:00, 7744.49 examples/s]\n",
      "Map: 100%|██████████| 828/828 [00:00<00:00, 9373.53 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:20<00:00,  2.20it/s, loss=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.7978\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:19<00:00,  2.21it/s, loss=0.423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8629\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:20<00:00,  2.19it/s, loss=0.0985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8955\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:20<00:00,  2.11it/s, loss=0.0744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.4186\n",
      "Test Metrics: Precision=0.9340, Recall=0.9340, F1=0.9340\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 350, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 350/350 [00:00<00:00, 9308.44 examples/s]\n",
      "Map: 100%|██████████| 70/70 [00:00<00:00, 7832.08 examples/s]\n",
      "Map: 100%|██████████| 828/828 [00:00<00:00, 9527.57 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:20<00:00,  2.10it/s, loss=1.02] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.9516\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.6849\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:20<00:00,  2.13it/s, loss=0.223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.3453\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:20<00:00,  2.11it/s, loss=0.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 9.6214\n",
      "Test Metrics: Precision=0.9102, Recall=0.9102, F1=0.9102\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 350, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 350/350 [00:00<00:00, 8525.06 examples/s]\n",
      "Map: 100%|██████████| 70/70 [00:00<00:00, 7459.19 examples/s]\n",
      "Map: 100%|██████████| 828/828 [00:00<00:00, 9295.80 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:22<00:00,  1.99it/s, loss=0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.8806\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:21<00:00,  2.01it/s, loss=0.303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.2143\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:21<00:00,  2.01it/s, loss=0.161] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.2989\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:22<00:00,  1.99it/s, loss=0.195] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9658\n",
      "Test Metrics: Precision=0.9346, Recall=0.9346, F1=0.9346\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 355, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 355/355 [00:00<00:00, 8840.91 examples/s]\n",
      "Map: 100%|██████████| 71/71 [00:00<00:00, 6066.57 examples/s]\n",
      "Map: 100%|██████████| 822/822 [00:00<00:00, 9487.03 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:20<00:00,  2.23it/s, loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.5288\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:19<00:00,  2.26it/s, loss=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.5825\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:20<00:00,  2.22it/s, loss=0.3]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4718\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:20<00:00,  2.23it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3825\n",
      "Test Metrics: Precision=0.9320, Recall=0.9320, F1=0.9320\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 355, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 355/355 [00:00<00:00, 8676.12 examples/s]\n",
      "Map: 100%|██████████| 71/71 [00:00<00:00, 7378.30 examples/s]\n",
      "Map: 100%|██████████| 822/822 [00:00<00:00, 9200.93 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:21<00:00,  2.14it/s, loss=0.323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.6179\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:20<00:00,  2.15it/s, loss=0.253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9594\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:20<00:00,  2.17it/s, loss=0.185] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8356\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:20<00:00,  2.21it/s, loss=0.185] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8361\n",
      "Test Metrics: Precision=0.9329, Recall=0.9329, F1=0.9329\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 355, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 355/355 [00:00<00:00, 8833.89 examples/s]\n",
      "Map: 100%|██████████| 71/71 [00:00<00:00, 8262.92 examples/s]\n",
      "Map: 100%|██████████| 822/822 [00:00<00:00, 9409.86 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:20<00:00,  2.18it/s, loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.1152\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:21<00:00,  2.08it/s, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.6552\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:21<00:00,  2.12it/s, loss=0.246] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1422\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:20<00:00,  2.17it/s, loss=0.0867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8907\n",
      "Test Metrics: Precision=0.9320, Recall=0.9320, F1=0.9320\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 355, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 355/355 [00:00<00:00, 8979.48 examples/s]\n",
      "Map: 100%|██████████| 71/71 [00:00<00:00, 7993.01 examples/s]\n",
      "Map: 100%|██████████| 822/822 [00:00<00:00, 9270.18 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:21<00:00,  2.13it/s, loss=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.2335\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:21<00:00,  2.13it/s, loss=0.17] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.7555\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:20<00:00,  2.14it/s, loss=0.0415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1825\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:21<00:00,  2.14it/s, loss=0.0667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3861\n",
      "Test Metrics: Precision=0.9283, Recall=0.9283, F1=0.9283\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 355, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 355/355 [00:00<00:00, 8851.43 examples/s]\n",
      "Map: 100%|██████████| 71/71 [00:00<00:00, 6899.96 examples/s]\n",
      "Map: 100%|██████████| 822/822 [00:00<00:00, 9334.30 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:22<00:00,  1.99it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.6447\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:22<00:00,  2.04it/s, loss=0.206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.4545\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:21<00:00,  2.05it/s, loss=0.157] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6926\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:22<00:00,  2.01it/s, loss=0.065] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.2998\n",
      "Test Metrics: Precision=0.9322, Recall=0.9322, F1=0.9322\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8973.11 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 6629.56 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9303.85 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:20<00:00,  2.19it/s, loss=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.0108\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:20<00:00,  2.20it/s, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.7652\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:20<00:00,  2.20it/s, loss=0.148] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5706\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:20<00:00,  2.22it/s, loss=0.178] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.6302\n",
      "Test Metrics: Precision=0.9306, Recall=0.9306, F1=0.9306\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8925.21 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 6949.65 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9448.30 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:21<00:00,  2.12it/s, loss=0.358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.7693\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:21<00:00,  2.09it/s, loss=0.198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8062\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:20<00:00,  2.16it/s, loss=0.198] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6337\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:21<00:00,  2.09it/s, loss=0.0495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0957\n",
      "Test Metrics: Precision=0.9322, Recall=0.9322, F1=0.9322\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8677.07 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7571.33 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9466.41 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:20<00:00,  2.21it/s, loss=0.458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.8262\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:21<00:00,  2.12it/s, loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.4939\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:20<00:00,  2.17it/s, loss=0.218] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2884\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:21<00:00,  2.14it/s, loss=0.126] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1341\n",
      "Test Metrics: Precision=0.9334, Recall=0.9334, F1=0.9334\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 2100.21 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 6874.81 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 8982.60 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:21<00:00,  2.13it/s, loss=0.666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.4503\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:20<00:00,  2.16it/s, loss=0.173] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.0254\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:21<00:00,  2.06it/s, loss=0.12]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4875\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:22<00:00,  2.02it/s, loss=0.0935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.4341\n",
      "Test Metrics: Precision=0.9294, Recall=0.9294, F1=0.9294\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8754.04 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7320.97 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9545.38 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:22<00:00,  1.96it/s, loss=0.414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.6402\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:23<00:00,  1.95it/s, loss=0.208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.1353\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:22<00:00,  2.03it/s, loss=0.217] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1276\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:22<00:00,  2.00it/s, loss=0.167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6951\n",
      "Test Metrics: Precision=0.9324, Recall=0.9324, F1=0.9324\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 365, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 365/365 [00:00<00:00, 9007.96 examples/s]\n",
      "Map: 100%|██████████| 73/73 [00:00<00:00, 6861.12 examples/s]\n",
      "Map: 100%|██████████| 810/810 [00:00<00:00, 9166.42 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 46/46 [00:20<00:00,  2.25it/s, loss=0.307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.4791\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 46/46 [00:20<00:00,  2.20it/s, loss=0.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.4857\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 46/46 [00:21<00:00,  2.17it/s, loss=0.158] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4489\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 46/46 [00:20<00:00,  2.27it/s, loss=0.134] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0302\n",
      "Test Metrics: Precision=0.9318, Recall=0.9318, F1=0.9318\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 365, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 365/365 [00:00<00:00, 8818.92 examples/s]\n",
      "Map: 100%|██████████| 73/73 [00:00<00:00, 6274.65 examples/s]\n",
      "Map: 100%|██████████| 810/810 [00:00<00:00, 9483.47 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 46/46 [00:21<00:00,  2.17it/s, loss=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.7957\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 46/46 [00:21<00:00,  2.17it/s, loss=0.173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.6737\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 46/46 [00:21<00:00,  2.14it/s, loss=0.216] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5634\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 46/46 [00:22<00:00,  2.07it/s, loss=0.083] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0791\n",
      "Test Metrics: Precision=0.9308, Recall=0.9308, F1=0.9308\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 365, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 365/365 [00:00<00:00, 8806.39 examples/s]\n",
      "Map: 100%|██████████| 73/73 [00:00<00:00, 7000.58 examples/s]\n",
      "Map: 100%|██████████| 810/810 [00:00<00:00, 9418.24 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 46/46 [00:22<00:00,  2.07it/s, loss=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.2031\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 46/46 [00:21<00:00,  2.13it/s, loss=0.0976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.4970\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 46/46 [00:21<00:00,  2.13it/s, loss=0.239] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0328\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 46/46 [00:21<00:00,  2.15it/s, loss=0.0992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7882\n",
      "Test Metrics: Precision=0.9292, Recall=0.9292, F1=0.9292\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 365, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 365/365 [00:00<00:00, 8962.92 examples/s]\n",
      "Map: 100%|██████████| 73/73 [00:00<00:00, 6323.90 examples/s]\n",
      "Map: 100%|██████████| 810/810 [00:00<00:00, 9390.23 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 46/46 [00:21<00:00,  2.18it/s, loss=0.354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.8532\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 46/46 [00:21<00:00,  2.12it/s, loss=0.347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1920\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 46/46 [00:21<00:00,  2.11it/s, loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8619\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 46/46 [00:20<00:00,  2.20it/s, loss=0.112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.5968\n",
      "Test Metrics: Precision=0.9227, Recall=0.9227, F1=0.9227\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 365, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 365/365 [00:00<00:00, 8872.59 examples/s]\n",
      "Map: 100%|██████████| 73/73 [00:00<00:00, 7388.97 examples/s]\n",
      "Map: 100%|██████████| 810/810 [00:00<00:00, 9413.47 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 46/46 [00:23<00:00,  2.00it/s, loss=0.433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.0681\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 46/46 [00:22<00:00,  2.05it/s, loss=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.0539\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 46/46 [00:22<00:00,  2.01it/s, loss=0.16]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0883\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 46/46 [00:23<00:00,  2.00it/s, loss=0.127] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7256\n",
      "Test Metrics: Precision=0.9342, Recall=0.9342, F1=0.9342\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 370, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 370/370 [00:00<00:00, 8807.16 examples/s]\n",
      "Map: 100%|██████████| 74/74 [00:00<00:00, 6937.07 examples/s]\n",
      "Map: 100%|██████████| 804/804 [00:00<00:00, 9191.27 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:20<00:00,  2.25it/s, loss=0.239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.7189\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:20<00:00,  2.26it/s, loss=0.146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.6789\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:21<00:00,  2.21it/s, loss=0.157] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5841\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:20<00:00,  2.32it/s, loss=0.0867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9317\n",
      "Test Metrics: Precision=0.9319, Recall=0.9319, F1=0.9319\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 370, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 370/370 [00:00<00:00, 8950.82 examples/s]\n",
      "Map: 100%|██████████| 74/74 [00:00<00:00, 6871.04 examples/s]\n",
      "Map: 100%|██████████| 804/804 [00:00<00:00, 9349.41 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:22<00:00,  2.13it/s, loss=0.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.3043\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:22<00:00,  2.11it/s, loss=0.276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.1467\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:22<00:00,  2.12it/s, loss=0.0381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2189\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:22<00:00,  2.12it/s, loss=0.119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0317\n",
      "Test Metrics: Precision=0.9337, Recall=0.9337, F1=0.9337\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 370, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 370/370 [00:00<00:00, 8722.47 examples/s]\n",
      "Map: 100%|██████████| 74/74 [00:00<00:00, 8276.76 examples/s]\n",
      "Map: 100%|██████████| 804/804 [00:00<00:00, 9456.72 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:22<00:00,  2.13it/s, loss=0.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.0216\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:22<00:00,  2.13it/s, loss=0.121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8854\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:21<00:00,  2.17it/s, loss=0.259] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8846\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:22<00:00,  2.12it/s, loss=0.172] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6621\n",
      "Test Metrics: Precision=0.9320, Recall=0.9320, F1=0.9320\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 370, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 370/370 [00:00<00:00, 9208.02 examples/s]\n",
      "Map: 100%|██████████| 74/74 [00:00<00:00, 7327.16 examples/s]\n",
      "Map: 100%|██████████| 804/804 [00:00<00:00, 9227.38 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:21<00:00,  2.21it/s, loss=0.154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.5837\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:21<00:00,  2.22it/s, loss=0.215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9989\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:22<00:00,  2.12it/s, loss=0.164] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6882\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:20<00:00,  2.25it/s, loss=0.0338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0976\n",
      "Test Metrics: Precision=0.9294, Recall=0.9294, F1=0.9294\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 370, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 370/370 [00:00<00:00, 9130.39 examples/s]\n",
      "Map: 100%|██████████| 74/74 [00:00<00:00, 6997.76 examples/s]\n",
      "Map: 100%|██████████| 804/804 [00:00<00:00, 9303.40 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:23<00:00,  2.01it/s, loss=0.355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.0692\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:23<00:00,  2.02it/s, loss=0.126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.7661\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:22<00:00,  2.06it/s, loss=0.0525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9402\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:22<00:00,  2.05it/s, loss=0.166] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9451\n",
      "Test Metrics: Precision=0.9334, Recall=0.9334, F1=0.9334\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 375, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 375/375 [00:00<00:00, 8794.52 examples/s]\n",
      "Map: 100%|██████████| 75/75 [00:00<00:00, 7746.76 examples/s]\n",
      "Map: 100%|██████████| 798/798 [00:00<00:00, 9347.83 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:21<00:00,  2.20it/s, loss=1.11] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.9742\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:21<00:00,  2.18it/s, loss=0.244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.1792\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:21<00:00,  2.15it/s, loss=0.0773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.9960\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:21<00:00,  2.18it/s, loss=0.0557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8589\n",
      "Test Metrics: Precision=0.9335, Recall=0.9335, F1=0.9335\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 375, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 375/375 [00:00<00:00, 8958.10 examples/s]\n",
      "Map: 100%|██████████| 75/75 [00:00<00:00, 6669.77 examples/s]\n",
      "Map: 100%|██████████| 798/798 [00:00<00:00, 9297.35 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:22<00:00,  2.11it/s, loss=0.307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.5118\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:22<00:00,  2.07it/s, loss=0.157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.5133\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:22<00:00,  2.11it/s, loss=0.155] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.7916\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:21<00:00,  2.14it/s, loss=0.144] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.2919\n",
      "Test Metrics: Precision=0.9348, Recall=0.9348, F1=0.9348\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 375, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 375/375 [00:00<00:00, 8636.23 examples/s]\n",
      "Map: 100%|██████████| 75/75 [00:00<00:00, 9010.71 examples/s]\n",
      "Map: 100%|██████████| 798/798 [00:00<00:00, 9181.66 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:22<00:00,  2.12it/s, loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.7543\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:22<00:00,  2.05it/s, loss=0.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8605\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:22<00:00,  2.09it/s, loss=0.181] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.5890\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:22<00:00,  2.12it/s, loss=0.0377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5106\n",
      "Test Metrics: Precision=0.9322, Recall=0.9322, F1=0.9322\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 375, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 375/375 [00:00<00:00, 9239.21 examples/s]\n",
      "Map: 100%|██████████| 75/75 [00:00<00:00, 7257.42 examples/s]\n",
      "Map: 100%|██████████| 798/798 [00:00<00:00, 9166.70 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:21<00:00,  2.14it/s, loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.9895\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:21<00:00,  2.16it/s, loss=0.283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.8534\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:21<00:00,  2.15it/s, loss=0.138] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6737\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:22<00:00,  2.11it/s, loss=0.392] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.0792\n",
      "Test Metrics: Precision=0.9279, Recall=0.9279, F1=0.9279\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 375, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 375/375 [00:00<00:00, 9072.61 examples/s]\n",
      "Map: 100%|██████████| 75/75 [00:00<00:00, 7253.74 examples/s]\n",
      "Map: 100%|██████████| 798/798 [00:00<00:00, 9540.93 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:22<00:00,  2.06it/s, loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.5495\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:23<00:00,  2.02it/s, loss=0.108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.8461\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:22<00:00,  2.04it/s, loss=0.226] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4056\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:22<00:00,  2.08it/s, loss=0.083] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9434\n",
      "Test Metrics: Precision=0.9332, Recall=0.9332, F1=0.9332\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8773.63 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7051.59 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9415.36 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:21<00:00,  2.23it/s, loss=0.473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.1871\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:22<00:00,  2.18it/s, loss=0.21]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.4290\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:22<00:00,  2.18it/s, loss=0.108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.5489\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 48/48 [00:21<00:00,  2.19it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9200\n",
      "Test Metrics: Precision=0.9323, Recall=0.9323, F1=0.9323\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8840.20 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7631.30 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9301.23 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:22<00:00,  2.16it/s, loss=0.337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.7338\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:21<00:00,  2.21it/s, loss=0.325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.0960\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:22<00:00,  2.14it/s, loss=0.396] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5646\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 48/48 [00:22<00:00,  2.13it/s, loss=0.204] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8503\n",
      "Test Metrics: Precision=0.9297, Recall=0.9297, F1=0.9297\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8869.82 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 8322.25 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9545.41 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:23<00:00,  2.05it/s, loss=0.376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.6148\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:22<00:00,  2.09it/s, loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.6077\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:22<00:00,  2.13it/s, loss=0.4]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9377\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 48/48 [00:23<00:00,  2.07it/s, loss=0.101] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.4125\n",
      "Test Metrics: Precision=0.9341, Recall=0.9341, F1=0.9341\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 9242.73 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7639.71 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9053.54 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:21<00:00,  2.18it/s, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.9683\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:21<00:00,  2.18it/s, loss=0.0915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.5819\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:22<00:00,  2.10it/s, loss=0.36]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9401\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 48/48 [00:22<00:00,  2.15it/s, loss=0.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8773\n",
      "Test Metrics: Precision=0.9313, Recall=0.9313, F1=0.9313\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8768.42 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 6825.26 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9457.55 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:24<00:00,  1.94it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.5398\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:24<00:00,  2.00it/s, loss=0.233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.4343\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:22<00:00,  2.12it/s, loss=0.0941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4441\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 48/48 [00:23<00:00,  2.01it/s, loss=0.0224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.2962\n",
      "Test Metrics: Precision=0.9364, Recall=0.9364, F1=0.9364\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 385, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 385/385 [00:00<00:00, 8831.51 examples/s]\n",
      "Map: 100%|██████████| 77/77 [00:00<00:00, 7576.81 examples/s]\n",
      "Map: 100%|██████████| 786/786 [00:00<00:00, 9159.40 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:22<00:00,  2.21it/s, loss=0.303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.5540\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:22<00:00,  2.21it/s, loss=1.09] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.2333\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:21<00:00,  2.26it/s, loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.3896\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:22<00:00,  2.20it/s, loss=0.0256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 8.8771\n",
      "Test Metrics: Precision=0.9289, Recall=0.9289, F1=0.9289\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 385, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 385/385 [00:00<00:00, 8784.29 examples/s]\n",
      "Map: 100%|██████████| 77/77 [00:00<00:00, 6959.18 examples/s]\n",
      "Map: 100%|██████████| 786/786 [00:00<00:00, 9468.04 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:22<00:00,  2.19it/s, loss=0.602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.8866\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:22<00:00,  2.14it/s, loss=0.0028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.4837\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:22<00:00,  2.19it/s, loss=0.0667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4702\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:22<00:00,  2.19it/s, loss=0.0765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2590\n",
      "Test Metrics: Precision=0.9333, Recall=0.9333, F1=0.9333\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 385, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 385/385 [00:00<00:00, 8607.90 examples/s]\n",
      "Map: 100%|██████████| 77/77 [00:00<00:00, 7601.42 examples/s]\n",
      "Map: 100%|██████████| 786/786 [00:00<00:00, 9177.93 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:23<00:00,  2.11it/s, loss=0.0391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.7436\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:23<00:00,  2.11it/s, loss=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.4631\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:22<00:00,  2.19it/s, loss=0.0269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8411\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:22<00:00,  2.14it/s, loss=0.0497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7191\n",
      "Test Metrics: Precision=0.9328, Recall=0.9328, F1=0.9328\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 385, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 385/385 [00:00<00:00, 8893.97 examples/s]\n",
      "Map: 100%|██████████| 77/77 [00:00<00:00, 6944.96 examples/s]\n",
      "Map: 100%|██████████| 786/786 [00:00<00:00, 9161.58 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:22<00:00,  2.17it/s, loss=0.044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.7389\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:23<00:00,  2.13it/s, loss=0.809]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.9940\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:22<00:00,  2.21it/s, loss=0.115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.0244\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:22<00:00,  2.22it/s, loss=0.0319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.1953\n",
      "Test Metrics: Precision=0.9313, Recall=0.9313, F1=0.9313\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 385, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 385/385 [00:00<00:00, 9031.97 examples/s]\n",
      "Map: 100%|██████████| 77/77 [00:00<00:00, 7006.73 examples/s]\n",
      "Map: 100%|██████████| 786/786 [00:00<00:00, 9580.60 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:23<00:00,  2.05it/s, loss=0.213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.0224\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:24<00:00,  2.03it/s, loss=0.0681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.7657\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:24<00:00,  2.04it/s, loss=0.304] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.6788\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:23<00:00,  2.09it/s, loss=0.178] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.8216\n",
      "Test Metrics: Precision=0.9348, Recall=0.9348, F1=0.9348\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 390, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 390/390 [00:00<00:00, 9001.94 examples/s]\n",
      "Map: 100%|██████████| 78/78 [00:00<00:00, 7551.72 examples/s]\n",
      "Map: 100%|██████████| 780/780 [00:00<00:00, 9223.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:22<00:00,  2.17it/s, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.2630\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:22<00:00,  2.19it/s, loss=0.396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.2637\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:22<00:00,  2.23it/s, loss=0.133] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2841\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:22<00:00,  2.22it/s, loss=0.0686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9524\n",
      "Test Metrics: Precision=0.9362, Recall=0.9362, F1=0.9362\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 390, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 390/390 [00:00<00:00, 8803.26 examples/s]\n",
      "Map: 100%|██████████| 78/78 [00:00<00:00, 7012.53 examples/s]\n",
      "Map: 100%|██████████| 780/780 [00:00<00:00, 9020.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:22<00:00,  2.15it/s, loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.4740\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:22<00:00,  2.16it/s, loss=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.5140\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:22<00:00,  2.19it/s, loss=0.197] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9968\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:23<00:00,  2.13it/s, loss=0.164] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3812\n",
      "Test Metrics: Precision=0.9332, Recall=0.9332, F1=0.9332\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 390, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 390/390 [00:00<00:00, 8726.01 examples/s]\n",
      "Map: 100%|██████████| 78/78 [00:00<00:00, 7809.13 examples/s]\n",
      "Map: 100%|██████████| 780/780 [00:00<00:00, 9684.58 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:23<00:00,  2.11it/s, loss=0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.0002\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:23<00:00,  2.08it/s, loss=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.3546\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:22<00:00,  2.16it/s, loss=0.174] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3276\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:24<00:00,  2.03it/s, loss=0.18]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3155\n",
      "Test Metrics: Precision=0.9319, Recall=0.9319, F1=0.9319\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 390, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 390/390 [00:00<00:00, 8938.49 examples/s]\n",
      "Map: 100%|██████████| 78/78 [00:00<00:00, 7145.63 examples/s]\n",
      "Map: 100%|██████████| 780/780 [00:00<00:00, 9396.33 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:23<00:00,  2.12it/s, loss=0.433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.6608\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:22<00:00,  2.18it/s, loss=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.3901\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:23<00:00,  2.10it/s, loss=0.145] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6819\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:23<00:00,  2.13it/s, loss=0.103] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6792\n",
      "Test Metrics: Precision=0.9320, Recall=0.9320, F1=0.9320\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 390, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 390/390 [00:00<00:00, 9181.46 examples/s]\n",
      "Map: 100%|██████████| 78/78 [00:00<00:00, 7627.96 examples/s]\n",
      "Map: 100%|██████████| 780/780 [00:00<00:00, 9372.69 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:24<00:00,  2.00it/s, loss=0.395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.2705\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:24<00:00,  2.01it/s, loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.7919\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:24<00:00,  2.01it/s, loss=0.296] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6960\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:24<00:00,  2.03it/s, loss=0.132] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3362\n",
      "Test Metrics: Precision=0.9339, Recall=0.9339, F1=0.9339\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 395, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 395/395 [00:00<00:00, 9197.06 examples/s]\n",
      "Map: 100%|██████████| 79/79 [00:00<00:00, 6947.56 examples/s]\n",
      "Map: 100%|██████████| 774/774 [00:00<00:00, 9361.07 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:22<00:00,  2.25it/s, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.7636\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:22<00:00,  2.24it/s, loss=0.354] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 20.8349\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:22<00:00,  2.22it/s, loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 18.0534\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:22<00:00,  2.21it/s, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 13.1681\n",
      "Test Metrics: Precision=0.9158, Recall=0.9158, F1=0.9158\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 395, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 395/395 [00:00<00:00, 8793.84 examples/s]\n",
      "Map: 100%|██████████| 79/79 [00:00<00:00, 7581.69 examples/s]\n",
      "Map: 100%|██████████| 774/774 [00:00<00:00, 9388.47 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:23<00:00,  2.13it/s, loss=0.0731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.4498\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:22<00:00,  2.20it/s, loss=0.178] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.9573\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:23<00:00,  2.16it/s, loss=0.182] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9113\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:23<00:00,  2.17it/s, loss=0.104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1392\n",
      "Test Metrics: Precision=0.9315, Recall=0.9315, F1=0.9315\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 395, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 395/395 [00:00<00:00, 8680.04 examples/s]\n",
      "Map: 100%|██████████| 79/79 [00:00<00:00, 7445.40 examples/s]\n",
      "Map: 100%|██████████| 774/774 [00:00<00:00, 9686.93 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:23<00:00,  2.17it/s, loss=0.285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.9433\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:22<00:00,  2.21it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.0170\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:23<00:00,  2.16it/s, loss=0.143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8497\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:22<00:00,  2.18it/s, loss=0.156] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6111\n",
      "Test Metrics: Precision=0.9342, Recall=0.9342, F1=0.9342\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 395, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 395/395 [00:00<00:00, 9188.44 examples/s]\n",
      "Map: 100%|██████████| 79/79 [00:00<00:00, 8159.92 examples/s]\n",
      "Map: 100%|██████████| 774/774 [00:00<00:00, 9216.16 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:23<00:00,  2.12it/s, loss=0.503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.4819\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:23<00:00,  2.16it/s, loss=0.00445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.6438\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:22<00:00,  2.18it/s, loss=0.111] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1691\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:23<00:00,  2.12it/s, loss=0.111] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0072\n",
      "Test Metrics: Precision=0.9323, Recall=0.9323, F1=0.9323\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 395, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 395/395 [00:00<00:00, 8777.67 examples/s]\n",
      "Map: 100%|██████████| 79/79 [00:00<00:00, 8045.99 examples/s]\n",
      "Map: 100%|██████████| 774/774 [00:00<00:00, 9193.61 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:24<00:00,  2.01it/s, loss=1.16] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.0762\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:24<00:00,  2.07it/s, loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.4003\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:24<00:00,  2.07it/s, loss=0.261] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.4364\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:24<00:00,  2.04it/s, loss=0.0643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.8388\n",
      "Test Metrics: Precision=0.9347, Recall=0.9347, F1=0.9347\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8986.96 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7481.31 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9341.68 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:22<00:00,  2.23it/s, loss=0.319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.2986\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:22<00:00,  2.18it/s, loss=0.345] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.1237\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:22<00:00,  2.23it/s, loss=0.0938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3664\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:23<00:00,  2.16it/s, loss=0.152] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0901\n",
      "Test Metrics: Precision=0.9360, Recall=0.9360, F1=0.9360\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8912.82 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7916.96 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9123.11 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:23<00:00,  2.14it/s, loss=0.473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.4676\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:22<00:00,  2.25it/s, loss=0.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.0937\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:23<00:00,  2.11it/s, loss=0.189] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.9286\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:23<00:00,  2.13it/s, loss=0.127] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.4150\n",
      "Test Metrics: Precision=0.9285, Recall=0.9285, F1=0.9285\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8767.77 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6959.91 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9638.64 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:23<00:00,  2.09it/s, loss=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.2664\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:23<00:00,  2.12it/s, loss=0.446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.9147\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:23<00:00,  2.12it/s, loss=0.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3168\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:23<00:00,  2.12it/s, loss=0.0951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8227\n",
      "Test Metrics: Precision=0.9352, Recall=0.9352, F1=0.9352\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 9246.85 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7051.92 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9323.08 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:22<00:00,  2.20it/s, loss=0.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.0778\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:22<00:00,  2.19it/s, loss=0.292] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5949\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:23<00:00,  2.16it/s, loss=0.28]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4141\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:23<00:00,  2.16it/s, loss=0.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.2445\n",
      "Test Metrics: Precision=0.9320, Recall=0.9320, F1=0.9320\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8696.69 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6904.63 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9196.91 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:24<00:00,  2.02it/s, loss=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.9684\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:24<00:00,  2.02it/s, loss=0.164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9650\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:24<00:00,  2.02it/s, loss=0.161] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6587\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:23<00:00,  2.10it/s, loss=0.231] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5816\n",
      "Test Metrics: Precision=0.9369, Recall=0.9369, F1=0.9369\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 405, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 405/405 [00:00<00:00, 8891.63 examples/s]\n",
      "Map: 100%|██████████| 81/81 [00:00<00:00, 6836.61 examples/s]\n",
      "Map: 100%|██████████| 762/762 [00:00<00:00, 9504.53 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 51/51 [00:23<00:00,  2.19it/s, loss=0.328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.3341\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 51/51 [00:23<00:00,  2.19it/s, loss=0.355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5045\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 51/51 [00:23<00:00,  2.17it/s, loss=0.0625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.2857\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 51/51 [00:23<00:00,  2.19it/s, loss=0.0713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8559\n",
      "Test Metrics: Precision=0.9336, Recall=0.9336, F1=0.9336\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 405, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 405/405 [00:00<00:00, 8956.23 examples/s]\n",
      "Map: 100%|██████████| 81/81 [00:00<00:00, 7862.14 examples/s]\n",
      "Map: 100%|██████████| 762/762 [00:00<00:00, 9314.02 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 51/51 [00:23<00:00,  2.15it/s, loss=0.58] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.0549\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 51/51 [00:24<00:00,  2.12it/s, loss=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.6743\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 51/51 [00:23<00:00,  2.21it/s, loss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.9036\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 51/51 [00:23<00:00,  2.18it/s, loss=0.208] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 8.2883\n",
      "Test Metrics: Precision=0.9267, Recall=0.9267, F1=0.9267\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 405, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 405/405 [00:00<00:00, 8694.30 examples/s]\n",
      "Map: 100%|██████████| 81/81 [00:00<00:00, 6221.41 examples/s]\n",
      "Map: 100%|██████████| 762/762 [00:00<00:00, 9312.61 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 51/51 [00:24<00:00,  2.11it/s, loss=0.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.2042\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 51/51 [00:24<00:00,  2.10it/s, loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.1481\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 51/51 [00:24<00:00,  2.08it/s, loss=0.172] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.0423\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 51/51 [00:24<00:00,  2.10it/s, loss=0.0569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7630\n",
      "Test Metrics: Precision=0.9359, Recall=0.9359, F1=0.9359\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 405, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 405/405 [00:00<00:00, 9081.10 examples/s]\n",
      "Map: 100%|██████████| 81/81 [00:00<00:00, 7043.11 examples/s]\n",
      "Map: 100%|██████████| 762/762 [00:00<00:00, 9376.90 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 51/51 [00:23<00:00,  2.13it/s, loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.1749\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 51/51 [00:24<00:00,  2.09it/s, loss=0.257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.2988\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 51/51 [00:23<00:00,  2.16it/s, loss=0.128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1886\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 51/51 [00:23<00:00,  2.18it/s, loss=0.151] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8339\n",
      "Test Metrics: Precision=0.9297, Recall=0.9297, F1=0.9297\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 405, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 405/405 [00:00<00:00, 8924.71 examples/s]\n",
      "Map: 100%|██████████| 81/81 [00:00<00:00, 7438.83 examples/s]\n",
      "Map: 100%|██████████| 762/762 [00:00<00:00, 9388.14 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 51/51 [00:25<00:00,  2.02it/s, loss=0.307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.4137\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 51/51 [00:25<00:00,  1.98it/s, loss=0.196] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.3336\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 51/51 [00:24<00:00,  2.08it/s, loss=0.492] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3844\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 51/51 [00:24<00:00,  2.06it/s, loss=0.0474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.6724\n",
      "Test Metrics: Precision=0.9356, Recall=0.9356, F1=0.9356\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 410, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 410/410 [00:00<00:00, 2440.75 examples/s]\n",
      "Map: 100%|██████████| 82/82 [00:00<00:00, 7509.62 examples/s]\n",
      "Map: 100%|██████████| 756/756 [00:00<00:00, 9179.71 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:23<00:00,  2.20it/s, loss=0.433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.1108\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:24<00:00,  2.16it/s, loss=0.278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.7555\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:23<00:00,  2.23it/s, loss=0.313] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7960\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:23<00:00,  2.19it/s, loss=0.128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9567\n",
      "Test Metrics: Precision=0.9338, Recall=0.9338, F1=0.9338\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 410, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 410/410 [00:00<00:00, 9070.77 examples/s]\n",
      "Map: 100%|██████████| 82/82 [00:00<00:00, 8279.96 examples/s]\n",
      "Map: 100%|██████████| 756/756 [00:00<00:00, 9033.63 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:24<00:00,  2.17it/s, loss=0.801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.9664\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:23<00:00,  2.18it/s, loss=0.34] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.4694\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:23<00:00,  2.18it/s, loss=0.184] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.3941\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:23<00:00,  2.23it/s, loss=0.0653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.5227\n",
      "Test Metrics: Precision=0.9329, Recall=0.9329, F1=0.9329\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 410, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 410/410 [00:00<00:00, 8813.23 examples/s]\n",
      "Map: 100%|██████████| 82/82 [00:00<00:00, 7232.01 examples/s]\n",
      "Map: 100%|██████████| 756/756 [00:00<00:00, 9435.70 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:24<00:00,  2.15it/s, loss=0.402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.6623\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:25<00:00,  2.07it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.4333\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:24<00:00,  2.13it/s, loss=0.0296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4360\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:24<00:00,  2.11it/s, loss=0.0314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9870\n",
      "Test Metrics: Precision=0.9339, Recall=0.9339, F1=0.9339\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 410, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 410/410 [00:00<00:00, 8758.16 examples/s]\n",
      "Map: 100%|██████████| 82/82 [00:00<00:00, 7659.81 examples/s]\n",
      "Map: 100%|██████████| 756/756 [00:00<00:00, 9071.02 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:24<00:00,  2.16it/s, loss=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.6271\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:24<00:00,  2.15it/s, loss=0.152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9161\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:23<00:00,  2.25it/s, loss=0.053] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1996\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:24<00:00,  2.14it/s, loss=0.14]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9551\n",
      "Test Metrics: Precision=0.9312, Recall=0.9312, F1=0.9312\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 410, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 410/410 [00:00<00:00, 9060.59 examples/s]\n",
      "Map: 100%|██████████| 82/82 [00:00<00:00, 8141.58 examples/s]\n",
      "Map: 100%|██████████| 756/756 [00:00<00:00, 9223.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:25<00:00,  2.07it/s, loss=0.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.4036\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:24<00:00,  2.09it/s, loss=0.054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.9020\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:25<00:00,  2.01it/s, loss=0.163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2953\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:25<00:00,  2.02it/s, loss=0.161] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.0521\n",
      "Test Metrics: Precision=0.9358, Recall=0.9358, F1=0.9358\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 415, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 415/415 [00:00<00:00, 8863.21 examples/s]\n",
      "Map: 100%|██████████| 83/83 [00:00<00:00, 7584.80 examples/s]\n",
      "Map: 100%|██████████| 750/750 [00:00<00:00, 9356.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:23<00:00,  2.20it/s, loss=0.341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.2613\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:23<00:00,  2.20it/s, loss=0.135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8361\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:23<00:00,  2.17it/s, loss=0.127] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.2335\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:23<00:00,  2.18it/s, loss=0.248] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5027\n",
      "Test Metrics: Precision=0.9328, Recall=0.9328, F1=0.9328\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 415, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 415/415 [00:00<00:00, 9163.75 examples/s]\n",
      "Map: 100%|██████████| 83/83 [00:00<00:00, 6849.53 examples/s]\n",
      "Map: 100%|██████████| 750/750 [00:00<00:00, 9362.34 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:23<00:00,  2.21it/s, loss=0.704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.4820\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:24<00:00,  2.15it/s, loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.0875\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:23<00:00,  2.18it/s, loss=0.0952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7136\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:24<00:00,  2.10it/s, loss=0.122] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1731\n",
      "Test Metrics: Precision=0.9334, Recall=0.9334, F1=0.9334\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 415, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 415/415 [00:00<00:00, 9060.01 examples/s]\n",
      "Map: 100%|██████████| 83/83 [00:00<00:00, 7052.96 examples/s]\n",
      "Map: 100%|██████████| 750/750 [00:00<00:00, 9383.23 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:25<00:00,  2.08it/s, loss=0.35] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.3676\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:24<00:00,  2.14it/s, loss=0.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.4219\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:25<00:00,  2.07it/s, loss=0.141] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2301\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:24<00:00,  2.09it/s, loss=0.138] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9515\n",
      "Test Metrics: Precision=0.9351, Recall=0.9351, F1=0.9351\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 415, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 415/415 [00:00<00:00, 8704.75 examples/s]\n",
      "Map: 100%|██████████| 83/83 [00:00<00:00, 7537.51 examples/s]\n",
      "Map: 100%|██████████| 750/750 [00:00<00:00, 9208.10 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:24<00:00,  2.14it/s, loss=0.413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.9050\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:24<00:00,  2.15it/s, loss=0.445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.5672\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:24<00:00,  2.08it/s, loss=0.212] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3830\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:24<00:00,  2.11it/s, loss=0.0598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0416\n",
      "Test Metrics: Precision=0.9333, Recall=0.9333, F1=0.9333\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 415, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 415/415 [00:00<00:00, 8779.34 examples/s]\n",
      "Map: 100%|██████████| 83/83 [00:00<00:00, 7061.40 examples/s]\n",
      "Map: 100%|██████████| 750/750 [00:00<00:00, 9601.49 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:26<00:00,  1.96it/s, loss=0.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.4507\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:26<00:00,  1.99it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.3075\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:26<00:00,  1.96it/s, loss=0.0827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7767\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:26<00:00,  1.98it/s, loss=0.1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0648\n",
      "Test Metrics: Precision=0.9357, Recall=0.9357, F1=0.9357\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 9058.18 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 8180.97 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9150.86 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:24<00:00,  2.17it/s, loss=0.794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.9182\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:23<00:00,  2.23it/s, loss=0.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.3591\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:24<00:00,  2.19it/s, loss=0.0782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0457\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 53/53 [00:23<00:00,  2.22it/s, loss=0.0735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9930\n",
      "Test Metrics: Precision=0.9339, Recall=0.9339, F1=0.9339\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 9220.61 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7764.49 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9269.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:23<00:00,  2.23it/s, loss=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.1124\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:24<00:00,  2.14it/s, loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.5495\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:23<00:00,  2.23it/s, loss=0.114] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8938\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 53/53 [00:24<00:00,  2.18it/s, loss=0.0369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.2284\n",
      "Test Metrics: Precision=0.9331, Recall=0.9331, F1=0.9331\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8775.39 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7378.15 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9731.90 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:25<00:00,  2.05it/s, loss=0.247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.1990\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:25<00:00,  2.07it/s, loss=0.34] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.4356\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:24<00:00,  2.12it/s, loss=0.0953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9064\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 53/53 [00:25<00:00,  2.11it/s, loss=0.0837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2613\n",
      "Test Metrics: Precision=0.9342, Recall=0.9342, F1=0.9342\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 9069.93 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7488.87 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9359.62 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:24<00:00,  2.15it/s, loss=0.492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.6256\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:24<00:00,  2.13it/s, loss=0.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.6374\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:24<00:00,  2.18it/s, loss=0.22]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.8279\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 53/53 [00:24<00:00,  2.14it/s, loss=0.0895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.5654\n",
      "Test Metrics: Precision=0.9294, Recall=0.9294, F1=0.9294\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8919.71 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7520.52 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9197.25 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:26<00:00,  2.01it/s, loss=0.463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.6609\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:26<00:00,  2.02it/s, loss=0.353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.3093\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:26<00:00,  2.04it/s, loss=0.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.0713\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 53/53 [00:26<00:00,  1.97it/s, loss=0.0619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.5230\n",
      "Test Metrics: Precision=0.9356, Recall=0.9356, F1=0.9356\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 425, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 425/425 [00:00<00:00, 9100.08 examples/s]\n",
      "Map: 100%|██████████| 85/85 [00:00<00:00, 8429.07 examples/s]\n",
      "Map: 100%|██████████| 738/738 [00:00<00:00, 9285.89 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:23<00:00,  2.27it/s, loss=0.762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.5488\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:23<00:00,  2.25it/s, loss=0.319] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.5286\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:23<00:00,  2.26it/s, loss=0.0714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.4805\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:23<00:00,  2.25it/s, loss=0.261] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.9146\n",
      "Test Metrics: Precision=0.9337, Recall=0.9337, F1=0.9337\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 425, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 425/425 [00:00<00:00, 9190.40 examples/s]\n",
      "Map: 100%|██████████| 85/85 [00:00<00:00, 6417.12 examples/s]\n",
      "Map: 100%|██████████| 738/738 [00:00<00:00, 9215.09 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:24<00:00,  2.20it/s, loss=0.145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.8446\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:24<00:00,  2.18it/s, loss=0.152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.1189\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:24<00:00,  2.20it/s, loss=0.0947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9416\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:24<00:00,  2.18it/s, loss=0.000955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3907\n",
      "Test Metrics: Precision=0.9320, Recall=0.9320, F1=0.9320\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 425, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 425/425 [00:00<00:00, 8654.14 examples/s]\n",
      "Map: 100%|██████████| 85/85 [00:00<00:00, 6696.39 examples/s]\n",
      "Map: 100%|██████████| 738/738 [00:00<00:00, 9412.71 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:25<00:00,  2.08it/s, loss=0.399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.1019\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:24<00:00,  2.21it/s, loss=0.115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8957\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:25<00:00,  2.10it/s, loss=0.233] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0170\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:24<00:00,  2.19it/s, loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3336\n",
      "Test Metrics: Precision=0.9354, Recall=0.9354, F1=0.9354\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 425, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 425/425 [00:00<00:00, 9029.65 examples/s]\n",
      "Map: 100%|██████████| 85/85 [00:00<00:00, 8421.30 examples/s]\n",
      "Map: 100%|██████████| 738/738 [00:00<00:00, 9102.21 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:25<00:00,  2.11it/s, loss=0.683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.6364\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:25<00:00,  2.16it/s, loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2594\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:25<00:00,  2.11it/s, loss=0.283] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.3959\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:24<00:00,  2.16it/s, loss=0.0626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.2291\n",
      "Test Metrics: Precision=0.9298, Recall=0.9298, F1=0.9298\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 425, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 425/425 [00:00<00:00, 8769.82 examples/s]\n",
      "Map: 100%|██████████| 85/85 [00:00<00:00, 7574.97 examples/s]\n",
      "Map: 100%|██████████| 738/738 [00:00<00:00, 9531.66 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:27<00:00,  1.96it/s, loss=0.136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.0860\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:26<00:00,  2.02it/s, loss=0.462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.5567\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:26<00:00,  2.05it/s, loss=0.0917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.4310\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:26<00:00,  2.05it/s, loss=0.0933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0742\n",
      "Test Metrics: Precision=0.9367, Recall=0.9367, F1=0.9367\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 430, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 430/430 [00:00<00:00, 8701.33 examples/s]\n",
      "Map: 100%|██████████| 86/86 [00:00<00:00, 7131.05 examples/s]\n",
      "Map: 100%|██████████| 732/732 [00:00<00:00, 9310.64 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:24<00:00,  2.18it/s, loss=0.418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.9572\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:24<00:00,  2.22it/s, loss=0.127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.9481\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:24<00:00,  2.23it/s, loss=0.184] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5625\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:23<00:00,  2.25it/s, loss=0.129] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9923\n",
      "Test Metrics: Precision=0.9341, Recall=0.9341, F1=0.9341\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 430, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 430/430 [00:00<00:00, 9072.51 examples/s]\n",
      "Map: 100%|██████████| 86/86 [00:00<00:00, 6668.58 examples/s]\n",
      "Map: 100%|██████████| 732/732 [00:00<00:00, 9453.03 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:25<00:00,  2.14it/s, loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.2894\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:25<00:00,  2.15it/s, loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.9720\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:25<00:00,  2.11it/s, loss=0.133] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5956\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:24<00:00,  2.18it/s, loss=0.105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.2394\n",
      "Test Metrics: Precision=0.9321, Recall=0.9321, F1=0.9321\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 430, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 430/430 [00:00<00:00, 8622.83 examples/s]\n",
      "Map: 100%|██████████| 86/86 [00:00<00:00, 7286.04 examples/s]\n",
      "Map: 100%|██████████| 732/732 [00:00<00:00, 3579.30 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:26<00:00,  2.06it/s, loss=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.5621\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:25<00:00,  2.08it/s, loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.9460\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:26<00:00,  2.06it/s, loss=0.183] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6892\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:26<00:00,  2.06it/s, loss=0.103] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.1501\n",
      "Test Metrics: Precision=0.9287, Recall=0.9287, F1=0.9287\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 430, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 430/430 [00:00<00:00, 8788.81 examples/s]\n",
      "Map: 100%|██████████| 86/86 [00:00<00:00, 6817.82 examples/s]\n",
      "Map: 100%|██████████| 732/732 [00:00<00:00, 9497.65 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:25<00:00,  2.15it/s, loss=0.291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.6490\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:25<00:00,  2.10it/s, loss=0.153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.6946\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:25<00:00,  2.11it/s, loss=0.183] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.8659\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:25<00:00,  2.13it/s, loss=0.165] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.1539\n",
      "Test Metrics: Precision=0.9290, Recall=0.9290, F1=0.9290\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 430, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 430/430 [00:00<00:00, 8726.80 examples/s]\n",
      "Map: 100%|██████████| 86/86 [00:00<00:00, 7162.35 examples/s]\n",
      "Map: 100%|██████████| 732/732 [00:00<00:00, 9631.73 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:27<00:00,  1.98it/s, loss=0.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.2494\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:27<00:00,  1.96it/s, loss=0.187] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.8670\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:27<00:00,  1.97it/s, loss=0.25]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2408\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:26<00:00,  2.01it/s, loss=0.1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.8324\n",
      "Test Metrics: Precision=0.9345, Recall=0.9345, F1=0.9345\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 435, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 435/435 [00:00<00:00, 8849.77 examples/s]\n",
      "Map: 100%|██████████| 87/87 [00:00<00:00, 7586.06 examples/s]\n",
      "Map: 100%|██████████| 726/726 [00:00<00:00, 9151.68 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:24<00:00,  2.21it/s, loss=0.444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.5033\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:25<00:00,  2.20it/s, loss=0.122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.9661\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:24<00:00,  2.20it/s, loss=0.113] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4366\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:24<00:00,  2.22it/s, loss=0.064] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8724\n",
      "Test Metrics: Precision=0.9346, Recall=0.9346, F1=0.9346\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 435, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 435/435 [00:00<00:00, 9137.87 examples/s]\n",
      "Map: 100%|██████████| 87/87 [00:00<00:00, 7444.45 examples/s]\n",
      "Map: 100%|██████████| 726/726 [00:00<00:00, 9163.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:25<00:00,  2.18it/s, loss=0.309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.4319\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:24<00:00,  2.22it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.1718\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:25<00:00,  2.17it/s, loss=0.137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.7651\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:25<00:00,  2.17it/s, loss=0.134] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.8073\n",
      "Test Metrics: Precision=0.9325, Recall=0.9325, F1=0.9325\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 435, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 435/435 [00:00<00:00, 8582.31 examples/s]\n",
      "Map: 100%|██████████| 87/87 [00:00<00:00, 7761.45 examples/s]\n",
      "Map: 100%|██████████| 726/726 [00:00<00:00, 9432.50 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:25<00:00,  2.14it/s, loss=0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.4317\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:26<00:00,  2.08it/s, loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.4561\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:26<00:00,  2.09it/s, loss=0.146] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3164\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:25<00:00,  2.13it/s, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0020\n",
      "Test Metrics: Precision=0.9329, Recall=0.9329, F1=0.9329\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 435, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 435/435 [00:00<00:00, 9059.56 examples/s]\n",
      "Map: 100%|██████████| 87/87 [00:00<00:00, 7153.87 examples/s]\n",
      "Map: 100%|██████████| 726/726 [00:00<00:00, 9417.74 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:25<00:00,  2.14it/s, loss=0.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.7842\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:26<00:00,  2.08it/s, loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5144\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:25<00:00,  2.13it/s, loss=0.0997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4388\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:25<00:00,  2.17it/s, loss=0.0523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8118\n",
      "Test Metrics: Precision=0.9317, Recall=0.9317, F1=0.9317\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 435, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 435/435 [00:00<00:00, 8539.05 examples/s]\n",
      "Map: 100%|██████████| 87/87 [00:00<00:00, 7503.23 examples/s]\n",
      "Map: 100%|██████████| 726/726 [00:00<00:00, 9588.61 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:26<00:00,  2.04it/s, loss=0.279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.6078\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:28<00:00,  1.94it/s, loss=0.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.2696\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:27<00:00,  1.97it/s, loss=0.116] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.5279\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:27<00:00,  1.98it/s, loss=0.114] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.8558\n",
      "Test Metrics: Precision=0.9331, Recall=0.9331, F1=0.9331\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8848.45 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7897.86 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9423.43 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:25<00:00,  2.18it/s, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.4347\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:24<00:00,  2.23it/s, loss=0.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.4236\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:25<00:00,  2.19it/s, loss=0.208] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9749\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:24<00:00,  2.22it/s, loss=0.0447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3138\n",
      "Test Metrics: Precision=0.9350, Recall=0.9350, F1=0.9350\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 9041.40 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7830.01 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9512.99 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:26<00:00,  2.11it/s, loss=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.1395\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:25<00:00,  2.15it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2394\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:26<00:00,  2.10it/s, loss=0.109] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6419\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:25<00:00,  2.15it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3461\n",
      "Test Metrics: Precision=0.9327, Recall=0.9327, F1=0.9327\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8750.44 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7385.67 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9726.49 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:27<00:00,  2.01it/s, loss=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.3333\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:26<00:00,  2.07it/s, loss=0.12] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.1963\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:26<00:00,  2.06it/s, loss=0.0754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0839\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:26<00:00,  2.11it/s, loss=0.104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1140\n",
      "Test Metrics: Precision=0.9357, Recall=0.9357, F1=0.9357\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8565.68 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7079.13 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9645.87 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:26<00:00,  2.07it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.9278\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:26<00:00,  2.06it/s, loss=0.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.6236\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:26<00:00,  2.05it/s, loss=0.113] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.7310\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:27<00:00,  2.03it/s, loss=0.0924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.2485\n",
      "Test Metrics: Precision=0.9307, Recall=0.9307, F1=0.9307\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8760.87 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7680.92 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9420.93 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:27<00:00,  2.03it/s, loss=0.309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.2495\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:27<00:00,  2.00it/s, loss=0.168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.4658\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:27<00:00,  1.99it/s, loss=0.234] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3698\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:27<00:00,  2.01it/s, loss=0.152] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7582\n",
      "Test Metrics: Precision=0.9379, Recall=0.9379, F1=0.9379\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 445, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 445/445 [00:00<00:00, 9132.24 examples/s]\n",
      "Map: 100%|██████████| 89/89 [00:00<00:00, 7596.83 examples/s]\n",
      "Map: 100%|██████████| 714/714 [00:00<00:00, 9570.07 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 56/56 [00:25<00:00,  2.21it/s, loss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.5755\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 56/56 [00:24<00:00,  2.27it/s, loss=0.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.8507\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 56/56 [00:24<00:00,  2.25it/s, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0905\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 56/56 [00:25<00:00,  2.19it/s, loss=0.137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.8684\n",
      "Test Metrics: Precision=0.9327, Recall=0.9327, F1=0.9327\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 445, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 445/445 [00:00<00:00, 8809.57 examples/s]\n",
      "Map: 100%|██████████| 89/89 [00:00<00:00, 7284.34 examples/s]\n",
      "Map: 100%|██████████| 714/714 [00:00<00:00, 9362.75 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 56/56 [00:26<00:00,  2.11it/s, loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.3099\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 56/56 [00:26<00:00,  2.14it/s, loss=0.301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1932\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 56/56 [00:26<00:00,  2.10it/s, loss=0.188] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9429\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 56/56 [00:26<00:00,  2.13it/s, loss=0.198] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.4434\n",
      "Test Metrics: Precision=0.9342, Recall=0.9342, F1=0.9342\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 445, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 445/445 [00:00<00:00, 8622.44 examples/s]\n",
      "Map: 100%|██████████| 89/89 [00:00<00:00, 6763.41 examples/s]\n",
      "Map: 100%|██████████| 714/714 [00:00<00:00, 9531.69 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 56/56 [00:26<00:00,  2.09it/s, loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.2775\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 56/56 [00:26<00:00,  2.14it/s, loss=0.189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.3567\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 56/56 [00:26<00:00,  2.12it/s, loss=0.139] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9965\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 56/56 [00:26<00:00,  2.11it/s, loss=0.0372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.5750\n",
      "Test Metrics: Precision=0.9277, Recall=0.9277, F1=0.9277\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 445, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 445/445 [00:00<00:00, 8833.33 examples/s]\n",
      "Map: 100%|██████████| 89/89 [00:00<00:00, 8294.11 examples/s]\n",
      "Map: 100%|██████████| 714/714 [00:00<00:00, 9408.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 56/56 [00:26<00:00,  2.10it/s, loss=0.473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.5953\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 56/56 [00:26<00:00,  2.12it/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.2834\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 56/56 [00:25<00:00,  2.17it/s, loss=0.0494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1711\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 56/56 [00:26<00:00,  2.11it/s, loss=0.108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0327\n",
      "Test Metrics: Precision=0.9339, Recall=0.9339, F1=0.9339\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 445, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 445/445 [00:00<00:00, 8468.04 examples/s]\n",
      "Map: 100%|██████████| 89/89 [00:00<00:00, 7748.21 examples/s]\n",
      "Map: 100%|██████████| 714/714 [00:00<00:00, 9512.10 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 56/56 [00:28<00:00,  1.99it/s, loss=0.291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.6308\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 56/56 [00:27<00:00,  2.00it/s, loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.2449\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 56/56 [00:28<00:00,  1.98it/s, loss=0.0749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8365\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 56/56 [00:28<00:00,  1.99it/s, loss=0.0911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6191\n",
      "Test Metrics: Precision=0.9380, Recall=0.9380, F1=0.9380\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 450, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 450/450 [00:00<00:00, 8742.10 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7174.52 examples/s]\n",
      "Map: 100%|██████████| 708/708 [00:00<00:00, 9380.06 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:25<00:00,  2.20it/s, loss=0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.2872\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:25<00:00,  2.21it/s, loss=0.0861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.3153\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:25<00:00,  2.24it/s, loss=0.0938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.7179\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:25<00:00,  2.21it/s, loss=0.123] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.7936\n",
      "Test Metrics: Precision=0.9328, Recall=0.9328, F1=0.9328\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 450, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 450/450 [00:00<00:00, 9277.79 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7298.67 examples/s]\n",
      "Map: 100%|██████████| 708/708 [00:00<00:00, 9609.25 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:26<00:00,  2.14it/s, loss=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.0770\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:26<00:00,  2.11it/s, loss=0.089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.7473\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:26<00:00,  2.17it/s, loss=0.306] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8619\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:25<00:00,  2.21it/s, loss=0.518] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.2418\n",
      "Test Metrics: Precision=0.9349, Recall=0.9349, F1=0.9349\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 450, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 450/450 [00:00<00:00, 9105.25 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 8671.09 examples/s]\n",
      "Map: 100%|██████████| 708/708 [00:00<00:00, 9571.01 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:26<00:00,  2.15it/s, loss=0.475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.0395\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:25<00:00,  2.20it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.9870\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:27<00:00,  2.10it/s, loss=0.0486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6498\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:27<00:00,  2.10it/s, loss=0.203] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.2634\n",
      "Test Metrics: Precision=0.9354, Recall=0.9354, F1=0.9354\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 450, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 450/450 [00:00<00:00, 8598.64 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 6969.21 examples/s]\n",
      "Map: 100%|██████████| 708/708 [00:00<00:00, 9404.06 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:26<00:00,  2.11it/s, loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.0977\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:26<00:00,  2.13it/s, loss=0.0573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.1782\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:26<00:00,  2.11it/s, loss=0.142] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0572\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:27<00:00,  2.09it/s, loss=0.145] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.5907\n",
      "Test Metrics: Precision=0.9335, Recall=0.9335, F1=0.9335\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 450, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 450/450 [00:00<00:00, 8749.03 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7474.70 examples/s]\n",
      "Map: 100%|██████████| 708/708 [00:00<00:00, 9378.16 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:27<00:00,  2.08it/s, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 35.1932\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:28<00:00,  2.00it/s, loss=0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.7897\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:28<00:00,  2.00it/s, loss=0.129] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.2694\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:28<00:00,  1.99it/s, loss=0.116] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 8.2788\n",
      "Test Metrics: Precision=0.9357, Recall=0.9357, F1=0.9357\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 455, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 455/455 [00:00<00:00, 9257.69 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:00<00:00, 7589.31 examples/s]\n",
      "Map: 100%|██████████| 702/702 [00:00<00:00, 9216.63 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:25<00:00,  2.21it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.1015\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:25<00:00,  2.22it/s, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.2532\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:25<00:00,  2.20it/s, loss=0.164] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.5204\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:25<00:00,  2.22it/s, loss=0.186] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 9.9074\n",
      "Test Metrics: Precision=0.9272, Recall=0.9272, F1=0.9272\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 455, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 455/455 [00:00<00:00, 9297.97 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:00<00:00, 7817.34 examples/s]\n",
      "Map: 100%|██████████| 702/702 [00:00<00:00, 9434.28 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:26<00:00,  2.12it/s, loss=0.283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.0942\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:26<00:00,  2.13it/s, loss=0.0802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.6013\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:26<00:00,  2.12it/s, loss=0.137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3481\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:27<00:00,  2.08it/s, loss=0.0553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7876\n",
      "Test Metrics: Precision=0.9378, Recall=0.9378, F1=0.9378\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 455, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 455/455 [00:00<00:00, 8608.42 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:00<00:00, 7678.01 examples/s]\n",
      "Map: 100%|██████████| 702/702 [00:00<00:00, 9341.08 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:27<00:00,  2.08it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.8025\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:26<00:00,  2.15it/s, loss=0.318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.1629\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:26<00:00,  2.13it/s, loss=0.161] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.8085\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:27<00:00,  2.09it/s, loss=0.0829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 8.4282\n",
      "Test Metrics: Precision=0.9315, Recall=0.9315, F1=0.9315\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 455, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 455/455 [00:00<00:00, 8923.97 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:00<00:00, 7726.35 examples/s]\n",
      "Map: 100%|██████████| 702/702 [00:00<00:00, 9344.67 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:27<00:00,  2.09it/s, loss=0.356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.4400\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:27<00:00,  2.07it/s, loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.7233\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:27<00:00,  2.08it/s, loss=0.184] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.3856\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:26<00:00,  2.14it/s, loss=0.0612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3968\n",
      "Test Metrics: Precision=0.9318, Recall=0.9318, F1=0.9318\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 455, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 455/455 [00:00<00:00, 9009.83 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:00<00:00, 9135.07 examples/s]\n",
      "Map: 100%|██████████| 702/702 [00:00<00:00, 9493.66 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:28<00:00,  2.02it/s, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.5812\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:28<00:00,  2.04it/s, loss=0.356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 19.2760\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:28<00:00,  1.97it/s, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.7401\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:28<00:00,  2.01it/s, loss=0.108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 9.5604\n",
      "Test Metrics: Precision=0.9337, Recall=0.9337, F1=0.9337\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 9095.83 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7202.13 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9504.70 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:25<00:00,  2.23it/s, loss=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.6980\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:26<00:00,  2.21it/s, loss=0.212] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.0487\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:26<00:00,  2.22it/s, loss=0.127] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3409\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 58/58 [00:26<00:00,  2.21it/s, loss=0.154] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9270\n",
      "Test Metrics: Precision=0.9337, Recall=0.9337, F1=0.9337\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8988.83 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7324.35 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9478.59 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:26<00:00,  2.16it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.3778\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:27<00:00,  2.14it/s, loss=0.0749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.4863\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:26<00:00,  2.21it/s, loss=0.121] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4642\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 58/58 [00:26<00:00,  2.17it/s, loss=0.115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7256\n",
      "Test Metrics: Precision=0.9350, Recall=0.9350, F1=0.9350\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8719.81 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7923.53 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9557.70 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:27<00:00,  2.14it/s, loss=0.198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.2376\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:27<00:00,  2.12it/s, loss=0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.0997\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:27<00:00,  2.09it/s, loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.5917\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 58/58 [00:27<00:00,  2.08it/s, loss=0.159] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.1610\n",
      "Test Metrics: Precision=0.9341, Recall=0.9341, F1=0.9341\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8742.92 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7742.29 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9287.17 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:27<00:00,  2.10it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.3719\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:26<00:00,  2.15it/s, loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.5868\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:27<00:00,  2.11it/s, loss=0.0376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0034\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 58/58 [00:27<00:00,  2.14it/s, loss=0.0747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1271\n",
      "Test Metrics: Precision=0.9349, Recall=0.9349, F1=0.9349\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8818.17 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7168.68 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9631.33 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:28<00:00,  2.00it/s, loss=0.303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.6591\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:28<00:00,  2.05it/s, loss=0.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.6718\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:28<00:00,  2.02it/s, loss=0.213] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0556\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 58/58 [00:28<00:00,  2.02it/s, loss=0.0812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0937\n",
      "Test Metrics: Precision=0.9390, Recall=0.9390, F1=0.9390\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 465, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 465/465 [00:00<00:00, 9044.65 examples/s]\n",
      "Map: 100%|██████████| 93/93 [00:00<00:00, 8071.81 examples/s]\n",
      "Map: 100%|██████████| 690/690 [00:00<00:00, 9295.86 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:26<00:00,  2.26it/s, loss=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.8064\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:25<00:00,  2.31it/s, loss=0.164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.7625\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:26<00:00,  2.22it/s, loss=0.0852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.5554\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:26<00:00,  2.25it/s, loss=0.0657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.0561\n",
      "Test Metrics: Precision=0.9332, Recall=0.9332, F1=0.9332\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 465, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 465/465 [00:00<00:00, 9169.02 examples/s]\n",
      "Map: 100%|██████████| 93/93 [00:00<00:00, 7655.79 examples/s]\n",
      "Map: 100%|██████████| 690/690 [00:00<00:00, 9412.10 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:26<00:00,  2.21it/s, loss=0.155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.1652\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:27<00:00,  2.14it/s, loss=0.0609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.7374\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:27<00:00,  2.16it/s, loss=0.0177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9362\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:27<00:00,  2.17it/s, loss=0.00463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0688\n",
      "Test Metrics: Precision=0.9333, Recall=0.9333, F1=0.9333\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 465, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 465/465 [00:00<00:00, 8680.54 examples/s]\n",
      "Map: 100%|██████████| 93/93 [00:00<00:00, 7989.64 examples/s]\n",
      "Map: 100%|██████████| 690/690 [00:00<00:00, 9356.81 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:28<00:00,  2.09it/s, loss=0.255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.6627\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:28<00:00,  2.07it/s, loss=0.0878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.3224\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:27<00:00,  2.13it/s, loss=0.0519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.4647\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:28<00:00,  2.05it/s, loss=0.104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3671\n",
      "Test Metrics: Precision=0.9352, Recall=0.9352, F1=0.9352\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 465, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 465/465 [00:00<00:00, 8597.46 examples/s]\n",
      "Map: 100%|██████████| 93/93 [00:00<00:00, 8298.84 examples/s]\n",
      "Map: 100%|██████████| 690/690 [00:00<00:00, 9189.48 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:27<00:00,  2.14it/s, loss=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.2837\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:27<00:00,  2.16it/s, loss=0.136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.0960\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:27<00:00,  2.11it/s, loss=0.0959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2313\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:27<00:00,  2.16it/s, loss=0.0125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3706\n",
      "Test Metrics: Precision=0.9354, Recall=0.9354, F1=0.9354\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 465, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 465/465 [00:00<00:00, 8603.83 examples/s]\n",
      "Map: 100%|██████████| 93/93 [00:00<00:00, 6918.96 examples/s]\n",
      "Map: 100%|██████████| 690/690 [00:00<00:00, 9205.26 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:28<00:00,  2.04it/s, loss=0.137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.1011\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:28<00:00,  2.05it/s, loss=0.0675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2605\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:28<00:00,  2.08it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9218\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:29<00:00,  2.02it/s, loss=0.186] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.6150\n",
      "Test Metrics: Precision=0.9387, Recall=0.9387, F1=0.9387\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 470, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 470/470 [00:00<00:00, 9349.94 examples/s]\n",
      "Map: 100%|██████████| 94/94 [00:00<00:00, 7569.20 examples/s]\n",
      "Map: 100%|██████████| 684/684 [00:00<00:00, 9755.82 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:26<00:00,  2.25it/s, loss=0.376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.1797\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:26<00:00,  2.21it/s, loss=0.221] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.6013\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:26<00:00,  2.19it/s, loss=0.0729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.2756\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:26<00:00,  2.23it/s, loss=0.126] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0400\n",
      "Test Metrics: Precision=0.9341, Recall=0.9341, F1=0.9341\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 470, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 470/470 [00:00<00:00, 9153.23 examples/s]\n",
      "Map: 100%|██████████| 94/94 [00:00<00:00, 7410.57 examples/s]\n",
      "Map: 100%|██████████| 684/684 [00:00<00:00, 9419.86 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:27<00:00,  2.14it/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.2514\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:27<00:00,  2.17it/s, loss=0.165] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.9936\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:26<00:00,  2.19it/s, loss=0.0942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4757\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:27<00:00,  2.14it/s, loss=0.0803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5994\n",
      "Test Metrics: Precision=0.9367, Recall=0.9367, F1=0.9367\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 470, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 470/470 [00:00<00:00, 8526.08 examples/s]\n",
      "Map: 100%|██████████| 94/94 [00:00<00:00, 8394.32 examples/s]\n",
      "Map: 100%|██████████| 684/684 [00:00<00:00, 9470.71 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:29<00:00,  2.03it/s, loss=0.158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.1741\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:27<00:00,  2.12it/s, loss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.7337\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:27<00:00,  2.11it/s, loss=0.129] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3096\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:28<00:00,  2.06it/s, loss=0.0624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5725\n",
      "Test Metrics: Precision=0.9357, Recall=0.9357, F1=0.9357\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 470, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 470/470 [00:00<00:00, 8769.58 examples/s]\n",
      "Map: 100%|██████████| 94/94 [00:00<00:00, 7670.96 examples/s]\n",
      "Map: 100%|██████████| 684/684 [00:00<00:00, 9239.60 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:27<00:00,  2.13it/s, loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.5497\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:27<00:00,  2.14it/s, loss=0.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.9585\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:26<00:00,  2.19it/s, loss=0.109] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.1928\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:27<00:00,  2.16it/s, loss=0.159] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.9599\n",
      "Test Metrics: Precision=0.9303, Recall=0.9303, F1=0.9303\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 470, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 470/470 [00:00<00:00, 8739.53 examples/s]\n",
      "Map: 100%|██████████| 94/94 [00:00<00:00, 7476.19 examples/s]\n",
      "Map: 100%|██████████| 684/684 [00:00<00:00, 9479.53 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:28<00:00,  2.05it/s, loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.4211\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:29<00:00,  1.99it/s, loss=0.18] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1680\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:29<00:00,  2.03it/s, loss=0.192] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.2563\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:29<00:00,  1.99it/s, loss=0.156] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7802\n",
      "Test Metrics: Precision=0.9397, Recall=0.9397, F1=0.9397\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 475, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 475/475 [00:00<00:00, 8873.18 examples/s]\n",
      "Map: 100%|██████████| 95/95 [00:00<00:00, 6888.39 examples/s]\n",
      "Map: 100%|██████████| 678/678 [00:00<00:00, 9389.67 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:26<00:00,  2.28it/s, loss=0.811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.8854\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:26<00:00,  2.28it/s, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.1310\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:26<00:00,  2.24it/s, loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.1070\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:26<00:00,  2.27it/s, loss=0.0932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 8.9778\n",
      "Test Metrics: Precision=0.9273, Recall=0.9273, F1=0.9273\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 475, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 475/475 [00:00<00:00, 8772.53 examples/s]\n",
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7251.69 examples/s]\n",
      "Map: 100%|██████████| 678/678 [00:00<00:00, 9464.68 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:27<00:00,  2.16it/s, loss=0.0743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.0204\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:28<00:00,  2.13it/s, loss=0.0405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.0065\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:27<00:00,  2.18it/s, loss=0.0765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.8266\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:27<00:00,  2.22it/s, loss=0.0995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.7942\n",
      "Test Metrics: Precision=0.9328, Recall=0.9328, F1=0.9328\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 475, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 475/475 [00:00<00:00, 8496.43 examples/s]\n",
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7077.93 examples/s]\n",
      "Map: 100%|██████████| 678/678 [00:00<00:00, 9451.69 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:29<00:00,  2.03it/s, loss=0.247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.8289\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:28<00:00,  2.09it/s, loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.0326\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:29<00:00,  2.06it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.4364\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:28<00:00,  2.11it/s, loss=0.0417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.4956\n",
      "Test Metrics: Precision=0.9335, Recall=0.9335, F1=0.9335\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 475, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 475/475 [00:00<00:00, 8618.79 examples/s]\n",
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7196.17 examples/s]\n",
      "Map: 100%|██████████| 678/678 [00:00<00:00, 9661.77 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:28<00:00,  2.11it/s, loss=0.308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.5412\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:28<00:00,  2.11it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2752\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:28<00:00,  2.13it/s, loss=0.108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7070\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:27<00:00,  2.18it/s, loss=0.0528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0971\n",
      "Test Metrics: Precision=0.9343, Recall=0.9343, F1=0.9343\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 475, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 475/475 [00:00<00:00, 8742.81 examples/s]\n",
      "Map: 100%|██████████| 95/95 [00:00<00:00, 6778.36 examples/s]\n",
      "Map: 100%|██████████| 678/678 [00:00<00:00, 9569.72 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:29<00:00,  2.04it/s, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.4650\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:29<00:00,  2.05it/s, loss=0.313] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.3487\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:28<00:00,  2.13it/s, loss=0.057] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6092\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:28<00:00,  2.11it/s, loss=0.125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0818\n",
      "Test Metrics: Precision=0.9370, Recall=0.9370, F1=0.9370\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 9042.90 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7029.68 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9279.01 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:26<00:00,  2.26it/s, loss=0.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.9595\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:27<00:00,  2.21it/s, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.0560\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:26<00:00,  2.23it/s, loss=0.227] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.3498\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:26<00:00,  2.25it/s, loss=0.108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 9.0702\n",
      "Test Metrics: Precision=0.9289, Recall=0.9289, F1=0.9289\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 9023.20 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7716.77 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9022.95 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:28<00:00,  2.14it/s, loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.1911\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:28<00:00,  2.11it/s, loss=0.327] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.5385\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:28<00:00,  2.11it/s, loss=0.289] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.4241\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:28<00:00,  2.14it/s, loss=0.131] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.8396\n",
      "Test Metrics: Precision=0.9346, Recall=0.9346, F1=0.9346\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8694.21 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7281.38 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9640.30 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:28<00:00,  2.12it/s, loss=0.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.8414\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:29<00:00,  2.04it/s, loss=0.324] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.4589\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:29<00:00,  2.04it/s, loss=0.0905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7004\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:28<00:00,  2.09it/s, loss=0.139] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9690\n",
      "Test Metrics: Precision=0.9366, Recall=0.9366, F1=0.9366\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8832.67 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7045.18 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9289.56 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:28<00:00,  2.08it/s, loss=0.244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.5062\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:27<00:00,  2.17it/s, loss=0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.7462\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:28<00:00,  2.09it/s, loss=0.199] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3151\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:28<00:00,  2.13it/s, loss=0.084] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0868\n",
      "Test Metrics: Precision=0.9350, Recall=0.9350, F1=0.9350\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8844.97 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 8004.56 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9236.62 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:29<00:00,  2.00it/s, loss=0.257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.1819\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:30<00:00,  1.99it/s, loss=0.206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.7717\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:28<00:00,  2.07it/s, loss=0.155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.8600\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:29<00:00,  2.06it/s, loss=0.259] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 10.0555\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 485, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 485/485 [00:00<00:00, 9051.88 examples/s]\n",
      "Map: 100%|██████████| 97/97 [00:00<00:00, 7704.13 examples/s]\n",
      "Map: 100%|██████████| 666/666 [00:00<00:00, 9269.43 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 61/61 [00:27<00:00,  2.24it/s, loss=0.513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.9220\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 61/61 [00:27<00:00,  2.21it/s, loss=0.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.0988\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 61/61 [00:27<00:00,  2.19it/s, loss=0.198] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.9813\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 61/61 [00:26<00:00,  2.31it/s, loss=0.167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 9.1461\n",
      "Test Metrics: Precision=0.9305, Recall=0.9305, F1=0.9305\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 485, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 485/485 [00:00<00:00, 9294.57 examples/s]\n",
      "Map: 100%|██████████| 97/97 [00:00<00:00, 7479.09 examples/s]\n",
      "Map: 100%|██████████| 666/666 [00:00<00:00, 9220.53 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 61/61 [00:28<00:00,  2.14it/s, loss=0.221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.0264\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 61/61 [00:27<00:00,  2.19it/s, loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.1466\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 61/61 [00:28<00:00,  2.12it/s, loss=0.164] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.5001\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 61/61 [00:27<00:00,  2.23it/s, loss=0.154] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.5763\n",
      "Test Metrics: Precision=0.9345, Recall=0.9345, F1=0.9345\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 485, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 485/485 [00:00<00:00, 8488.05 examples/s]\n",
      "Map: 100%|██████████| 97/97 [00:00<00:00, 8728.01 examples/s]\n",
      "Map: 100%|██████████| 666/666 [00:00<00:00, 9655.28 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 61/61 [00:29<00:00,  2.07it/s, loss=0.192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.5770\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 61/61 [00:29<00:00,  2.04it/s, loss=0.397] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.8501\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 61/61 [00:29<00:00,  2.10it/s, loss=0.221] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5951\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 61/61 [00:29<00:00,  2.06it/s, loss=0.0889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1206\n",
      "Test Metrics: Precision=0.9367, Recall=0.9367, F1=0.9367\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 485, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 485/485 [00:00<00:00, 8844.20 examples/s]\n",
      "Map: 100%|██████████| 97/97 [00:00<00:00, 7360.03 examples/s]\n",
      "Map: 100%|██████████| 666/666 [00:00<00:00, 9409.78 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 61/61 [00:28<00:00,  2.12it/s, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.3633\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 61/61 [00:28<00:00,  2.14it/s, loss=0.212] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.6283\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 61/61 [00:28<00:00,  2.14it/s, loss=0.155] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5982\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 61/61 [00:28<00:00,  2.12it/s, loss=0.0906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0170\n",
      "Test Metrics: Precision=0.9356, Recall=0.9356, F1=0.9356\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 485, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 485/485 [00:00<00:00, 8700.28 examples/s]\n",
      "Map: 100%|██████████| 97/97 [00:00<00:00, 7309.38 examples/s]\n",
      "Map: 100%|██████████| 666/666 [00:00<00:00, 9348.56 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 61/61 [00:29<00:00,  2.08it/s, loss=0.582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.1729\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 61/61 [00:28<00:00,  2.11it/s, loss=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.6257\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 61/61 [00:29<00:00,  2.09it/s, loss=0.133] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.6456\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 61/61 [00:29<00:00,  2.10it/s, loss=0.179] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 8.7257\n",
      "Test Metrics: Precision=0.9363, Recall=0.9363, F1=0.9363\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 490, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 490/490 [00:00<00:00, 8986.76 examples/s]\n",
      "Map: 100%|██████████| 98/98 [00:00<00:00, 7572.34 examples/s]\n",
      "Map: 100%|██████████| 660/660 [00:00<00:00, 9565.12 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:27<00:00,  2.24it/s, loss=0.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.4700\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:27<00:00,  2.24it/s, loss=0.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.6687\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:27<00:00,  2.28it/s, loss=0.135] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.8543\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:27<00:00,  2.27it/s, loss=0.0316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.5252\n",
      "Test Metrics: Precision=0.9338, Recall=0.9338, F1=0.9338\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 490, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 490/490 [00:00<00:00, 9220.94 examples/s]\n",
      "Map: 100%|██████████| 98/98 [00:00<00:00, 7614.71 examples/s]\n",
      "Map: 100%|██████████| 660/660 [00:00<00:00, 9001.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:28<00:00,  2.16it/s, loss=0.497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.8881\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:28<00:00,  2.17it/s, loss=0.311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.4914\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:29<00:00,  2.13it/s, loss=0.0881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.9979\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:27<00:00,  2.22it/s, loss=0.225] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.1194\n",
      "Test Metrics: Precision=0.9351, Recall=0.9351, F1=0.9351\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 490, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 490/490 [00:00<00:00, 8641.29 examples/s]\n",
      "Map: 100%|██████████| 98/98 [00:00<00:00, 8302.70 examples/s]\n",
      "Map: 100%|██████████| 660/660 [00:00<00:00, 9080.07 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:29<00:00,  2.09it/s, loss=0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.3996\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:29<00:00,  2.09it/s, loss=0.138] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2683\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:29<00:00,  2.09it/s, loss=0.133] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7368\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:29<00:00,  2.11it/s, loss=0.196] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0232\n",
      "Test Metrics: Precision=0.9365, Recall=0.9365, F1=0.9365\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 490, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 490/490 [00:00<00:00, 8942.07 examples/s]\n",
      "Map: 100%|██████████| 98/98 [00:00<00:00, 8306.22 examples/s]\n",
      "Map: 100%|██████████| 660/660 [00:00<00:00, 9678.28 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:28<00:00,  2.14it/s, loss=0.879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.0513\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:29<00:00,  2.12it/s, loss=0.103] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.9625\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:28<00:00,  2.14it/s, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.5266\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:28<00:00,  2.16it/s, loss=0.0308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.7110\n",
      "Test Metrics: Precision=0.9332, Recall=0.9332, F1=0.9332\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 490, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 490/490 [00:00<00:00, 8879.29 examples/s]\n",
      "Map: 100%|██████████| 98/98 [00:00<00:00, 8012.98 examples/s]\n",
      "Map: 100%|██████████| 660/660 [00:00<00:00, 9420.01 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:29<00:00,  2.11it/s, loss=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.9901\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:30<00:00,  2.03it/s, loss=0.111] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.9113\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:29<00:00,  2.09it/s, loss=0.64]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.9020\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:30<00:00,  2.05it/s, loss=0.109] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.1799\n",
      "Test Metrics: Precision=0.9388, Recall=0.9388, F1=0.9388\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 495, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 495/495 [00:00<00:00, 9167.90 examples/s]\n",
      "Map: 100%|██████████| 99/99 [00:00<00:00, 7715.56 examples/s]\n",
      "Map: 100%|██████████| 654/654 [00:00<00:00, 8965.62 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:27<00:00,  2.24it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.7847\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:26<00:00,  2.30it/s, loss=0.212] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.5569\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:27<00:00,  2.24it/s, loss=0.126] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.6253\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:27<00:00,  2.22it/s, loss=0.0919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0865\n",
      "Test Metrics: Precision=0.9349, Recall=0.9349, F1=0.9349\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 495, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 495/495 [00:00<00:00, 9313.86 examples/s]\n",
      "Map: 100%|██████████| 99/99 [00:00<00:00, 8077.58 examples/s]\n",
      "Map: 100%|██████████| 654/654 [00:00<00:00, 9012.66 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:29<00:00,  2.11it/s, loss=0.405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.1485\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:29<00:00,  2.11it/s, loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.0710\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:28<00:00,  2.16it/s, loss=0.217] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.5287\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:28<00:00,  2.15it/s, loss=0.109] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 8.2710\n",
      "Test Metrics: Precision=0.9342, Recall=0.9342, F1=0.9342\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 495, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 495/495 [00:00<00:00, 8772.06 examples/s]\n",
      "Map: 100%|██████████| 99/99 [00:00<00:00, 7858.07 examples/s]\n",
      "Map: 100%|██████████| 654/654 [00:00<00:00, 9658.20 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:30<00:00,  2.04it/s, loss=0.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.6555\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:29<00:00,  2.07it/s, loss=0.13]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.3169\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:29<00:00,  2.08it/s, loss=0.133] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8697\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:30<00:00,  2.06it/s, loss=0.0993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1419\n",
      "Test Metrics: Precision=0.9350, Recall=0.9350, F1=0.9350\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 495, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 495/495 [00:00<00:00, 8773.88 examples/s]\n",
      "Map: 100%|██████████| 99/99 [00:00<00:00, 7973.96 examples/s]\n",
      "Map: 100%|██████████| 654/654 [00:00<00:00, 9291.50 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:29<00:00,  2.13it/s, loss=0.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.6965\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:29<00:00,  2.12it/s, loss=0.248] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.3764\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:28<00:00,  2.18it/s, loss=0.0985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.7349\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:28<00:00,  2.14it/s, loss=0.0849]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.0286\n",
      "Test Metrics: Precision=0.9327, Recall=0.9327, F1=0.9327\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 495, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 495/495 [00:00<00:00, 8867.22 examples/s]\n",
      "Map: 100%|██████████| 99/99 [00:00<00:00, 7564.46 examples/s]\n",
      "Map: 100%|██████████| 654/654 [00:00<00:00, 9477.90 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:31<00:00,  1.98it/s, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.0850\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:31<00:00,  1.97it/s, loss=0.24]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.6247\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:30<00:00,  2.03it/s, loss=0.0979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.8165\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:29<00:00,  2.07it/s, loss=0.0821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.6731\n",
      "Test Metrics: Precision=0.9392, Recall=0.9392, F1=0.9392\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9000.69 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7631.56 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 3407.34 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:27<00:00,  2.27it/s, loss=0.265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.4240\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:27<00:00,  2.26it/s, loss=0.198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1369\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:28<00:00,  2.23it/s, loss=0.144] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9358\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 63/63 [00:27<00:00,  2.31it/s, loss=0.0825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3657\n",
      "Test Metrics: Precision=0.9349, Recall=0.9349, F1=0.9349\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8887.14 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7756.46 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9022.40 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:28<00:00,  2.20it/s, loss=0.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.0617\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:28<00:00,  2.22it/s, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.1360\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:28<00:00,  2.18it/s, loss=0.0442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.4000\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 63/63 [00:28<00:00,  2.23it/s, loss=0.0716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.4198\n",
      "Test Metrics: Precision=0.9347, Recall=0.9347, F1=0.9347\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8697.54 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8337.25 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9606.29 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:30<00:00,  2.06it/s, loss=0.175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.4008\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:30<00:00,  2.07it/s, loss=0.208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.4994\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:30<00:00,  2.06it/s, loss=0.207] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.1451\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 63/63 [00:30<00:00,  2.07it/s, loss=0.0622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.4835\n",
      "Test Metrics: Precision=0.9350, Recall=0.9350, F1=0.9350\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8909.45 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7265.76 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9401.24 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:28<00:00,  2.19it/s, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.9188\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:29<00:00,  2.16it/s, loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.8515\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:29<00:00,  2.15it/s, loss=0.476] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.5943\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 63/63 [00:28<00:00,  2.17it/s, loss=0.0593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.8765\n",
      "Test Metrics: Precision=0.9336, Recall=0.9336, F1=0.9336\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8821.46 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8667.71 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9040.41 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:30<00:00,  2.07it/s, loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.1973\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:30<00:00,  2.06it/s, loss=0.217] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.6231\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:29<00:00,  2.12it/s, loss=0.196] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.5744\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 63/63 [00:31<00:00,  2.01it/s, loss=0.0924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.6449\n",
      "Test Metrics: Precision=0.9372, Recall=0.9372, F1=0.9372\n",
      "Results saved to Experiments_epoch4_roberta.xlsx\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"large\": \"roberta-large\"\n",
    "}\n",
    "\n",
    "iterate_and_finetune_with_torch(dataset=dataset, file_name='Experiments_epoch4_roberta.xlsx', models=models, start_size=230, end_size=500, step_size=5, num_epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7304c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 210, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 210/210 [00:00<00:00, 7713.02 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 6234.23 examples/s]\n",
      "Map: 100%|██████████| 996/996 [00:00<00:00, 9574.06 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 27/27 [00:11<00:00,  2.35it/s, loss=0.356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.5578\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 27/27 [00:11<00:00,  2.34it/s, loss=0.255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.8953\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 27/27 [00:11<00:00,  2.29it/s, loss=0.0559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4315\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 27/27 [00:11<00:00,  2.30it/s, loss=0.173] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.0549\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 27/27 [00:12<00:00,  2.22it/s, loss=0.0975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.2279\n",
      "Test Metrics: Precision=0.9260, Recall=0.9260, F1=0.9260\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 210, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 210/210 [00:00<00:00, 8324.71 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 6689.48 examples/s]\n",
      "Map: 100%|██████████| 996/996 [00:00<00:00, 9636.87 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 27/27 [00:12<00:00,  2.21it/s, loss=0.691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.0394\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 27/27 [00:12<00:00,  2.15it/s, loss=0.452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.1653\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 27/27 [00:11<00:00,  2.33it/s, loss=0.0417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0342\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 27/27 [00:11<00:00,  2.32it/s, loss=0.0639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.9956\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 27/27 [00:10<00:00,  2.46it/s, loss=0.184] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.1816\n",
      "Test Metrics: Precision=0.9272, Recall=0.9272, F1=0.9272\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 210, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 210/210 [00:00<00:00, 8533.02 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 6102.29 examples/s]\n",
      "Map: 100%|██████████| 996/996 [00:00<00:00, 9614.58 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 27/27 [00:11<00:00,  2.34it/s, loss=0.391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.1447\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 27/27 [00:11<00:00,  2.26it/s, loss=0.458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.0076\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 27/27 [00:11<00:00,  2.32it/s, loss=0.33] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.6543\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 27/27 [00:11<00:00,  2.29it/s, loss=0.101] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.5945\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 27/27 [00:11<00:00,  2.33it/s, loss=0.129] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.6827\n",
      "Test Metrics: Precision=0.9286, Recall=0.9286, F1=0.9286\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 210, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 210/210 [00:00<00:00, 8605.72 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 5224.69 examples/s]\n",
      "Map: 100%|██████████| 996/996 [00:00<00:00, 9421.45 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 27/27 [00:10<00:00,  2.47it/s, loss=0.536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.8096\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 27/27 [00:10<00:00,  2.49it/s, loss=0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.7420\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 27/27 [00:10<00:00,  2.46it/s, loss=0.199] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.1734\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 27/27 [00:11<00:00,  2.38it/s, loss=0.0161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.3721\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 27/27 [00:11<00:00,  2.34it/s, loss=0.0976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.4677\n",
      "Test Metrics: Precision=0.9266, Recall=0.9266, F1=0.9266\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 210, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 210/210 [00:00<00:00, 8590.53 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 6016.21 examples/s]\n",
      "Map: 100%|██████████| 996/996 [00:00<00:00, 9837.37 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 27/27 [00:13<00:00,  1.94it/s, loss=0.246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.9874\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 27/27 [00:14<00:00,  1.92it/s, loss=1.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.4822\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 27/27 [00:14<00:00,  1.91it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8966\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 27/27 [00:14<00:00,  1.85it/s, loss=0.0721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1915\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 27/27 [00:13<00:00,  2.06it/s, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.4290\n",
      "Test Metrics: Precision=0.9248, Recall=0.9248, F1=0.9248\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 215, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 215/215 [00:00<00:00, 8430.80 examples/s]\n",
      "Map: 100%|██████████| 43/43 [00:00<00:00, 6080.75 examples/s]\n",
      "Map: 100%|██████████| 990/990 [00:00<00:00, 9501.58 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 27/27 [00:12<00:00,  2.25it/s, loss=0.579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.5754\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 27/27 [00:11<00:00,  2.32it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.5765\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 27/27 [00:11<00:00,  2.28it/s, loss=0.16]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.8402\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 27/27 [00:12<00:00,  2.19it/s, loss=0.0794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.3644\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 27/27 [00:12<00:00,  2.23it/s, loss=0.0991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.5844\n",
      "Test Metrics: Precision=0.9285, Recall=0.9285, F1=0.9285\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 215, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 215/215 [00:00<00:00, 8860.92 examples/s]\n",
      "Map: 100%|██████████| 43/43 [00:00<00:00, 5865.40 examples/s]\n",
      "Map: 100%|██████████| 990/990 [00:00<00:00, 9679.32 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 27/27 [00:12<00:00,  2.21it/s, loss=0.395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.4168\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 27/27 [00:12<00:00,  2.19it/s, loss=0.272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.8689\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 27/27 [00:11<00:00,  2.33it/s, loss=0.117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.8602\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 27/27 [00:12<00:00,  2.10it/s, loss=0.151] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.9754\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 27/27 [00:12<00:00,  2.23it/s, loss=0.063] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.0744\n",
      "Test Metrics: Precision=0.9243, Recall=0.9243, F1=0.9243\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 215, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 215/215 [00:00<00:00, 8200.79 examples/s]\n",
      "Map: 100%|██████████| 43/43 [00:00<00:00, 5722.11 examples/s]\n",
      "Map: 100%|██████████| 990/990 [00:00<00:00, 9436.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 27/27 [00:13<00:00,  2.03it/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.4871\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 27/27 [00:12<00:00,  2.11it/s, loss=0.448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.1738\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 27/27 [00:13<00:00,  2.06it/s, loss=0.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.7201\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 27/27 [00:13<00:00,  2.06it/s, loss=0.116] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.9588\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 27/27 [00:13<00:00,  2.03it/s, loss=0.135] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.0854\n",
      "Test Metrics: Precision=0.9289, Recall=0.9289, F1=0.9289\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 215, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 215/215 [00:00<00:00, 8467.53 examples/s]\n",
      "Map: 100%|██████████| 43/43 [00:00<00:00, 5529.82 examples/s]\n",
      "Map: 100%|██████████| 990/990 [00:00<00:00, 9459.50 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 27/27 [00:12<00:00,  2.17it/s, loss=0.387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.4364\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 27/27 [00:12<00:00,  2.19it/s, loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.7330\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 27/27 [00:12<00:00,  2.17it/s, loss=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3836\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 27/27 [00:12<00:00,  2.22it/s, loss=0.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1570\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 27/27 [00:11<00:00,  2.27it/s, loss=0.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.9296\n",
      "Test Metrics: Precision=0.9235, Recall=0.9235, F1=0.9235\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 215, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 215/215 [00:00<00:00, 8422.93 examples/s]\n",
      "Map: 100%|██████████| 43/43 [00:00<00:00, 5986.10 examples/s]\n",
      "Map: 100%|██████████| 990/990 [00:00<00:00, 9553.96 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 27/27 [00:13<00:00,  1.95it/s, loss=0.557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.7233\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 27/27 [00:13<00:00,  1.95it/s, loss=0.432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.3666\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 27/27 [00:13<00:00,  2.01it/s, loss=0.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1785\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 27/27 [00:14<00:00,  1.87it/s, loss=0.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.2361\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 27/27 [00:13<00:00,  1.99it/s, loss=0.114] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.4643\n",
      "Test Metrics: Precision=0.9243, Recall=0.9243, F1=0.9243\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 220, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 220/220 [00:00<00:00, 8347.63 examples/s]\n",
      "Map: 100%|██████████| 44/44 [00:00<00:00, 5154.29 examples/s]\n",
      "Map: 100%|██████████| 984/984 [00:00<00:00, 9312.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 28/28 [00:11<00:00,  2.38it/s, loss=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.9334\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 28/28 [00:12<00:00,  2.20it/s, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.4079\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 28/28 [00:12<00:00,  2.32it/s, loss=0.242] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.6585\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 28/28 [00:11<00:00,  2.36it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.1387\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 28/28 [00:12<00:00,  2.28it/s, loss=0.0739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.0703\n",
      "Test Metrics: Precision=0.9265, Recall=0.9265, F1=0.9265\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 220, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 220/220 [00:00<00:00, 8350.57 examples/s]\n",
      "Map: 100%|██████████| 44/44 [00:00<00:00, 6894.15 examples/s]\n",
      "Map: 100%|██████████| 984/984 [00:00<00:00, 9368.96 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 28/28 [00:12<00:00,  2.25it/s, loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.5868\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 28/28 [00:12<00:00,  2.31it/s, loss=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.7428\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 28/28 [00:11<00:00,  2.37it/s, loss=0.225] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.6603\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 28/28 [00:13<00:00,  2.11it/s, loss=0.141] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.9163\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 28/28 [00:12<00:00,  2.21it/s, loss=0.0735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.8185\n",
      "Test Metrics: Precision=0.9279, Recall=0.9279, F1=0.9279\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 220, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 220/220 [00:00<00:00, 8667.55 examples/s]\n",
      "Map: 100%|██████████| 44/44 [00:00<00:00, 5464.57 examples/s]\n",
      "Map: 100%|██████████| 984/984 [00:00<00:00, 9823.19 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 28/28 [00:13<00:00,  2.13it/s, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.1874\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 28/28 [00:12<00:00,  2.21it/s, loss=0.577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.2300\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 28/28 [00:12<00:00,  2.18it/s, loss=0.272] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.0817\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 28/28 [00:13<00:00,  2.13it/s, loss=0.139] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.4534\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 28/28 [00:12<00:00,  2.17it/s, loss=0.0455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.5801\n",
      "Test Metrics: Precision=0.9287, Recall=0.9287, F1=0.9287\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 220, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 220/220 [00:00<00:00, 8357.08 examples/s]\n",
      "Map: 100%|██████████| 44/44 [00:00<00:00, 6773.95 examples/s]\n",
      "Map: 100%|██████████| 984/984 [00:00<00:00, 9421.31 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 28/28 [00:12<00:00,  2.25it/s, loss=0.318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.7594\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 28/28 [00:12<00:00,  2.19it/s, loss=0.229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.9065\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 28/28 [00:12<00:00,  2.29it/s, loss=0.223] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.1017\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 28/28 [00:12<00:00,  2.20it/s, loss=0.0691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.4459\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 28/28 [00:13<00:00,  2.10it/s, loss=0.071] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.6755\n",
      "Test Metrics: Precision=0.9255, Recall=0.9255, F1=0.9255\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 220, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 220/220 [00:00<00:00, 8460.37 examples/s]\n",
      "Map: 100%|██████████| 44/44 [00:00<00:00, 6349.54 examples/s]\n",
      "Map: 100%|██████████| 984/984 [00:00<00:00, 9700.73 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 28/28 [00:14<00:00,  1.96it/s, loss=0.387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.0134\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 28/28 [00:13<00:00,  2.07it/s, loss=0.0494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.1305\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 28/28 [00:14<00:00,  1.94it/s, loss=0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.7263\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 28/28 [00:14<00:00,  1.98it/s, loss=0.318] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.1233\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 28/28 [00:14<00:00,  1.97it/s, loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.1694\n",
      "Test Metrics: Precision=0.9304, Recall=0.9304, F1=0.9304\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 225, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 225/225 [00:00<00:00, 8596.45 examples/s]\n",
      "Map: 100%|██████████| 45/45 [00:00<00:00, 6338.36 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 9415.58 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 29/29 [00:12<00:00,  2.26it/s, loss=1.05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.3272\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 29/29 [00:12<00:00,  2.31it/s, loss=0.116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.2728\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 29/29 [00:12<00:00,  2.32it/s, loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4795\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 29/29 [00:12<00:00,  2.27it/s, loss=0.17]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.0713\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 29/29 [00:13<00:00,  2.21it/s, loss=0.00621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.8062\n",
      "Test Metrics: Precision=0.9288, Recall=0.9288, F1=0.9288\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 225, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 225/225 [00:00<00:00, 8513.86 examples/s]\n",
      "Map: 100%|██████████| 45/45 [00:00<00:00, 5652.19 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 9553.69 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 29/29 [00:13<00:00,  2.14it/s, loss=0.591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.8248\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 29/29 [00:13<00:00,  2.17it/s, loss=0.635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.6327\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 29/29 [00:12<00:00,  2.28it/s, loss=0.655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.7986\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 29/29 [00:13<00:00,  2.19it/s, loss=0.295] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7849\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 29/29 [00:13<00:00,  2.21it/s, loss=0.274] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.8631\n",
      "Test Metrics: Precision=0.9242, Recall=0.9242, F1=0.9242\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 225, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 225/225 [00:00<00:00, 8485.38 examples/s]\n",
      "Map: 100%|██████████| 45/45 [00:00<00:00, 6718.05 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 9370.43 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 29/29 [00:12<00:00,  2.25it/s, loss=0.379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.9095\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 29/29 [00:13<00:00,  2.10it/s, loss=0.247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.0172\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 29/29 [00:13<00:00,  2.14it/s, loss=0.32] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.8554\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 29/29 [00:13<00:00,  2.18it/s, loss=0.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 10.2048\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 29/29 [00:12<00:00,  2.24it/s, loss=0.399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 9.4754\n",
      "Test Metrics: Precision=0.8790, Recall=0.8790, F1=0.8790\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 225, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 225/225 [00:00<00:00, 8500.13 examples/s]\n",
      "Map: 100%|██████████| 45/45 [00:00<00:00, 5641.89 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 9232.09 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 29/29 [00:12<00:00,  2.26it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.4271\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 29/29 [00:13<00:00,  2.23it/s, loss=0.167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.2790\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 29/29 [00:13<00:00,  2.16it/s, loss=0.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3116\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 29/29 [00:13<00:00,  2.18it/s, loss=0.312] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8269\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 29/29 [00:13<00:00,  2.23it/s, loss=0.0122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5039\n",
      "Test Metrics: Precision=0.9246, Recall=0.9246, F1=0.9246\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 225, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 225/225 [00:00<00:00, 8359.18 examples/s]\n",
      "Map: 100%|██████████| 45/45 [00:00<00:00, 5474.00 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 9653.83 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 29/29 [00:14<00:00,  2.04it/s, loss=0.325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.5336\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 29/29 [00:14<00:00,  2.02it/s, loss=0.415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.0705\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 29/29 [00:14<00:00,  2.05it/s, loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.2552\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 29/29 [00:14<00:00,  2.01it/s, loss=0.0364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.4464\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 29/29 [00:13<00:00,  2.08it/s, loss=0.0833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.3738\n",
      "Test Metrics: Precision=0.9293, Recall=0.9293, F1=0.9293\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 230, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 230/230 [00:00<00:00, 8435.92 examples/s]\n",
      "Map: 100%|██████████| 46/46 [00:00<00:00, 5809.64 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:00<00:00, 9450.35 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 29/29 [00:12<00:00,  2.25it/s, loss=0.506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.5089\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 29/29 [00:12<00:00,  2.26it/s, loss=0.179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.8045\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 29/29 [00:13<00:00,  2.22it/s, loss=0.149] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.1463\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 29/29 [00:13<00:00,  2.22it/s, loss=0.171] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.5019\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 29/29 [00:12<00:00,  2.26it/s, loss=0.103] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.4330\n",
      "Test Metrics: Precision=0.9280, Recall=0.9280, F1=0.9280\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 230, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 230/230 [00:00<00:00, 8425.46 examples/s]\n",
      "Map: 100%|██████████| 46/46 [00:00<00:00, 4488.60 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:00<00:00, 9630.55 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 29/29 [00:13<00:00,  2.22it/s, loss=0.415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.2018\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 29/29 [00:13<00:00,  2.15it/s, loss=0.368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.7137\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 29/29 [00:13<00:00,  2.19it/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.8320\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 29/29 [00:13<00:00,  2.08it/s, loss=0.173] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.8096\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 29/29 [00:13<00:00,  2.12it/s, loss=0.0478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.8524\n",
      "Test Metrics: Precision=0.9301, Recall=0.9301, F1=0.9301\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 230, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 230/230 [00:00<00:00, 8485.49 examples/s]\n",
      "Map: 100%|██████████| 46/46 [00:00<00:00, 5949.18 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:00<00:00, 9390.28 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 29/29 [00:13<00:00,  2.10it/s, loss=0.395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.8949\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 29/29 [00:13<00:00,  2.08it/s, loss=0.481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.6839\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 29/29 [00:13<00:00,  2.18it/s, loss=0.222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.9910\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 29/29 [00:13<00:00,  2.13it/s, loss=0.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1305\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 29/29 [00:13<00:00,  2.08it/s, loss=0.153] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.9694\n",
      "Test Metrics: Precision=0.9244, Recall=0.9244, F1=0.9244\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 230, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 230/230 [00:00<00:00, 8451.15 examples/s]\n",
      "Map: 100%|██████████| 46/46 [00:00<00:00, 5788.54 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:00<00:00, 4384.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 29/29 [00:13<00:00,  2.18it/s, loss=0.315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.1479\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 29/29 [00:13<00:00,  2.10it/s, loss=0.307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.2908\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 29/29 [00:14<00:00,  2.06it/s, loss=0.304] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.6306\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 29/29 [00:13<00:00,  2.13it/s, loss=0.125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.8956\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 29/29 [00:13<00:00,  2.11it/s, loss=0.0599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.9105\n",
      "Test Metrics: Precision=0.9263, Recall=0.9263, F1=0.9263\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 230, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 230/230 [00:00<00:00, 8295.55 examples/s]\n",
      "Map: 100%|██████████| 46/46 [00:00<00:00, 6904.95 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:00<00:00, 9123.40 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 29/29 [00:14<00:00,  2.03it/s, loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.4369\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 29/29 [00:14<00:00,  1.99it/s, loss=0.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.1030\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 29/29 [00:14<00:00,  2.06it/s, loss=0.142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1008\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 29/29 [00:14<00:00,  2.07it/s, loss=0.112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2712\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 29/29 [00:14<00:00,  2.03it/s, loss=0.17]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.2715\n",
      "Test Metrics: Precision=0.9273, Recall=0.9273, F1=0.9273\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 235, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 235/235 [00:00<00:00, 8451.26 examples/s]\n",
      "Map: 100%|██████████| 47/47 [00:00<00:00, 5939.33 examples/s]\n",
      "Map: 100%|██████████| 966/966 [00:00<00:00, 9232.64 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:13<00:00,  2.27it/s, loss=0.193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.2821\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:12<00:00,  2.32it/s, loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.3273\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:13<00:00,  2.26it/s, loss=0.0743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.7092\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:13<00:00,  2.21it/s, loss=0.171] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.0650\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 30/30 [00:12<00:00,  2.40it/s, loss=0.0935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.0635\n",
      "Test Metrics: Precision=0.9290, Recall=0.9290, F1=0.9290\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 235, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 235/235 [00:00<00:00, 8742.55 examples/s]\n",
      "Map: 100%|██████████| 47/47 [00:00<00:00, 6189.20 examples/s]\n",
      "Map: 100%|██████████| 966/966 [00:00<00:00, 9617.63 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:13<00:00,  2.19it/s, loss=0.446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.4111\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:13<00:00,  2.25it/s, loss=0.276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.0742\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:13<00:00,  2.21it/s, loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0711\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:13<00:00,  2.21it/s, loss=0.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1818\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 30/30 [00:13<00:00,  2.18it/s, loss=0.0633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.6521\n",
      "Test Metrics: Precision=0.9220, Recall=0.9220, F1=0.9220\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 235, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 235/235 [00:00<00:00, 8309.61 examples/s]\n",
      "Map: 100%|██████████| 47/47 [00:00<00:00, 6209.67 examples/s]\n",
      "Map: 100%|██████████| 966/966 [00:00<00:00, 9224.86 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:14<00:00,  2.11it/s, loss=0.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.2722\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:13<00:00,  2.16it/s, loss=0.179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.8781\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:13<00:00,  2.19it/s, loss=0.372] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.1773\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:14<00:00,  2.13it/s, loss=0.053] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.5136\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 30/30 [00:13<00:00,  2.23it/s, loss=0.0592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.6167\n",
      "Test Metrics: Precision=0.9263, Recall=0.9263, F1=0.9263\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 235, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 235/235 [00:00<00:00, 8769.46 examples/s]\n",
      "Map: 100%|██████████| 47/47 [00:00<00:00, 6564.51 examples/s]\n",
      "Map: 100%|██████████| 966/966 [00:00<00:00, 9366.33 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:14<00:00,  2.10it/s, loss=0.33] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.1275\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:13<00:00,  2.21it/s, loss=0.0975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.0554\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:14<00:00,  2.13it/s, loss=0.093] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.1228\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:13<00:00,  2.15it/s, loss=0.106] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7121\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 30/30 [00:13<00:00,  2.22it/s, loss=0.0877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.7277\n",
      "Test Metrics: Precision=0.9256, Recall=0.9256, F1=0.9256\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 235, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 235/235 [00:00<00:00, 8235.67 examples/s]\n",
      "Map: 100%|██████████| 47/47 [00:00<00:00, 6286.91 examples/s]\n",
      "Map: 100%|██████████| 966/966 [00:00<00:00, 9172.90 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:14<00:00,  2.02it/s, loss=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.8266\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:14<00:00,  2.00it/s, loss=0.619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.8112\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:14<00:00,  2.08it/s, loss=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8182\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:14<00:00,  2.04it/s, loss=0.169] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7342\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 30/30 [00:15<00:00,  2.00it/s, loss=0.118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5190\n",
      "Test Metrics: Precision=0.9303, Recall=0.9303, F1=0.9303\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 240, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 240/240 [00:00<00:00, 8273.81 examples/s]\n",
      "Map: 100%|██████████| 48/48 [00:00<00:00, 6914.64 examples/s]\n",
      "Map: 100%|██████████| 960/960 [00:00<00:00, 9192.83 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:13<00:00,  2.20it/s, loss=0.348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.8129\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:13<00:00,  2.17it/s, loss=0.334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.5848\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:13<00:00,  2.23it/s, loss=0.193] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.5881\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:12<00:00,  2.35it/s, loss=0.0871]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.4469\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 30/30 [00:13<00:00,  2.30it/s, loss=0.119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.1632\n",
      "Test Metrics: Precision=0.9290, Recall=0.9290, F1=0.9290\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 240, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 240/240 [00:00<00:00, 8573.44 examples/s]\n",
      "Map: 100%|██████████| 48/48 [00:00<00:00, 6049.30 examples/s]\n",
      "Map: 100%|██████████| 960/960 [00:00<00:00, 9450.53 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:13<00:00,  2.20it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.5502\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:14<00:00,  2.10it/s, loss=0.233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.0850\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:13<00:00,  2.20it/s, loss=0.152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1240\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:13<00:00,  2.15it/s, loss=0.144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2095\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 30/30 [00:14<00:00,  2.12it/s, loss=0.135] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0731\n",
      "Test Metrics: Precision=0.9272, Recall=0.9272, F1=0.9272\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 240, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 240/240 [00:00<00:00, 8370.47 examples/s]\n",
      "Map: 100%|██████████| 48/48 [00:00<00:00, 5011.24 examples/s]\n",
      "Map: 100%|██████████| 960/960 [00:00<00:00, 9248.93 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:14<00:00,  2.05it/s, loss=0.574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.3044\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:14<00:00,  2.06it/s, loss=0.221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.3418\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:14<00:00,  2.12it/s, loss=0.239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.2584\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:14<00:00,  2.09it/s, loss=0.16]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6517\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 30/30 [00:14<00:00,  2.07it/s, loss=0.117] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5323\n",
      "Test Metrics: Precision=0.9268, Recall=0.9268, F1=0.9268\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 240, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 240/240 [00:00<00:00, 8370.61 examples/s]\n",
      "Map: 100%|██████████| 48/48 [00:00<00:00, 5877.81 examples/s]\n",
      "Map: 100%|██████████| 960/960 [00:00<00:00, 9362.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:13<00:00,  2.15it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.4312\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:14<00:00,  2.12it/s, loss=0.193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.2962\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:14<00:00,  2.11it/s, loss=0.132] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4571\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:14<00:00,  2.11it/s, loss=0.168] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.9115\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 30/30 [00:14<00:00,  2.06it/s, loss=0.191] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.2582\n",
      "Test Metrics: Precision=0.9300, Recall=0.9300, F1=0.9300\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 240, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 240/240 [00:00<00:00, 8407.31 examples/s]\n",
      "Map: 100%|██████████| 48/48 [00:00<00:00, 6127.92 examples/s]\n",
      "Map: 100%|██████████| 960/960 [00:00<00:00, 9160.56 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30/30 [00:15<00:00,  1.91it/s, loss=0.379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.6991\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30/30 [00:15<00:00,  1.90it/s, loss=0.544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.2846\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30/30 [00:15<00:00,  1.99it/s, loss=0.506] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9032\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30/30 [00:15<00:00,  1.96it/s, loss=0.256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1400\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 30/30 [00:15<00:00,  1.91it/s, loss=0.167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.0832\n",
      "Test Metrics: Precision=0.9265, Recall=0.9265, F1=0.9265\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 245, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 245/245 [00:00<00:00, 8254.78 examples/s]\n",
      "Map: 100%|██████████| 49/49 [00:00<00:00, 6066.32 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 9456.95 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 31/31 [00:13<00:00,  2.28it/s, loss=0.424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.9395\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 31/31 [00:13<00:00,  2.29it/s, loss=0.445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.6352\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 31/31 [00:13<00:00,  2.23it/s, loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0995\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 31/31 [00:13<00:00,  2.23it/s, loss=0.17]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.2145\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 31/31 [00:13<00:00,  2.25it/s, loss=0.114] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.2621\n",
      "Test Metrics: Precision=0.9270, Recall=0.9270, F1=0.9270\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 245, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 245/245 [00:00<00:00, 8171.09 examples/s]\n",
      "Map: 100%|██████████| 49/49 [00:00<00:00, 5527.43 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 9628.32 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 31/31 [00:13<00:00,  2.23it/s, loss=0.354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.9169\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 31/31 [00:13<00:00,  2.22it/s, loss=0.529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.3395\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 31/31 [00:14<00:00,  2.18it/s, loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5037\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 31/31 [00:14<00:00,  2.10it/s, loss=0.187] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.4526\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 31/31 [00:13<00:00,  2.24it/s, loss=0.0834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.2559\n",
      "Test Metrics: Precision=0.9319, Recall=0.9319, F1=0.9319\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 245, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 245/245 [00:00<00:00, 8095.64 examples/s]\n",
      "Map: 100%|██████████| 49/49 [00:00<00:00, 6101.44 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 4296.56 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 31/31 [00:14<00:00,  2.15it/s, loss=0.444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.4410\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 31/31 [00:14<00:00,  2.09it/s, loss=0.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.8410\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 31/31 [00:15<00:00,  2.04it/s, loss=0.166] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.7718\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 31/31 [00:14<00:00,  2.15it/s, loss=0.114] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.6023\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 31/31 [00:14<00:00,  2.15it/s, loss=0.0754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.6382\n",
      "Test Metrics: Precision=0.9300, Recall=0.9300, F1=0.9300\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 245, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 245/245 [00:00<00:00, 8587.42 examples/s]\n",
      "Map: 100%|██████████| 49/49 [00:00<00:00, 6096.91 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 9069.36 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 31/31 [00:14<00:00,  2.13it/s, loss=0.262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.4200\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 31/31 [00:14<00:00,  2.13it/s, loss=0.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.6125\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 31/31 [00:14<00:00,  2.18it/s, loss=0.166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5611\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 31/31 [00:14<00:00,  2.15it/s, loss=0.137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7495\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 31/31 [00:14<00:00,  2.10it/s, loss=0.0825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.7129\n",
      "Test Metrics: Precision=0.9279, Recall=0.9279, F1=0.9279\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 245, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 245/245 [00:00<00:00, 8474.04 examples/s]\n",
      "Map: 100%|██████████| 49/49 [00:00<00:00, 6578.56 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 9263.19 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 31/31 [00:15<00:00,  1.94it/s, loss=0.444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.1950\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 31/31 [00:15<00:00,  2.01it/s, loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.2044\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 31/31 [00:15<00:00,  1.97it/s, loss=0.346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.0333\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 31/31 [00:16<00:00,  1.93it/s, loss=0.0934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7483\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 31/31 [00:15<00:00,  1.99it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.7987\n",
      "Test Metrics: Precision=0.9324, Recall=0.9324, F1=0.9324\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 250, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 250/250 [00:00<00:00, 8610.91 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6710.24 examples/s]\n",
      "Map: 100%|██████████| 948/948 [00:00<00:00, 4435.78 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:14<00:00,  2.23it/s, loss=0.565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.0371\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:14<00:00,  2.28it/s, loss=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.1964\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:14<00:00,  2.25it/s, loss=0.121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0757\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:13<00:00,  2.37it/s, loss=0.187] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.4739\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 32/32 [00:13<00:00,  2.33it/s, loss=0.0836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5608\n",
      "Test Metrics: Precision=0.9280, Recall=0.9280, F1=0.9280\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 250, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 250/250 [00:00<00:00, 8391.76 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6376.45 examples/s]\n",
      "Map: 100%|██████████| 948/948 [00:00<00:00, 9440.26 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:14<00:00,  2.19it/s, loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.6956\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:14<00:00,  2.21it/s, loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.3876\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, loss=0.0266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.8256\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:14<00:00,  2.14it/s, loss=0.141] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.8454\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 32/32 [00:14<00:00,  2.24it/s, loss=0.117] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.7522\n",
      "Test Metrics: Precision=0.9295, Recall=0.9295, F1=0.9295\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 250, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 250/250 [00:00<00:00, 8621.53 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6972.61 examples/s]\n",
      "Map: 100%|██████████| 948/948 [00:00<00:00, 9606.07 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:15<00:00,  2.09it/s, loss=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.1069\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, loss=0.455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.4940\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:14<00:00,  2.14it/s, loss=0.216] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2019\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, loss=0.248] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0429\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, loss=0.319] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0903\n",
      "Test Metrics: Precision=0.9281, Recall=0.9281, F1=0.9281\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 250, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 250/250 [00:00<00:00, 8386.80 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6672.03 examples/s]\n",
      "Map: 100%|██████████| 948/948 [00:00<00:00, 9102.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:14<00:00,  2.20it/s, loss=0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.0531\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.6723\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:15<00:00,  2.11it/s, loss=0.2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.2275\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:15<00:00,  2.10it/s, loss=0.057] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.5620\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, loss=0.136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.8461\n",
      "Test Metrics: Precision=0.9262, Recall=0.9262, F1=0.9262\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 250, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 250/250 [00:00<00:00, 8436.93 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6445.64 examples/s]\n",
      "Map: 100%|██████████| 948/948 [00:00<00:00, 9228.39 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s, loss=0.546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.5300\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:15<00:00,  2.11it/s, loss=0.485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.7483\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:16<00:00,  1.95it/s, loss=0.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6222\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:16<00:00,  1.99it/s, loss=0.0361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7320\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 32/32 [00:15<00:00,  2.04it/s, loss=0.253] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.6700\n",
      "Test Metrics: Precision=0.9265, Recall=0.9265, F1=0.9265\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 255, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 255/255 [00:00<00:00, 8542.37 examples/s]\n",
      "Map: 100%|██████████| 51/51 [00:00<00:00, 6483.28 examples/s]\n",
      "Map: 100%|██████████| 942/942 [00:00<00:00, 9452.62 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:13<00:00,  2.32it/s, loss=0.386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.7201\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:14<00:00,  2.23it/s, loss=0.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.4702\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:14<00:00,  2.22it/s, loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0898\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, loss=0.204] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.2921\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 32/32 [00:14<00:00,  2.22it/s, loss=0.151] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.1986\n",
      "Test Metrics: Precision=0.9289, Recall=0.9289, F1=0.9289\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 255, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 255/255 [00:00<00:00, 8580.89 examples/s]\n",
      "Map: 100%|██████████| 51/51 [00:00<00:00, 7154.64 examples/s]\n",
      "Map: 100%|██████████| 942/942 [00:00<00:00, 9326.00 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:14<00:00,  2.14it/s, loss=0.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.0336\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:14<00:00,  2.14it/s, loss=0.311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.0260\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8887\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, loss=0.0823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7385\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 32/32 [00:14<00:00,  2.14it/s, loss=0.0756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.4457\n",
      "Test Metrics: Precision=0.9297, Recall=0.9297, F1=0.9297\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 255, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 255/255 [00:00<00:00, 8703.22 examples/s]\n",
      "Map: 100%|██████████| 51/51 [00:00<00:00, 6156.20 examples/s]\n",
      "Map: 100%|██████████| 942/942 [00:00<00:00, 9565.07 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:14<00:00,  2.15it/s, loss=0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.8356\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:15<00:00,  2.06it/s, loss=0.323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.3565\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.7327\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:15<00:00,  2.09it/s, loss=0.113] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.5991\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 32/32 [00:15<00:00,  2.09it/s, loss=0.111] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.6722\n",
      "Test Metrics: Precision=0.9315, Recall=0.9315, F1=0.9315\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 255, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 255/255 [00:00<00:00, 8729.93 examples/s]\n",
      "Map: 100%|██████████| 51/51 [00:00<00:00, 6752.83 examples/s]\n",
      "Map: 100%|██████████| 942/942 [00:00<00:00, 9328.97 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:15<00:00,  2.08it/s, loss=0.448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.6206\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:15<00:00,  2.08it/s, loss=0.329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.8572\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:15<00:00,  2.05it/s, loss=0.253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2011\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:15<00:00,  2.09it/s, loss=0.253] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3033\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 32/32 [00:15<00:00,  2.04it/s, loss=0.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.4410\n",
      "Test Metrics: Precision=0.9258, Recall=0.9258, F1=0.9258\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 255, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 255/255 [00:00<00:00, 8383.22 examples/s]\n",
      "Map: 100%|██████████| 51/51 [00:00<00:00, 7316.90 examples/s]\n",
      "Map: 100%|██████████| 942/942 [00:00<00:00, 9151.48 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 32/32 [00:16<00:00,  1.99it/s, loss=0.416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.0292\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s, loss=0.352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.7569\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 32/32 [00:16<00:00,  1.93it/s, loss=0.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.7499\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 32/32 [00:16<00:00,  1.97it/s, loss=0.0778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.5308\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 32/32 [00:15<00:00,  2.06it/s, loss=0.0796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.3111\n",
      "Test Metrics: Precision=0.9331, Recall=0.9331, F1=0.9331\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 260, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 260/260 [00:00<00:00, 8585.89 examples/s]\n",
      "Map: 100%|██████████| 52/52 [00:00<00:00, 6506.48 examples/s]\n",
      "Map: 100%|██████████| 936/936 [00:00<00:00, 9537.09 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 33/33 [00:14<00:00,  2.30it/s, loss=0.433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.0403\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 33/33 [00:14<00:00,  2.25it/s, loss=0.658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.3173\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 33/33 [00:13<00:00,  2.36it/s, loss=0.214] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4433\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 33/33 [00:14<00:00,  2.27it/s, loss=0.156] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3867\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 33/33 [00:14<00:00,  2.26it/s, loss=0.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1802\n",
      "Test Metrics: Precision=0.9282, Recall=0.9282, F1=0.9282\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 260, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 260/260 [00:00<00:00, 8731.98 examples/s]\n",
      "Map: 100%|██████████| 52/52 [00:00<00:00, 6214.14 examples/s]\n",
      "Map: 100%|██████████| 936/936 [00:00<00:00, 9001.25 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 33/33 [00:15<00:00,  2.17it/s, loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.0068\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 33/33 [00:14<00:00,  2.26it/s, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.5348\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 33/33 [00:15<00:00,  2.16it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2419\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 33/33 [00:14<00:00,  2.24it/s, loss=0.227] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9979\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 33/33 [00:15<00:00,  2.17it/s, loss=0.147] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.7757\n",
      "Test Metrics: Precision=0.9276, Recall=0.9276, F1=0.9276\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 260, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 260/260 [00:00<00:00, 8627.39 examples/s]\n",
      "Map: 100%|██████████| 52/52 [00:00<00:00, 7112.00 examples/s]\n",
      "Map: 100%|██████████| 936/936 [00:00<00:00, 9398.48 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 33/33 [00:15<00:00,  2.11it/s, loss=0.623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.5317\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 33/33 [00:15<00:00,  2.16it/s, loss=0.198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.5456\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 33/33 [00:15<00:00,  2.17it/s, loss=0.289] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5541\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 33/33 [00:15<00:00,  2.12it/s, loss=0.13]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.5270\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 33/33 [00:15<00:00,  2.14it/s, loss=0.0597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.6191\n",
      "Test Metrics: Precision=0.9306, Recall=0.9306, F1=0.9306\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 260, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 260/260 [00:00<00:00, 8565.25 examples/s]\n",
      "Map: 100%|██████████| 52/52 [00:00<00:00, 6517.95 examples/s]\n",
      "Map: 100%|██████████| 936/936 [00:00<00:00, 9043.36 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 33/33 [00:15<00:00,  2.08it/s, loss=0.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.4769\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 33/33 [00:15<00:00,  2.09it/s, loss=0.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.4797\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 33/33 [00:15<00:00,  2.15it/s, loss=0.13]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.1594\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 33/33 [00:15<00:00,  2.15it/s, loss=0.143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.4308\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 33/33 [00:15<00:00,  2.09it/s, loss=0.0759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5301\n",
      "Test Metrics: Precision=0.9281, Recall=0.9281, F1=0.9281\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 260, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 260/260 [00:00<00:00, 8596.44 examples/s]\n",
      "Map: 100%|██████████| 52/52 [00:00<00:00, 6480.58 examples/s]\n",
      "Map: 100%|██████████| 936/936 [00:00<00:00, 9301.07 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 33/33 [00:17<00:00,  1.93it/s, loss=0.409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.1262\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 33/33 [00:16<00:00,  2.02it/s, loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.9270\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 33/33 [00:16<00:00,  1.96it/s, loss=0.136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0094\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 33/33 [00:17<00:00,  1.92it/s, loss=0.0957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.1641\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 33/33 [00:16<00:00,  1.98it/s, loss=0.0686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.8735\n",
      "Test Metrics: Precision=0.9300, Recall=0.9300, F1=0.9300\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 265, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 265/265 [00:00<00:00, 8572.35 examples/s]\n",
      "Map: 100%|██████████| 53/53 [00:00<00:00, 6834.68 examples/s]\n",
      "Map: 100%|██████████| 930/930 [00:00<00:00, 9180.76 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:14<00:00,  2.27it/s, loss=0.0113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.7091\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:14<00:00,  2.30it/s, loss=0.62] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.4114\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:15<00:00,  2.20it/s, loss=0.112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.6696\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:14<00:00,  2.34it/s, loss=0.232] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.0027\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 34/34 [00:14<00:00,  2.29it/s, loss=0.14]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.9339\n",
      "Test Metrics: Precision=0.9293, Recall=0.9293, F1=0.9293\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 265, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 265/265 [00:00<00:00, 8050.08 examples/s]\n",
      "Map: 100%|██████████| 53/53 [00:00<00:00, 6896.17 examples/s]\n",
      "Map: 100%|██████████| 930/930 [00:00<00:00, 8989.99 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:14<00:00,  2.28it/s, loss=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.4220\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:15<00:00,  2.23it/s, loss=0.0217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.9534\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:14<00:00,  2.32it/s, loss=0.0679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2631\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:15<00:00,  2.19it/s, loss=0.283] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6519\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 34/34 [00:15<00:00,  2.21it/s, loss=0.0552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0964\n",
      "Test Metrics: Precision=0.9306, Recall=0.9306, F1=0.9306\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 265, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 265/265 [00:00<00:00, 8849.45 examples/s]\n",
      "Map: 100%|██████████| 53/53 [00:00<00:00, 7244.28 examples/s]\n",
      "Map: 100%|██████████| 930/930 [00:00<00:00, 9380.32 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:15<00:00,  2.16it/s, loss=0.0757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.2784\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:15<00:00,  2.20it/s, loss=0.02] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.3142\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:15<00:00,  2.25it/s, loss=0.366] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.6434\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:15<00:00,  2.23it/s, loss=0.0448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.8328\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 34/34 [00:15<00:00,  2.22it/s, loss=0.153] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.5574\n",
      "Test Metrics: Precision=0.9324, Recall=0.9324, F1=0.9324\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 265, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 265/265 [00:00<00:00, 8938.55 examples/s]\n",
      "Map: 100%|██████████| 53/53 [00:00<00:00, 5846.72 examples/s]\n",
      "Map: 100%|██████████| 930/930 [00:00<00:00, 9375.81 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:16<00:00,  2.12it/s, loss=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.8249\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:16<00:00,  2.08it/s, loss=0.286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.2425\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:15<00:00,  2.20it/s, loss=0.517] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4723\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:16<00:00,  2.12it/s, loss=0.0904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1891\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 34/34 [00:15<00:00,  2.16it/s, loss=0.261] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.3287\n",
      "Test Metrics: Precision=0.9268, Recall=0.9268, F1=0.9268\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 265, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 265/265 [00:00<00:00, 8673.97 examples/s]\n",
      "Map: 100%|██████████| 53/53 [00:00<00:00, 7288.94 examples/s]\n",
      "Map: 100%|██████████| 930/930 [00:00<00:00, 9475.59 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:16<00:00,  2.06it/s, loss=0.972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.0280\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:16<00:00,  2.10it/s, loss=0.0996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.2241\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:16<00:00,  2.02it/s, loss=0.155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8940\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:16<00:00,  2.11it/s, loss=0.137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8538\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 34/34 [00:17<00:00,  2.00it/s, loss=0.0113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5067\n",
      "Test Metrics: Precision=0.9345, Recall=0.9345, F1=0.9345\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 270, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 270/270 [00:00<00:00, 8482.23 examples/s]\n",
      "Map: 100%|██████████| 54/54 [00:00<00:00, 5402.19 examples/s]\n",
      "Map: 100%|██████████| 924/924 [00:00<00:00, 9211.06 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:15<00:00,  2.20it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.6882\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:15<00:00,  2.27it/s, loss=0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.1234\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:15<00:00,  2.26it/s, loss=0.163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.6498\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:15<00:00,  2.16it/s, loss=0.112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.9267\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 34/34 [00:15<00:00,  2.20it/s, loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.9288\n",
      "Test Metrics: Precision=0.9285, Recall=0.9285, F1=0.9285\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 270, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 270/270 [00:00<00:00, 8562.46 examples/s]\n",
      "Map: 100%|██████████| 54/54 [00:00<00:00, 6161.05 examples/s]\n",
      "Map: 100%|██████████| 924/924 [00:00<00:00, 9100.39 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:15<00:00,  2.25it/s, loss=0.578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.3200\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:15<00:00,  2.18it/s, loss=0.155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.0105\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:15<00:00,  2.13it/s, loss=0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1207\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:15<00:00,  2.14it/s, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9252\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 34/34 [00:15<00:00,  2.17it/s, loss=0.135] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.7917\n",
      "Test Metrics: Precision=0.9254, Recall=0.9254, F1=0.9254\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 270, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 270/270 [00:00<00:00, 8617.12 examples/s]\n",
      "Map: 100%|██████████| 54/54 [00:00<00:00, 7395.43 examples/s]\n",
      "Map: 100%|██████████| 924/924 [00:00<00:00, 9316.58 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:16<00:00,  2.11it/s, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.0505\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:15<00:00,  2.13it/s, loss=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.5665\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:16<00:00,  2.11it/s, loss=0.231] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5757\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:16<00:00,  2.08it/s, loss=0.115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.0860\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 34/34 [00:15<00:00,  2.19it/s, loss=0.0681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.8589\n",
      "Test Metrics: Precision=0.9341, Recall=0.9341, F1=0.9341\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 270, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 270/270 [00:00<00:00, 8558.25 examples/s]\n",
      "Map: 100%|██████████| 54/54 [00:00<00:00, 6522.46 examples/s]\n",
      "Map: 100%|██████████| 924/924 [00:00<00:00, 8990.71 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:16<00:00,  2.10it/s, loss=0.265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.9830\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:16<00:00,  2.11it/s, loss=0.136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.0226\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:17<00:00,  1.98it/s, loss=0.0341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.4716\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:16<00:00,  2.10it/s, loss=0.0804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7386\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 34/34 [00:15<00:00,  2.13it/s, loss=0.0583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.2379\n",
      "Test Metrics: Precision=0.9291, Recall=0.9291, F1=0.9291\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 270, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 270/270 [00:00<00:00, 7982.17 examples/s]\n",
      "Map: 100%|██████████| 54/54 [00:00<00:00, 6377.37 examples/s]\n",
      "Map: 100%|██████████| 924/924 [00:00<00:00, 8937.74 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 34/34 [00:17<00:00,  2.00it/s, loss=0.582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.7089\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 34/34 [00:17<00:00,  1.99it/s, loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.5090\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 34/34 [00:17<00:00,  1.94it/s, loss=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3189\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 34/34 [00:17<00:00,  1.94it/s, loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2350\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 34/34 [00:17<00:00,  1.92it/s, loss=0.0941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0718\n",
      "Test Metrics: Precision=0.9303, Recall=0.9303, F1=0.9303\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 275, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 275/275 [00:00<00:00, 8485.75 examples/s]\n",
      "Map: 100%|██████████| 55/55 [00:00<00:00, 7293.29 examples/s]\n",
      "Map: 100%|██████████| 918/918 [00:00<00:00, 9369.19 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:15<00:00,  2.30it/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.2119\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:15<00:00,  2.24it/s, loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.3991\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:15<00:00,  2.25it/s, loss=0.222] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.2341\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:15<00:00,  2.26it/s, loss=0.0484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.1272\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 35/35 [00:15<00:00,  2.29it/s, loss=0.145] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.1440\n",
      "Test Metrics: Precision=0.9293, Recall=0.9293, F1=0.9293\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 275, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 275/275 [00:00<00:00, 8823.36 examples/s]\n",
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6221.83 examples/s]\n",
      "Map: 100%|██████████| 918/918 [00:00<00:00, 9267.11 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:16<00:00,  2.13it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.4841\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:15<00:00,  2.22it/s, loss=0.159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.4669\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:16<00:00,  2.18it/s, loss=0.0994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2714\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:15<00:00,  2.23it/s, loss=0.0396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2530\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 35/35 [00:16<00:00,  2.16it/s, loss=0.109] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.5011\n",
      "Test Metrics: Precision=0.9272, Recall=0.9272, F1=0.9272\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 275, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 275/275 [00:00<00:00, 8755.38 examples/s]\n",
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6563.11 examples/s]\n",
      "Map: 100%|██████████| 918/918 [00:00<00:00, 9275.54 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:16<00:00,  2.15it/s, loss=0.416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.2649\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:16<00:00,  2.18it/s, loss=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.2605\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:16<00:00,  2.17it/s, loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.0671\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:15<00:00,  2.20it/s, loss=0.0805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3470\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 35/35 [00:16<00:00,  2.06it/s, loss=0.0502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.6209\n",
      "Test Metrics: Precision=0.9314, Recall=0.9314, F1=0.9314\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 275, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 275/275 [00:00<00:00, 8759.43 examples/s]\n",
      "Map: 100%|██████████| 55/55 [00:00<00:00, 8484.56 examples/s]\n",
      "Map: 100%|██████████| 918/918 [00:00<00:00, 9152.23 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:16<00:00,  2.07it/s, loss=0.301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.8126\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:16<00:00,  2.14it/s, loss=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.7068\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:16<00:00,  2.07it/s, loss=0.222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3662\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:16<00:00,  2.08it/s, loss=0.159] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.4037\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 35/35 [00:16<00:00,  2.15it/s, loss=0.0839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.3402\n",
      "Test Metrics: Precision=0.9300, Recall=0.9300, F1=0.9300\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 275, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 275/275 [00:00<00:00, 8357.61 examples/s]\n",
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6178.17 examples/s]\n",
      "Map: 100%|██████████| 918/918 [00:00<00:00, 9295.20 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:17<00:00,  1.99it/s, loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.1128\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:17<00:00,  2.05it/s, loss=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.7169\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:18<00:00,  1.91it/s, loss=0.169] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5327\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:17<00:00,  1.94it/s, loss=0.056] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.5699\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 35/35 [00:18<00:00,  1.91it/s, loss=0.0102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.2218\n",
      "Test Metrics: Precision=0.9353, Recall=0.9353, F1=0.9353\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 280, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 280/280 [00:00<00:00, 8246.94 examples/s]\n",
      "Map: 100%|██████████| 56/56 [00:00<00:00, 6914.57 examples/s]\n",
      "Map: 100%|██████████| 912/912 [00:00<00:00, 9226.37 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:15<00:00,  2.21it/s, loss=0.36] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.5640\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:16<00:00,  2.17it/s, loss=0.155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.9841\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:16<00:00,  2.16it/s, loss=0.154] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.1996\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:15<00:00,  2.23it/s, loss=0.0888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6307\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 35/35 [00:15<00:00,  2.26it/s, loss=0.094] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.3814\n",
      "Test Metrics: Precision=0.9300, Recall=0.9300, F1=0.9300\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 280, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 280/280 [00:00<00:00, 8491.60 examples/s]\n",
      "Map: 100%|██████████| 56/56 [00:00<00:00, 6642.19 examples/s]\n",
      "Map: 100%|██████████| 912/912 [00:00<00:00, 8600.16 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:16<00:00,  2.07it/s, loss=0.553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.4573\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:15<00:00,  2.20it/s, loss=0.191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.6611\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:16<00:00,  2.11it/s, loss=0.285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.6566\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:16<00:00,  2.13it/s, loss=0.0638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7119\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 35/35 [00:16<00:00,  2.10it/s, loss=0.122] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.4948\n",
      "Test Metrics: Precision=0.9324, Recall=0.9324, F1=0.9324\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 280, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 280/280 [00:00<00:00, 8870.46 examples/s]\n",
      "Map: 100%|██████████| 56/56 [00:00<00:00, 5691.60 examples/s]\n",
      "Map: 100%|██████████| 912/912 [00:00<00:00, 9426.15 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:16<00:00,  2.09it/s, loss=0.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.8409\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:16<00:00,  2.11it/s, loss=0.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.0033\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:15<00:00,  2.24it/s, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0362\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:16<00:00,  2.12it/s, loss=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0398\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 35/35 [00:16<00:00,  2.09it/s, loss=0.243] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.8552\n",
      "Test Metrics: Precision=0.9278, Recall=0.9278, F1=0.9278\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 280, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 280/280 [00:00<00:00, 8558.06 examples/s]\n",
      "Map: 100%|██████████| 56/56 [00:00<00:00, 6218.39 examples/s]\n",
      "Map: 100%|██████████| 912/912 [00:00<00:00, 3898.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:17<00:00,  2.05it/s, loss=0.268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.6117\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:17<00:00,  1.98it/s, loss=0.35] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.6616\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:16<00:00,  2.06it/s, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.0119\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:17<00:00,  2.02it/s, loss=0.151] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0437\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 35/35 [00:16<00:00,  2.06it/s, loss=0.0726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0515\n",
      "Test Metrics: Precision=0.9280, Recall=0.9280, F1=0.9280\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 280, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 280/280 [00:00<00:00, 8296.17 examples/s]\n",
      "Map: 100%|██████████| 56/56 [00:00<00:00, 6413.66 examples/s]\n",
      "Map: 100%|██████████| 912/912 [00:00<00:00, 9272.03 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [00:18<00:00,  1.87it/s, loss=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.2052\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 35/35 [00:17<00:00,  1.94it/s, loss=0.283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.0920\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 35/35 [00:17<00:00,  1.96it/s, loss=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8718\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 35/35 [00:17<00:00,  1.99it/s, loss=0.108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6295\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 35/35 [00:17<00:00,  1.95it/s, loss=0.121] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.3299\n",
      "Test Metrics: Precision=0.9334, Recall=0.9334, F1=0.9334\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 285, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 285/285 [00:00<00:00, 8555.94 examples/s]\n",
      "Map: 100%|██████████| 57/57 [00:00<00:00, 7819.82 examples/s]\n",
      "Map: 100%|██████████| 906/906 [00:00<00:00, 9351.69 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 36/36 [00:16<00:00,  2.18it/s, loss=0.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.5789\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 36/36 [00:16<00:00,  2.19it/s, loss=0.353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.4589\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 36/36 [00:17<00:00,  2.12it/s, loss=0.118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4702\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 36/36 [00:15<00:00,  2.26it/s, loss=0.0891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3359\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 36/36 [00:15<00:00,  2.26it/s, loss=0.0846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1805\n",
      "Test Metrics: Precision=0.9311, Recall=0.9311, F1=0.9311\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 285, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 285/285 [00:00<00:00, 8516.75 examples/s]\n",
      "Map: 100%|██████████| 57/57 [00:00<00:00, 6453.47 examples/s]\n",
      "Map: 100%|██████████| 906/906 [00:00<00:00, 9234.92 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 36/36 [00:16<00:00,  2.20it/s, loss=0.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.2791\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 36/36 [00:16<00:00,  2.16it/s, loss=0.281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.4957\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 36/36 [00:16<00:00,  2.19it/s, loss=0.142] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3797\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 36/36 [00:16<00:00,  2.17it/s, loss=0.152] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7820\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 36/36 [00:16<00:00,  2.17it/s, loss=0.0652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.3475\n",
      "Test Metrics: Precision=0.9335, Recall=0.9335, F1=0.9335\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 285, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 285/285 [00:00<00:00, 8213.28 examples/s]\n",
      "Map: 100%|██████████| 57/57 [00:00<00:00, 5019.95 examples/s]\n",
      "Map: 100%|██████████| 906/906 [00:00<00:00, 9691.43 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 36/36 [00:16<00:00,  2.19it/s, loss=0.392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.7407\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 36/36 [00:17<00:00,  2.09it/s, loss=0.164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.4976\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 36/36 [00:16<00:00,  2.19it/s, loss=0.211] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3784\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 36/36 [00:16<00:00,  2.14it/s, loss=0.0841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.2818\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 36/36 [00:16<00:00,  2.16it/s, loss=0.122] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.2065\n",
      "Test Metrics: Precision=0.9320, Recall=0.9320, F1=0.9320\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 285, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 285/285 [00:00<00:00, 8707.58 examples/s]\n",
      "Map: 100%|██████████| 57/57 [00:00<00:00, 6400.09 examples/s]\n",
      "Map: 100%|██████████| 906/906 [00:00<00:00, 9292.69 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 36/36 [00:17<00:00,  2.08it/s, loss=0.465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.8067\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 36/36 [00:16<00:00,  2.13it/s, loss=0.265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.5076\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 36/36 [00:17<00:00,  2.04it/s, loss=0.0811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.6846\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 36/36 [00:17<00:00,  2.06it/s, loss=0.122] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9456\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 36/36 [00:16<00:00,  2.18it/s, loss=0.0674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.6822\n",
      "Test Metrics: Precision=0.9307, Recall=0.9307, F1=0.9307\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 285, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 285/285 [00:00<00:00, 8658.13 examples/s]\n",
      "Map: 100%|██████████| 57/57 [00:00<00:00, 7307.60 examples/s]\n",
      "Map: 100%|██████████| 906/906 [00:00<00:00, 9243.95 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 36/36 [00:17<00:00,  2.00it/s, loss=0.558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.8396\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 36/36 [00:17<00:00,  2.03it/s, loss=0.353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.4894\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 36/36 [00:18<00:00,  1.97it/s, loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6606\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 36/36 [00:18<00:00,  2.00it/s, loss=0.0587]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5537\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 36/36 [00:18<00:00,  1.97it/s, loss=0.111] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.3981\n",
      "Test Metrics: Precision=0.9332, Recall=0.9332, F1=0.9332\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 290, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 290/290 [00:00<00:00, 8587.30 examples/s]\n",
      "Map: 100%|██████████| 58/58 [00:00<00:00, 6764.44 examples/s]\n",
      "Map: 100%|██████████| 900/900 [00:00<00:00, 9428.75 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:15<00:00,  2.35it/s, loss=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.4803\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:15<00:00,  2.31it/s, loss=0.471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.4883\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:16<00:00,  2.26it/s, loss=0.0396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.9186\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:16<00:00,  2.26it/s, loss=0.0412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7681\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 37/37 [00:16<00:00,  2.27it/s, loss=0.141] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.6920\n",
      "Test Metrics: Precision=0.9296, Recall=0.9296, F1=0.9296\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 290, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 290/290 [00:00<00:00, 8474.23 examples/s]\n",
      "Map: 100%|██████████| 58/58 [00:00<00:00, 6496.89 examples/s]\n",
      "Map: 100%|██████████| 900/900 [00:00<00:00, 9168.19 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:16<00:00,  2.19it/s, loss=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.6976\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:16<00:00,  2.19it/s, loss=0.244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8247\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:16<00:00,  2.19it/s, loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9244\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:16<00:00,  2.20it/s, loss=0.157] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5504\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 37/37 [00:16<00:00,  2.18it/s, loss=0.24]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.3231\n",
      "Test Metrics: Precision=0.9236, Recall=0.9236, F1=0.9236\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 290, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 290/290 [00:00<00:00, 8764.07 examples/s]\n",
      "Map: 100%|██████████| 58/58 [00:00<00:00, 6087.22 examples/s]\n",
      "Map: 100%|██████████| 900/900 [00:00<00:00, 9342.34 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:17<00:00,  2.17it/s, loss=0.416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.0100\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:16<00:00,  2.18it/s, loss=0.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.9023\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:17<00:00,  2.15it/s, loss=0.0177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.7316\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:17<00:00,  2.10it/s, loss=0.119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7121\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 37/37 [00:16<00:00,  2.27it/s, loss=0.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5591\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 290, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 290/290 [00:00<00:00, 8943.21 examples/s]\n",
      "Map: 100%|██████████| 58/58 [00:00<00:00, 7495.14 examples/s]\n",
      "Map: 100%|██████████| 900/900 [00:00<00:00, 9246.20 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:16<00:00,  2.19it/s, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.2290\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:17<00:00,  2.09it/s, loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.6650\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:16<00:00,  2.18it/s, loss=0.0872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1823\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:16<00:00,  2.18it/s, loss=0.175] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8946\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 37/37 [00:17<00:00,  2.14it/s, loss=0.0483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.6990\n",
      "Test Metrics: Precision=0.9305, Recall=0.9305, F1=0.9305\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 290, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 290/290 [00:00<00:00, 8496.90 examples/s]\n",
      "Map: 100%|██████████| 58/58 [00:00<00:00, 7801.60 examples/s]\n",
      "Map: 100%|██████████| 900/900 [00:00<00:00, 9215.66 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:18<00:00,  2.01it/s, loss=0.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.0863\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:18<00:00,  2.04it/s, loss=0.329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8786\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:18<00:00,  2.01it/s, loss=0.212] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2374\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:18<00:00,  2.05it/s, loss=0.0469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8456\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 37/37 [00:17<00:00,  2.06it/s, loss=0.0556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.4150\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 295, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 295/295 [00:00<00:00, 8221.34 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 5818.85 examples/s]\n",
      "Map: 100%|██████████| 894/894 [00:00<00:00, 9308.41 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:16<00:00,  2.22it/s, loss=0.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.6679\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:16<00:00,  2.18it/s, loss=0.189] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.0645\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:16<00:00,  2.23it/s, loss=0.154] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.1643\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:15<00:00,  2.33it/s, loss=0.262] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.0962\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 37/37 [00:16<00:00,  2.24it/s, loss=0.0805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.0659\n",
      "Test Metrics: Precision=0.9326, Recall=0.9326, F1=0.9326\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 295, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 295/295 [00:00<00:00, 8941.27 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 7189.33 examples/s]\n",
      "Map: 100%|██████████| 894/894 [00:00<00:00, 9280.54 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:17<00:00,  2.12it/s, loss=0.52] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.0693\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:17<00:00,  2.12it/s, loss=0.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.9040\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:16<00:00,  2.19it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9495\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:17<00:00,  2.17it/s, loss=0.151] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6081\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 37/37 [00:16<00:00,  2.22it/s, loss=0.158] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.3527\n",
      "Test Metrics: Precision=0.9302, Recall=0.9302, F1=0.9302\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 295, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 295/295 [00:00<00:00, 8145.46 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 7561.23 examples/s]\n",
      "Map: 100%|██████████| 894/894 [00:00<00:00, 9157.51 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:17<00:00,  2.16it/s, loss=0.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.2890\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:17<00:00,  2.14it/s, loss=0.101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.7054\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:16<00:00,  2.22it/s, loss=0.195] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3692\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:17<00:00,  2.10it/s, loss=0.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.9771\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 37/37 [00:17<00:00,  2.15it/s, loss=0.0938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.0020\n",
      "Test Metrics: Precision=0.9335, Recall=0.9335, F1=0.9335\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 295, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 295/295 [00:00<00:00, 8860.15 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 6450.93 examples/s]\n",
      "Map: 100%|██████████| 894/894 [00:00<00:00, 9213.79 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:17<00:00,  2.15it/s, loss=0.411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.9195\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:17<00:00,  2.10it/s, loss=0.214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.4028\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:17<00:00,  2.07it/s, loss=0.173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9190\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:18<00:00,  2.05it/s, loss=0.151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.4321\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 37/37 [00:18<00:00,  2.03it/s, loss=0.121] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.0473\n",
      "Test Metrics: Precision=0.9267, Recall=0.9267, F1=0.9267\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 295, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 295/295 [00:00<00:00, 8585.88 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 7366.75 examples/s]\n",
      "Map: 100%|██████████| 894/894 [00:00<00:00, 9210.85 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 37/37 [00:18<00:00,  2.00it/s, loss=0.465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.5265\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 37/37 [00:18<00:00,  1.99it/s, loss=0.233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.1432\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 37/37 [00:18<00:00,  1.95it/s, loss=0.154] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0750\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 37/37 [00:18<00:00,  1.96it/s, loss=0.195] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.4487\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 37/37 [00:18<00:00,  1.96it/s, loss=0.0718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.9563\n",
      "Test Metrics: Precision=0.9330, Recall=0.9330, F1=0.9330\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 300, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 8461.09 examples/s]\n",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 5902.48 examples/s]\n",
      "Map: 100%|██████████| 888/888 [00:00<00:00, 9702.81 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 38/38 [00:16<00:00,  2.24it/s, loss=0.33] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.2836\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 38/38 [00:16<00:00,  2.27it/s, loss=0.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9312\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 38/38 [00:16<00:00,  2.24it/s, loss=0.164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0464\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 38/38 [00:17<00:00,  2.23it/s, loss=0.0581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5693\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 38/38 [00:17<00:00,  2.20it/s, loss=0.0825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.2192\n",
      "Test Metrics: Precision=0.9319, Recall=0.9319, F1=0.9319\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 300, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 8884.92 examples/s]\n",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 7199.08 examples/s]\n",
      "Map: 100%|██████████| 888/888 [00:00<00:00, 9241.01 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 38/38 [00:16<00:00,  2.28it/s, loss=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.5084\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 38/38 [00:17<00:00,  2.22it/s, loss=0.39] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.4061\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 38/38 [00:17<00:00,  2.20it/s, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.1651\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 38/38 [00:16<00:00,  2.26it/s, loss=0.149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.9675\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 38/38 [00:17<00:00,  2.19it/s, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.3011\n",
      "Test Metrics: Precision=0.9260, Recall=0.9260, F1=0.9260\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 300, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 8576.96 examples/s]\n",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 7027.40 examples/s]\n",
      "Map: 100%|██████████| 888/888 [00:00<00:00, 9225.08 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 38/38 [00:17<00:00,  2.21it/s, loss=0.303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.6653\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 38/38 [00:17<00:00,  2.15it/s, loss=0.167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.1035\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 38/38 [00:17<00:00,  2.16it/s, loss=0.153] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3684\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 38/38 [00:17<00:00,  2.17it/s, loss=0.0924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.1447\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 38/38 [00:17<00:00,  2.15it/s, loss=0.0297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.8873\n",
      "Test Metrics: Precision=0.9330, Recall=0.9330, F1=0.9330\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 300, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 8343.83 examples/s]\n",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 6106.88 examples/s]\n",
      "Map: 100%|██████████| 888/888 [00:00<00:00, 9281.35 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 38/38 [00:17<00:00,  2.15it/s, loss=0.554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.6410\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 38/38 [00:18<00:00,  2.04it/s, loss=0.161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.0378\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 38/38 [00:17<00:00,  2.14it/s, loss=0.093] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.6637\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 38/38 [00:18<00:00,  2.09it/s, loss=0.109] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7423\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 38/38 [00:18<00:00,  2.03it/s, loss=0.155] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.7883\n",
      "Test Metrics: Precision=0.9297, Recall=0.9297, F1=0.9297\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 300, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 8397.68 examples/s]\n",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 6632.88 examples/s]\n",
      "Map: 100%|██████████| 888/888 [00:00<00:00, 9699.28 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 38/38 [00:19<00:00,  1.97it/s, loss=0.376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.5795\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 38/38 [00:18<00:00,  2.09it/s, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.1723\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 38/38 [00:18<00:00,  2.02it/s, loss=0.0627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4842\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 38/38 [00:18<00:00,  2.07it/s, loss=0.187] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2690\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 38/38 [00:18<00:00,  2.08it/s, loss=0.0387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.2916\n",
      "Test Metrics: Precision=0.9332, Recall=0.9332, F1=0.9332\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 305, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 305/305 [00:00<00:00, 7936.56 examples/s]\n",
      "Map: 100%|██████████| 61/61 [00:00<00:00, 6337.06 examples/s]\n",
      "Map: 100%|██████████| 882/882 [00:00<00:00, 9307.50 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:17<00:00,  2.27it/s, loss=0.418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.3667\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:17<00:00,  2.21it/s, loss=0.521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.3167\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:17<00:00,  2.28it/s, loss=0.0383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.0117\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:17<00:00,  2.23it/s, loss=0.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 8.4326\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 39/39 [00:17<00:00,  2.20it/s, loss=0.267] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 7.2053\n",
      "Test Metrics: Precision=0.9217, Recall=0.9217, F1=0.9217\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 305, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 305/305 [00:00<00:00, 8724.13 examples/s]\n",
      "Map: 100%|██████████| 61/61 [00:00<00:00, 6881.46 examples/s]\n",
      "Map: 100%|██████████| 882/882 [00:00<00:00, 9115.65 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:18<00:00,  2.16it/s, loss=0.102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.6937\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:17<00:00,  2.17it/s, loss=0.0964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.7400\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:17<00:00,  2.28it/s, loss=0.542] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3187\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:17<00:00,  2.28it/s, loss=0.0911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1674\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 39/39 [00:17<00:00,  2.23it/s, loss=0.0168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.4738\n",
      "Test Metrics: Precision=0.9282, Recall=0.9282, F1=0.9282\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 305, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 305/305 [00:00<00:00, 8542.09 examples/s]\n",
      "Map: 100%|██████████| 61/61 [00:00<00:00, 7888.16 examples/s]\n",
      "Map: 100%|██████████| 882/882 [00:00<00:00, 9187.12 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:17<00:00,  2.18it/s, loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.8702\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:17<00:00,  2.19it/s, loss=0.0188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.6346\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:17<00:00,  2.19it/s, loss=0.128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5688\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:17<00:00,  2.18it/s, loss=0.0661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.3032\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 39/39 [00:18<00:00,  2.16it/s, loss=0.0113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.1694\n",
      "Test Metrics: Precision=0.9316, Recall=0.9316, F1=0.9316\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 305, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 305/305 [00:00<00:00, 8296.56 examples/s]\n",
      "Map: 100%|██████████| 61/61 [00:00<00:00, 7161.72 examples/s]\n",
      "Map: 100%|██████████| 882/882 [00:00<00:00, 9253.11 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:18<00:00,  2.13it/s, loss=0.462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.2511\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:18<00:00,  2.11it/s, loss=0.193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8069\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:18<00:00,  2.07it/s, loss=0.0169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8771\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:18<00:00,  2.12it/s, loss=0.00646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2118\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 39/39 [00:18<00:00,  2.13it/s, loss=0.00105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.8178\n",
      "Test Metrics: Precision=0.9315, Recall=0.9315, F1=0.9315\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 305, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 305/305 [00:00<00:00, 8567.66 examples/s]\n",
      "Map: 100%|██████████| 61/61 [00:00<00:00, 8325.28 examples/s]\n",
      "Map: 100%|██████████| 882/882 [00:00<00:00, 9124.66 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:19<00:00,  2.04it/s, loss=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.3828\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:19<00:00,  2.00it/s, loss=0.0537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.2395\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:19<00:00,  2.01it/s, loss=0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3281\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:19<00:00,  2.01it/s, loss=0.0261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9427\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 39/39 [00:18<00:00,  2.06it/s, loss=0.00558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.6632\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 310, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 310/310 [00:00<00:00, 8107.66 examples/s]\n",
      "Map: 100%|██████████| 62/62 [00:00<00:00, 6990.32 examples/s]\n",
      "Map: 100%|██████████| 876/876 [00:00<00:00, 9145.88 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:17<00:00,  2.24it/s, loss=0.545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.2345\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:16<00:00,  2.30it/s, loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.2723\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:17<00:00,  2.21it/s, loss=0.151] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6912\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:18<00:00,  2.16it/s, loss=0.163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.4359\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 39/39 [00:17<00:00,  2.28it/s, loss=0.167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1639\n",
      "Test Metrics: Precision=0.9307, Recall=0.9307, F1=0.9307\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 310, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 310/310 [00:00<00:00, 8607.12 examples/s]\n",
      "Map: 100%|██████████| 62/62 [00:00<00:00, 6417.27 examples/s]\n",
      "Map: 100%|██████████| 876/876 [00:00<00:00, 9050.63 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:17<00:00,  2.29it/s, loss=0.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.5795\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:17<00:00,  2.23it/s, loss=0.345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.1100\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:17<00:00,  2.24it/s, loss=0.178] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6203\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:16<00:00,  2.33it/s, loss=0.162] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5696\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 39/39 [00:17<00:00,  2.27it/s, loss=0.137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.3071\n",
      "Test Metrics: Precision=0.9285, Recall=0.9285, F1=0.9285\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 310, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 310/310 [00:00<00:00, 8698.91 examples/s]\n",
      "Map: 100%|██████████| 62/62 [00:00<00:00, 6184.96 examples/s]\n",
      "Map: 100%|██████████| 876/876 [00:00<00:00, 9543.82 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:17<00:00,  2.19it/s, loss=0.553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.1682\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:17<00:00,  2.17it/s, loss=0.414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.6520\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:18<00:00,  2.12it/s, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.1810\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:17<00:00,  2.18it/s, loss=0.264] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.9230\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 39/39 [00:18<00:00,  2.15it/s, loss=0.142] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 6.7904\n",
      "Test Metrics: Precision=0.9213, Recall=0.9213, F1=0.9213\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 310, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 310/310 [00:00<00:00, 8937.12 examples/s]\n",
      "Map: 100%|██████████| 62/62 [00:00<00:00, 6363.71 examples/s]\n",
      "Map: 100%|██████████| 876/876 [00:00<00:00, 9115.29 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:18<00:00,  2.09it/s, loss=0.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.8056\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:18<00:00,  2.15it/s, loss=0.279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.5722\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:18<00:00,  2.09it/s, loss=0.0962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.7718\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:17<00:00,  2.17it/s, loss=0.152] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.4745\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 39/39 [00:18<00:00,  2.13it/s, loss=0.14]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.2123\n",
      "Test Metrics: Precision=0.9304, Recall=0.9304, F1=0.9304\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 310, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 310/310 [00:00<00:00, 7518.85 examples/s]\n",
      "Map: 100%|██████████| 62/62 [00:00<00:00, 7285.45 examples/s]\n",
      "Map: 100%|██████████| 876/876 [00:00<00:00, 9195.62 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 39/39 [00:19<00:00,  1.99it/s, loss=0.593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.3565\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 39/39 [00:19<00:00,  1.97it/s, loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8290\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 39/39 [00:19<00:00,  2.02it/s, loss=0.0896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2530\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 39/39 [00:20<00:00,  1.94it/s, loss=0.0926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1947\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 39/39 [00:20<00:00,  1.94it/s, loss=0.0889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.9556\n",
      "Test Metrics: Precision=0.9327, Recall=0.9327, F1=0.9327\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 315, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 315/315 [00:00<00:00, 8608.77 examples/s]\n",
      "Map: 100%|██████████| 63/63 [00:00<00:00, 7221.08 examples/s]\n",
      "Map: 100%|██████████| 870/870 [00:00<00:00, 9335.29 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:17<00:00,  2.27it/s, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.6074\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:17<00:00,  2.29it/s, loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.3490\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:17<00:00,  2.33it/s, loss=0.0984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1056\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:18<00:00,  2.22it/s, loss=0.0566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9621\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 40/40 [00:17<00:00,  2.28it/s, loss=0.12]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.4773\n",
      "Test Metrics: Precision=0.9330, Recall=0.9330, F1=0.9330\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 315, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 315/315 [00:00<00:00, 8868.05 examples/s]\n",
      "Map: 100%|██████████| 63/63 [00:00<00:00, 6630.23 examples/s]\n",
      "Map: 100%|██████████| 870/870 [00:00<00:00, 9350.70 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:17<00:00,  2.25it/s, loss=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.4992\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:18<00:00,  2.22it/s, loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.3995\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:18<00:00,  2.21it/s, loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3783\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:18<00:00,  2.22it/s, loss=0.14]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8737\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 40/40 [00:17<00:00,  2.25it/s, loss=0.061] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.6161\n",
      "Test Metrics: Precision=0.9345, Recall=0.9345, F1=0.9345\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 315, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 315/315 [00:00<00:00, 8720.54 examples/s]\n",
      "Map: 100%|██████████| 63/63 [00:00<00:00, 5552.92 examples/s]\n",
      "Map: 100%|██████████| 870/870 [00:00<00:00, 4123.88 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:18<00:00,  2.21it/s, loss=0.446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.8274\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:18<00:00,  2.15it/s, loss=0.356] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.5422\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:18<00:00,  2.16it/s, loss=0.195] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.7447\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:18<00:00,  2.16it/s, loss=0.121] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7489\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 40/40 [00:18<00:00,  2.21it/s, loss=0.109] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.4062\n",
      "Test Metrics: Precision=0.9327, Recall=0.9327, F1=0.9327\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 315, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 315/315 [00:00<00:00, 8797.25 examples/s]\n",
      "Map: 100%|██████████| 63/63 [00:00<00:00, 6720.44 examples/s]\n",
      "Map: 100%|██████████| 870/870 [00:00<00:00, 9195.16 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:18<00:00,  2.11it/s, loss=0.311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.1860\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:18<00:00,  2.20it/s, loss=0.362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.5332\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:19<00:00,  2.10it/s, loss=0.0529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5514\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:19<00:00,  2.09it/s, loss=0.039] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6657\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 40/40 [00:18<00:00,  2.16it/s, loss=0.157] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1972\n",
      "Test Metrics: Precision=0.9308, Recall=0.9308, F1=0.9308\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 315, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 315/315 [00:00<00:00, 8476.11 examples/s]\n",
      "Map: 100%|██████████| 63/63 [00:00<00:00, 7293.03 examples/s]\n",
      "Map: 100%|██████████| 870/870 [00:00<00:00, 9275.31 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:19<00:00,  2.02it/s, loss=0.53] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.2035\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:19<00:00,  2.05it/s, loss=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.0224\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:19<00:00,  2.03it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.1275\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:19<00:00,  2.04it/s, loss=0.0929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 8.8789\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 40/40 [00:19<00:00,  2.03it/s, loss=0.154] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 6.7591\n",
      "Test Metrics: Precision=0.9271, Recall=0.9271, F1=0.9271\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 320, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 320/320 [00:00<00:00, 8846.53 examples/s]\n",
      "Map: 100%|██████████| 64/64 [00:00<00:00, 6316.87 examples/s]\n",
      "Map: 100%|██████████| 864/864 [00:00<00:00, 9526.85 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:18<00:00,  2.20it/s, loss=0.236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.4937\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:18<00:00,  2.18it/s, loss=0.189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.9512\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:18<00:00,  2.20it/s, loss=0.0936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.2400\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:17<00:00,  2.25it/s, loss=0.0862]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.0574\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 40/40 [00:17<00:00,  2.26it/s, loss=0.0544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.9349\n",
      "Test Metrics: Precision=0.9327, Recall=0.9327, F1=0.9327\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 320, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 320/320 [00:00<00:00, 8861.01 examples/s]\n",
      "Map: 100%|██████████| 64/64 [00:00<00:00, 7884.96 examples/s]\n",
      "Map: 100%|██████████| 864/864 [00:00<00:00, 9174.07 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:17<00:00,  2.32it/s, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.9363\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:18<00:00,  2.20it/s, loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.0238\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:17<00:00,  2.27it/s, loss=0.127] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.2334\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:17<00:00,  2.30it/s, loss=0.122] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7894\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 40/40 [00:18<00:00,  2.21it/s, loss=0.0938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.4680\n",
      "Test Metrics: Precision=0.9317, Recall=0.9317, F1=0.9317\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 320, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 320/320 [00:00<00:00, 8608.83 examples/s]\n",
      "Map: 100%|██████████| 64/64 [00:00<00:00, 7229.81 examples/s]\n",
      "Map: 100%|██████████| 864/864 [00:00<00:00, 9266.86 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:18<00:00,  2.18it/s, loss=0.329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.5582\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:18<00:00,  2.20it/s, loss=0.387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.2223\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:18<00:00,  2.17it/s, loss=0.272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 14.6762\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:18<00:00,  2.18it/s, loss=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 8.5355\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 40/40 [00:18<00:00,  2.22it/s, loss=0.116] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 7.2815\n",
      "Test Metrics: Precision=0.9166, Recall=0.9166, F1=0.9166\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 320, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 320/320 [00:00<00:00, 8096.87 examples/s]\n",
      "Map: 100%|██████████| 64/64 [00:00<00:00, 7218.14 examples/s]\n",
      "Map: 100%|██████████| 864/864 [00:00<00:00, 9051.46 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:19<00:00,  2.02it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.1909\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:19<00:00,  2.06it/s, loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.3482\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:19<00:00,  2.10it/s, loss=0.128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.1155\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:19<00:00,  2.09it/s, loss=0.0688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.7869\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 40/40 [00:19<00:00,  2.09it/s, loss=0.0451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2.7337\n",
      "Test Metrics: Precision=0.9324, Recall=0.9324, F1=0.9324\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 320, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 320/320 [00:00<00:00, 8346.15 examples/s]\n",
      "Map: 100%|██████████| 64/64 [00:00<00:00, 6980.15 examples/s]\n",
      "Map: 100%|██████████| 864/864 [00:00<00:00, 9113.95 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 40/40 [00:20<00:00,  2.00it/s, loss=0.469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.1292\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 40/40 [00:20<00:00,  1.94it/s, loss=0.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9631\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 40/40 [00:19<00:00,  2.07it/s, loss=0.131] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8006\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 40/40 [00:20<00:00,  1.97it/s, loss=0.158] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5091\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 40/40 [00:20<00:00,  1.95it/s, loss=0.104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1976\n",
      "Test Metrics: Precision=0.9336, Recall=0.9336, F1=0.9336\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 325, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 325/325 [00:00<00:00, 8578.98 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6734.59 examples/s]\n",
      "Map: 100%|██████████| 858/858 [00:00<00:00, 9235.97 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 41/41 [00:17<00:00,  2.29it/s, loss=0.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20.5367\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 41/41 [00:18<00:00,  2.24it/s, loss=0.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.7112\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 41/41 [00:17<00:00,  2.30it/s, loss=0.125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.9581\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 41/41 [00:17<00:00,  2.34it/s, loss=0.159] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7861\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 41/41 [00:18<00:00,  2.21it/s, loss=0.0545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.4486\n",
      "Test Metrics: Precision=0.9340, Recall=0.9340, F1=0.9340\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 325, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 325/325 [00:00<00:00, 8713.89 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 7698.14 examples/s]\n",
      "Map: 100%|██████████| 858/858 [00:00<00:00, 9104.88 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 41/41 [00:18<00:00,  2.20it/s, loss=0.356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.9020\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 41/41 [00:17<00:00,  2.29it/s, loss=0.115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.5769\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 41/41 [00:18<00:00,  2.27it/s, loss=0.289] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8676\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 41/41 [00:17<00:00,  2.33it/s, loss=0.167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.4992\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 41/41 [00:18<00:00,  2.20it/s, loss=0.0251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.1972\n",
      "Test Metrics: Precision=0.9341, Recall=0.9341, F1=0.9341\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 325, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 325/325 [00:00<00:00, 8362.98 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6297.32 examples/s]\n",
      "Map: 100%|██████████| 858/858 [00:00<00:00, 9267.39 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 41/41 [00:18<00:00,  2.24it/s, loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.1487\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 41/41 [00:18<00:00,  2.22it/s, loss=0.152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.2225\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 41/41 [00:18<00:00,  2.25it/s, loss=0.218] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.9193\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 41/41 [00:18<00:00,  2.18it/s, loss=0.119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8560\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 41/41 [00:18<00:00,  2.16it/s, loss=0.0765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.4176\n",
      "Test Metrics: Precision=0.9346, Recall=0.9346, F1=0.9346\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 325, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 325/325 [00:00<00:00, 8492.40 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 8211.99 examples/s]\n",
      "Map: 100%|██████████| 858/858 [00:00<00:00, 9046.63 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 41/41 [00:18<00:00,  2.18it/s, loss=0.419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.3591\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 41/41 [00:18<00:00,  2.16it/s, loss=0.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.1406\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 41/41 [00:19<00:00,  2.14it/s, loss=0.188] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.0904\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 41/41 [00:18<00:00,  2.23it/s, loss=0.0641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8156\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 41/41 [00:19<00:00,  2.13it/s, loss=0.0904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.4790\n",
      "Test Metrics: Precision=0.9301, Recall=0.9301, F1=0.9301\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 325, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 325/325 [00:00<00:00, 8630.09 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 5848.04 examples/s]\n",
      "Map: 100%|██████████| 858/858 [00:00<00:00, 9474.39 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 41/41 [00:19<00:00,  2.08it/s, loss=0.452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.1716\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 41/41 [00:19<00:00,  2.09it/s, loss=0.318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.3393\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 41/41 [00:19<00:00,  2.05it/s, loss=0.089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9427\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 41/41 [00:19<00:00,  2.10it/s, loss=0.0282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3530\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 41/41 [00:20<00:00,  2.02it/s, loss=0.0875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1814\n",
      "Test Metrics: Precision=0.9320, Recall=0.9320, F1=0.9320\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 330, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 330/330 [00:00<00:00, 8517.61 examples/s]\n",
      "Map: 100%|██████████| 66/66 [00:00<00:00, 7822.10 examples/s]\n",
      "Map: 100%|██████████| 852/852 [00:00<00:00, 9073.14 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:18<00:00,  2.27it/s, loss=0.372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.7401\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s, loss=0.434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8967\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s, loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4502\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s, loss=0.067] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0160\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 42/42 [00:18<00:00,  2.28it/s, loss=0.0457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5587\n",
      "Test Metrics: Precision=0.9346, Recall=0.9346, F1=0.9346\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 330, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 330/330 [00:00<00:00, 8288.99 examples/s]\n",
      "Map: 100%|██████████| 66/66 [00:00<00:00, 7200.90 examples/s]\n",
      "Map: 100%|██████████| 852/852 [00:00<00:00, 8995.17 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:18<00:00,  2.21it/s, loss=0.435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.9491\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s, loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.9050\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s, loss=0.157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4294\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s, loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0628\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 42/42 [00:18<00:00,  2.30it/s, loss=0.0479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.4874\n",
      "Test Metrics: Precision=0.9347, Recall=0.9347, F1=0.9347\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 330, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 330/330 [00:00<00:00, 8962.54 examples/s]\n",
      "Map: 100%|██████████| 66/66 [00:00<00:00, 6713.16 examples/s]\n",
      "Map: 100%|██████████| 852/852 [00:00<00:00, 9135.91 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:19<00:00,  2.20it/s, loss=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.0725\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:19<00:00,  2.18it/s, loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.5262\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:18<00:00,  2.25it/s, loss=0.168] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.6545\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:19<00:00,  2.21it/s, loss=0.0521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9688\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 42/42 [00:19<00:00,  2.21it/s, loss=0.0502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5285\n",
      "Test Metrics: Precision=0.9358, Recall=0.9358, F1=0.9358\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 330, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 330/330 [00:00<00:00, 8692.69 examples/s]\n",
      "Map: 100%|██████████| 66/66 [00:00<00:00, 7848.71 examples/s]\n",
      "Map: 100%|██████████| 852/852 [00:00<00:00, 8835.77 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:19<00:00,  2.14it/s, loss=0.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.6351\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:19<00:00,  2.14it/s, loss=0.154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.3853\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:18<00:00,  2.29it/s, loss=0.101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.1012\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:19<00:00,  2.13it/s, loss=0.0391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1777\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 42/42 [00:19<00:00,  2.16it/s, loss=0.574] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.0948\n",
      "Test Metrics: Precision=0.9298, Recall=0.9298, F1=0.9298\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 330, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 330/330 [00:00<00:00, 7970.43 examples/s]\n",
      "Map: 100%|██████████| 66/66 [00:00<00:00, 6726.05 examples/s]\n",
      "Map: 100%|██████████| 852/852 [00:00<00:00, 9215.45 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:21<00:00,  1.99it/s, loss=0.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.3080\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:21<00:00,  1.97it/s, loss=0.317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.3615\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:20<00:00,  2.02it/s, loss=0.143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0454\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:20<00:00,  2.05it/s, loss=0.373] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.0127\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 42/42 [00:21<00:00,  1.96it/s, loss=0.0609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.9635\n",
      "Test Metrics: Precision=0.9307, Recall=0.9307, F1=0.9307\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 335, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 335/335 [00:00<00:00, 8581.29 examples/s]\n",
      "Map: 100%|██████████| 67/67 [00:00<00:00, 7153.14 examples/s]\n",
      "Map: 100%|██████████| 846/846 [00:00<00:00, 9270.54 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s, loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.1982\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:19<00:00,  2.16it/s, loss=0.222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.1414\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:18<00:00,  2.24it/s, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3293\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:18<00:00,  2.24it/s, loss=0.162] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2628\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 42/42 [00:18<00:00,  2.24it/s, loss=0.0969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.9053\n",
      "Test Metrics: Precision=0.9335, Recall=0.9335, F1=0.9335\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 335, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 335/335 [00:00<00:00, 8460.48 examples/s]\n",
      "Map: 100%|██████████| 67/67 [00:00<00:00, 6622.17 examples/s]\n",
      "Map: 100%|██████████| 846/846 [00:00<00:00, 8878.03 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s, loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.6754\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:18<00:00,  2.24it/s, loss=0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.2739\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:18<00:00,  2.26it/s, loss=0.0786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.7807\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:18<00:00,  2.23it/s, loss=0.0829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5062\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 42/42 [00:18<00:00,  2.28it/s, loss=0.0776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0273\n",
      "Test Metrics: Precision=0.9335, Recall=0.9335, F1=0.9335\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 335, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 335/335 [00:00<00:00, 8822.57 examples/s]\n",
      "Map: 100%|██████████| 67/67 [00:00<00:00, 7522.91 examples/s]\n",
      "Map: 100%|██████████| 846/846 [00:00<00:00, 9278.80 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:19<00:00,  2.16it/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.0612\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:18<00:00,  2.22it/s, loss=0.293] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.4014\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:19<00:00,  2.21it/s, loss=0.151] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3457\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:19<00:00,  2.17it/s, loss=0.069] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.7686\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 42/42 [00:19<00:00,  2.17it/s, loss=0.0476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.3811\n",
      "Test Metrics: Precision=0.9346, Recall=0.9346, F1=0.9346\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 335, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 335/335 [00:00<00:00, 8455.24 examples/s]\n",
      "Map: 100%|██████████| 67/67 [00:00<00:00, 7225.61 examples/s]\n",
      "Map: 100%|██████████| 846/846 [00:00<00:00, 9169.06 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:19<00:00,  2.15it/s, loss=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.4316\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:19<00:00,  2.20it/s, loss=0.148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8059\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:19<00:00,  2.16it/s, loss=0.228] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.5477\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:19<00:00,  2.19it/s, loss=0.137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1558\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 42/42 [00:20<00:00,  2.10it/s, loss=0.128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.8578\n",
      "Test Metrics: Precision=0.9298, Recall=0.9298, F1=0.9298\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 335, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 335/335 [00:00<00:00, 8580.98 examples/s]\n",
      "Map: 100%|██████████| 67/67 [00:00<00:00, 7371.55 examples/s]\n",
      "Map: 100%|██████████| 846/846 [00:00<00:00, 9245.68 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 42/42 [00:21<00:00,  1.94it/s, loss=0.268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.9810\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 42/42 [00:21<00:00,  1.96it/s, loss=0.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.5533\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 42/42 [00:21<00:00,  1.94it/s, loss=0.213] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8657\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 42/42 [00:20<00:00,  2.01it/s, loss=0.177] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5960\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 42/42 [00:21<00:00,  1.99it/s, loss=0.043] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0565\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 340, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 8607.02 examples/s]\n",
      "Map: 100%|██████████| 68/68 [00:00<00:00, 7101.73 examples/s]\n",
      "Map: 100%|██████████| 840/840 [00:00<00:00, 9107.14 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 43/43 [00:19<00:00,  2.25it/s, loss=0.341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.1643\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 43/43 [00:18<00:00,  2.27it/s, loss=0.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.3622\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 43/43 [00:19<00:00,  2.25it/s, loss=0.23]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1698\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 43/43 [00:19<00:00,  2.23it/s, loss=0.0393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6379\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 43/43 [00:18<00:00,  2.27it/s, loss=0.0901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.2902\n",
      "Test Metrics: Precision=0.9339, Recall=0.9339, F1=0.9339\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 340, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 8714.27 examples/s]\n",
      "Map: 100%|██████████| 68/68 [00:00<00:00, 7123.91 examples/s]\n",
      "Map: 100%|██████████| 840/840 [00:00<00:00, 9073.09 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 43/43 [00:19<00:00,  2.24it/s, loss=0.572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.6159\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 43/43 [00:18<00:00,  2.27it/s, loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.1587\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 43/43 [00:19<00:00,  2.25it/s, loss=0.171] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3707\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 43/43 [00:19<00:00,  2.22it/s, loss=0.0846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6850\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 43/43 [00:19<00:00,  2.21it/s, loss=0.0503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1065\n",
      "Test Metrics: Precision=0.9332, Recall=0.9332, F1=0.9332\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 340, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 8759.66 examples/s]\n",
      "Map: 100%|██████████| 68/68 [00:00<00:00, 7262.86 examples/s]\n",
      "Map: 100%|██████████| 840/840 [00:00<00:00, 9131.83 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 43/43 [00:19<00:00,  2.20it/s, loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.8980\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 43/43 [00:19<00:00,  2.22it/s, loss=0.0779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.7432\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 43/43 [00:19<00:00,  2.18it/s, loss=0.33]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3907\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 43/43 [00:19<00:00,  2.20it/s, loss=0.203] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3659\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 43/43 [00:18<00:00,  2.27it/s, loss=0.092] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0793\n",
      "Test Metrics: Precision=0.9349, Recall=0.9349, F1=0.9349\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 340, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 8896.88 examples/s]\n",
      "Map: 100%|██████████| 68/68 [00:00<00:00, 6585.53 examples/s]\n",
      "Map: 100%|██████████| 840/840 [00:00<00:00, 9134.70 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 43/43 [00:19<00:00,  2.17it/s, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.3830\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 43/43 [00:20<00:00,  2.12it/s, loss=0.453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.0802\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 43/43 [00:20<00:00,  2.12it/s, loss=0.2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5946\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 43/43 [00:20<00:00,  2.12it/s, loss=0.163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.5911\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 43/43 [00:19<00:00,  2.18it/s, loss=0.0446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.2861\n",
      "Test Metrics: Precision=0.9336, Recall=0.9336, F1=0.9336\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 340, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 7393.45 examples/s]\n",
      "Map: 100%|██████████| 68/68 [00:00<00:00, 8101.48 examples/s]\n",
      "Map: 100%|██████████| 840/840 [00:00<00:00, 9136.92 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 43/43 [00:21<00:00,  2.00it/s, loss=0.308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.6443\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 43/43 [00:21<00:00,  2.00it/s, loss=0.15] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.3224\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 43/43 [00:20<00:00,  2.06it/s, loss=0.268] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.5186\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 43/43 [00:20<00:00,  2.09it/s, loss=0.0699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3241\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 43/43 [00:21<00:00,  2.00it/s, loss=0.0747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.6828\n",
      "Test Metrics: Precision=0.9336, Recall=0.9336, F1=0.9336\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 345, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 345/345 [00:00<00:00, 8917.95 examples/s]\n",
      "Map: 100%|██████████| 69/69 [00:00<00:00, 6637.93 examples/s]\n",
      "Map: 100%|██████████| 834/834 [00:00<00:00, 9142.69 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:19<00:00,  2.30it/s, loss=0.528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.7318\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:19<00:00,  2.26it/s, loss=0.148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.0936\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:18<00:00,  2.32it/s, loss=0.0296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6433\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:19<00:00,  2.30it/s, loss=0.0384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3252\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 44/44 [00:18<00:00,  2.34it/s, loss=0.0138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0912\n",
      "Test Metrics: Precision=0.9321, Recall=0.9321, F1=0.9321\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 345, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 345/345 [00:00<00:00, 9059.82 examples/s]\n",
      "Map: 100%|██████████| 69/69 [00:00<00:00, 5868.06 examples/s]\n",
      "Map: 100%|██████████| 834/834 [00:00<00:00, 9381.22 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:19<00:00,  2.29it/s, loss=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.8422\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:19<00:00,  2.25it/s, loss=0.482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.0517\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:19<00:00,  2.26it/s, loss=0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.5299\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:19<00:00,  2.23it/s, loss=0.0322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.5097\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 44/44 [00:20<00:00,  2.19it/s, loss=0.198] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.9821\n",
      "Test Metrics: Precision=0.9311, Recall=0.9311, F1=0.9311\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 345, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 345/345 [00:00<00:00, 8707.79 examples/s]\n",
      "Map: 100%|██████████| 69/69 [00:00<00:00, 6836.12 examples/s]\n",
      "Map: 100%|██████████| 834/834 [00:00<00:00, 3981.55 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:19<00:00,  2.24it/s, loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.0218\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:19<00:00,  2.24it/s, loss=0.0702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.2147\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:19<00:00,  2.24it/s, loss=0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4532\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:19<00:00,  2.24it/s, loss=0.0358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2606\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 44/44 [00:19<00:00,  2.22it/s, loss=0.0278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5045\n",
      "Test Metrics: Precision=0.9338, Recall=0.9338, F1=0.9338\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 345, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 345/345 [00:00<00:00, 9003.84 examples/s]\n",
      "Map: 100%|██████████| 69/69 [00:00<00:00, 7530.36 examples/s]\n",
      "Map: 100%|██████████| 834/834 [00:00<00:00, 8842.30 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:19<00:00,  2.23it/s, loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.9965\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:19<00:00,  2.28it/s, loss=0.154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8698\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:20<00:00,  2.20it/s, loss=0.0941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3621\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:20<00:00,  2.16it/s, loss=0.163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3355\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.352] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.2485\n",
      "Test Metrics: Precision=0.9338, Recall=0.9338, F1=0.9338\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 345, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 345/345 [00:00<00:00, 8231.05 examples/s]\n",
      "Map: 100%|██████████| 69/69 [00:00<00:00, 7557.70 examples/s]\n",
      "Map: 100%|██████████| 834/834 [00:00<00:00, 9147.81 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:21<00:00,  2.01it/s, loss=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.6277\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:20<00:00,  2.12it/s, loss=0.496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.4721\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:21<00:00,  2.01it/s, loss=0.167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2691\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.397] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.4575\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 44/44 [00:21<00:00,  2.02it/s, loss=0.094] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.4891\n",
      "Test Metrics: Precision=0.9300, Recall=0.9300, F1=0.9300\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 350, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 350/350 [00:00<00:00, 8829.26 examples/s]\n",
      "Map: 100%|██████████| 70/70 [00:00<00:00, 6997.34 examples/s]\n",
      "Map: 100%|██████████| 828/828 [00:00<00:00, 9214.24 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:19<00:00,  2.28it/s, loss=0.479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.3434\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:19<00:00,  2.22it/s, loss=0.247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.4536\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:19<00:00,  2.27it/s, loss=0.17]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.5330\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:20<00:00,  2.19it/s, loss=0.092] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0251\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 44/44 [00:19<00:00,  2.26it/s, loss=0.0924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5199\n",
      "Test Metrics: Precision=0.9328, Recall=0.9328, F1=0.9328\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 350, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 350/350 [00:00<00:00, 8707.71 examples/s]\n",
      "Map: 100%|██████████| 70/70 [00:00<00:00, 7315.71 examples/s]\n",
      "Map: 100%|██████████| 828/828 [00:00<00:00, 8985.91 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:19<00:00,  2.28it/s, loss=0.405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.8986\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:20<00:00,  2.15it/s, loss=0.267] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.7531\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:20<00:00,  2.16it/s, loss=0.0566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9095\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:19<00:00,  2.25it/s, loss=0.0321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2268\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 44/44 [00:19<00:00,  2.23it/s, loss=0.118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0694\n",
      "Test Metrics: Precision=0.9356, Recall=0.9356, F1=0.9356\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 350, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 350/350 [00:00<00:00, 8787.09 examples/s]\n",
      "Map: 100%|██████████| 70/70 [00:00<00:00, 7507.06 examples/s]\n",
      "Map: 100%|██████████| 828/828 [00:00<00:00, 8952.18 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:20<00:00,  2.17it/s, loss=0.396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.2045\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:19<00:00,  2.23it/s, loss=0.154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.6428\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:19<00:00,  2.21it/s, loss=0.208] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.9322\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:19<00:00,  2.24it/s, loss=0.161] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6515\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 44/44 [00:19<00:00,  2.24it/s, loss=0.0542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.2978\n",
      "Test Metrics: Precision=0.9369, Recall=0.9369, F1=0.9369\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 350, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 350/350 [00:00<00:00, 8728.94 examples/s]\n",
      "Map: 100%|██████████| 70/70 [00:00<00:00, 7293.35 examples/s]\n",
      "Map: 100%|██████████| 828/828 [00:00<00:00, 9101.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:20<00:00,  2.12it/s, loss=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.2594\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:20<00:00,  2.11it/s, loss=0.139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.1738\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:20<00:00,  2.15it/s, loss=0.153] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3519\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:20<00:00,  2.14it/s, loss=0.0873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8558\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 44/44 [00:19<00:00,  2.21it/s, loss=0.0585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.6550\n",
      "Test Metrics: Precision=0.9323, Recall=0.9323, F1=0.9323\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 350, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 350/350 [00:00<00:00, 8763.85 examples/s]\n",
      "Map: 100%|██████████| 70/70 [00:00<00:00, 7306.42 examples/s]\n",
      "Map: 100%|██████████| 828/828 [00:00<00:00, 9126.58 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 44/44 [00:21<00:00,  2.05it/s, loss=0.458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.6615\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 44/44 [00:22<00:00,  1.97it/s, loss=0.216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9395\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 44/44 [00:21<00:00,  2.05it/s, loss=0.223] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8904\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 44/44 [00:22<00:00,  1.96it/s, loss=0.0548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2493\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 44/44 [00:20<00:00,  2.11it/s, loss=0.0264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.6713\n",
      "Test Metrics: Precision=0.9369, Recall=0.9369, F1=0.9369\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 355, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 355/355 [00:00<00:00, 8627.75 examples/s]\n",
      "Map: 100%|██████████| 71/71 [00:00<00:00, 6291.77 examples/s]\n",
      "Map: 100%|██████████| 822/822 [00:00<00:00, 9330.84 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:19<00:00,  2.27it/s, loss=0.763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.0448\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:19<00:00,  2.27it/s, loss=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.3846\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:19<00:00,  2.31it/s, loss=0.168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.2160\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:20<00:00,  2.22it/s, loss=0.265] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.4462\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 45/45 [00:20<00:00,  2.22it/s, loss=0.0921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 6.1088\n",
      "Test Metrics: Precision=0.9298, Recall=0.9298, F1=0.9298\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 355, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 355/355 [00:00<00:00, 9079.63 examples/s]\n",
      "Map: 100%|██████████| 71/71 [00:00<00:00, 7419.29 examples/s]\n",
      "Map: 100%|██████████| 822/822 [00:00<00:00, 9134.53 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:20<00:00,  2.14it/s, loss=0.282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.7504\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:20<00:00,  2.17it/s, loss=0.236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.6636\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:20<00:00,  2.14it/s, loss=0.242] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8297\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:21<00:00,  2.12it/s, loss=0.0967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6452\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 45/45 [00:20<00:00,  2.18it/s, loss=0.112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0467\n",
      "Test Metrics: Precision=0.9333, Recall=0.9333, F1=0.9333\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 355, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 355/355 [00:00<00:00, 8340.49 examples/s]\n",
      "Map: 100%|██████████| 71/71 [00:00<00:00, 8167.06 examples/s]\n",
      "Map: 100%|██████████| 822/822 [00:00<00:00, 9054.71 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:20<00:00,  2.21it/s, loss=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.1796\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:20<00:00,  2.18it/s, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.0466\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:20<00:00,  2.19it/s, loss=0.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6619\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:20<00:00,  2.18it/s, loss=0.0557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1632\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 45/45 [00:20<00:00,  2.16it/s, loss=0.0499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.6252\n",
      "Test Metrics: Precision=0.9345, Recall=0.9345, F1=0.9345\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 355, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 355/355 [00:00<00:00, 8891.07 examples/s]\n",
      "Map: 100%|██████████| 71/71 [00:00<00:00, 7809.39 examples/s]\n",
      "Map: 100%|██████████| 822/822 [00:00<00:00, 8986.41 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:20<00:00,  2.25it/s, loss=0.273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.2704\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:20<00:00,  2.19it/s, loss=0.232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9965\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:20<00:00,  2.18it/s, loss=0.133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.2448\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:20<00:00,  2.17it/s, loss=0.0989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9340\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 45/45 [00:20<00:00,  2.25it/s, loss=0.117] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.2282\n",
      "Test Metrics: Precision=0.9269, Recall=0.9269, F1=0.9269\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 355, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 355/355 [00:00<00:00, 8466.93 examples/s]\n",
      "Map: 100%|██████████| 71/71 [00:00<00:00, 6714.06 examples/s]\n",
      "Map: 100%|██████████| 822/822 [00:00<00:00, 9315.89 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:22<00:00,  2.01it/s, loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.2427\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:21<00:00,  2.08it/s, loss=0.129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.0232\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:22<00:00,  1.98it/s, loss=0.031] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4800\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:22<00:00,  2.03it/s, loss=0.206] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3774\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 45/45 [00:21<00:00,  2.07it/s, loss=0.216] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.6176\n",
      "Test Metrics: Precision=0.9370, Recall=0.9370, F1=0.9370\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8902.85 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 6952.85 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9261.96 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:20<00:00,  2.20it/s, loss=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.9110\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:20<00:00,  2.16it/s, loss=0.168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.6932\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:19<00:00,  2.29it/s, loss=0.0925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.5824\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:20<00:00,  2.20it/s, loss=0.101] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9776\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 45/45 [00:20<00:00,  2.20it/s, loss=0.0743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.4608\n",
      "Test Metrics: Precision=0.9349, Recall=0.9349, F1=0.9349\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9123.67 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 6952.69 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9260.53 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:22<00:00,  2.03it/s, loss=0.322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.6496\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:21<00:00,  2.12it/s, loss=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.0465\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:20<00:00,  2.18it/s, loss=0.157] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1560\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:21<00:00,  2.11it/s, loss=0.0472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0227\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 45/45 [00:21<00:00,  2.11it/s, loss=0.0542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5139\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8999.95 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7626.39 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9208.28 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:20<00:00,  2.16it/s, loss=0.326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.8143\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:21<00:00,  2.12it/s, loss=0.256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9519\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:21<00:00,  2.14it/s, loss=0.118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.5372\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:21<00:00,  2.14it/s, loss=0.0848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5222\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 45/45 [00:20<00:00,  2.16it/s, loss=0.0829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0624\n",
      "Test Metrics: Precision=0.9338, Recall=0.9338, F1=0.9338\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8094.07 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7120.39 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9064.66 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:21<00:00,  2.14it/s, loss=0.513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.0913\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:20<00:00,  2.17it/s, loss=0.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.1657\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:20<00:00,  2.17it/s, loss=0.143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.7580\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:21<00:00,  2.09it/s, loss=0.0781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5648\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 45/45 [00:21<00:00,  2.08it/s, loss=0.0396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1677\n",
      "Test Metrics: Precision=0.9327, Recall=0.9327, F1=0.9327\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8594.30 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7024.82 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9201.20 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:22<00:00,  2.02it/s, loss=0.214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.5551\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:23<00:00,  1.95it/s, loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.6181\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:22<00:00,  1.97it/s, loss=0.079] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4607\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 45/45 [00:22<00:00,  2.00it/s, loss=0.0961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2105\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 45/45 [00:21<00:00,  2.10it/s, loss=0.0668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.2268\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 365, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 365/365 [00:00<00:00, 8842.81 examples/s]\n",
      "Map: 100%|██████████| 73/73 [00:00<00:00, 6967.28 examples/s]\n",
      "Map: 100%|██████████| 810/810 [00:00<00:00, 9364.22 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 46/46 [00:20<00:00,  2.29it/s, loss=0.444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.3021\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 46/46 [00:19<00:00,  2.30it/s, loss=0.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.4347\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 46/46 [00:19<00:00,  2.33it/s, loss=0.262] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3697\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 46/46 [00:20<00:00,  2.26it/s, loss=0.1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6520\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 46/46 [00:20<00:00,  2.25it/s, loss=0.0669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1334\n",
      "Test Metrics: Precision=0.9333, Recall=0.9333, F1=0.9333\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 365, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 365/365 [00:00<00:00, 9112.25 examples/s]\n",
      "Map: 100%|██████████| 73/73 [00:00<00:00, 6121.48 examples/s]\n",
      "Map: 100%|██████████| 810/810 [00:00<00:00, 8694.79 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 46/46 [00:21<00:00,  2.16it/s, loss=0.257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.7482\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 46/46 [00:21<00:00,  2.16it/s, loss=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8908\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 46/46 [00:21<00:00,  2.16it/s, loss=0.187] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8133\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 46/46 [00:21<00:00,  2.11it/s, loss=0.163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.4660\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 46/46 [00:21<00:00,  2.14it/s, loss=0.0588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.7301\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 365, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 365/365 [00:00<00:00, 8587.32 examples/s]\n",
      "Map: 100%|██████████| 73/73 [00:00<00:00, 7588.21 examples/s]\n",
      "Map: 100%|██████████| 810/810 [00:00<00:00, 9504.72 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 46/46 [00:21<00:00,  2.13it/s, loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.7396\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 46/46 [00:21<00:00,  2.14it/s, loss=0.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.4262\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 46/46 [00:21<00:00,  2.17it/s, loss=0.135] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3166\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 46/46 [00:21<00:00,  2.16it/s, loss=0.0621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9677\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 46/46 [00:21<00:00,  2.14it/s, loss=0.0317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5279\n",
      "Test Metrics: Precision=0.9347, Recall=0.9347, F1=0.9347\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 365, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 365/365 [00:00<00:00, 8826.24 examples/s]\n",
      "Map: 100%|██████████| 73/73 [00:00<00:00, 6503.49 examples/s]\n",
      "Map: 100%|██████████| 810/810 [00:00<00:00, 9107.15 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 46/46 [00:20<00:00,  2.28it/s, loss=0.281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.4918\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 46/46 [00:21<00:00,  2.13it/s, loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.2400\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 46/46 [00:20<00:00,  2.20it/s, loss=0.0905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5470\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 46/46 [00:21<00:00,  2.14it/s, loss=0.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.5859\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 46/46 [00:21<00:00,  2.15it/s, loss=0.0805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.3011\n",
      "Test Metrics: Precision=0.9343, Recall=0.9343, F1=0.9343\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 365, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 365/365 [00:00<00:00, 8753.12 examples/s]\n",
      "Map: 100%|██████████| 73/73 [00:00<00:00, 6846.85 examples/s]\n",
      "Map: 100%|██████████| 810/810 [00:00<00:00, 9038.53 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 46/46 [00:22<00:00,  2.03it/s, loss=0.405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.0059\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 46/46 [00:22<00:00,  2.01it/s, loss=0.215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9806\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 46/46 [00:23<00:00,  2.00it/s, loss=0.209] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.5364\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 46/46 [00:21<00:00,  2.10it/s, loss=0.141] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2626\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 46/46 [00:22<00:00,  2.06it/s, loss=0.158] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5633\n",
      "Test Metrics: Precision=0.9348, Recall=0.9348, F1=0.9348\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 370, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 370/370 [00:00<00:00, 8638.42 examples/s]\n",
      "Map: 100%|██████████| 74/74 [00:00<00:00, 6501.30 examples/s]\n",
      "Map: 100%|██████████| 804/804 [00:00<00:00, 8997.46 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:19<00:00,  2.38it/s, loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.2242\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:20<00:00,  2.25it/s, loss=0.223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.4858\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:20<00:00,  2.27it/s, loss=0.179] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0622\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:20<00:00,  2.24it/s, loss=0.0795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5407\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 47/47 [00:20<00:00,  2.26it/s, loss=0.0148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.2227\n",
      "Test Metrics: Precision=0.9353, Recall=0.9353, F1=0.9353\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 370, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 370/370 [00:00<00:00, 8985.64 examples/s]\n",
      "Map: 100%|██████████| 74/74 [00:00<00:00, 6941.26 examples/s]\n",
      "Map: 100%|██████████| 804/804 [00:00<00:00, 9141.41 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:21<00:00,  2.14it/s, loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.7031\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:21<00:00,  2.16it/s, loss=0.137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8046\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:21<00:00,  2.15it/s, loss=0.197] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2729\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:21<00:00,  2.15it/s, loss=0.147] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2862\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 47/47 [00:21<00:00,  2.14it/s, loss=0.056] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.7172\n",
      "Test Metrics: Precision=0.9337, Recall=0.9337, F1=0.9337\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 370, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 370/370 [00:00<00:00, 8564.01 examples/s]\n",
      "Map: 100%|██████████| 74/74 [00:00<00:00, 8217.59 examples/s]\n",
      "Map: 100%|██████████| 804/804 [00:00<00:00, 9002.60 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:21<00:00,  2.15it/s, loss=0.626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.9041\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:21<00:00,  2.14it/s, loss=0.103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.1589\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:21<00:00,  2.19it/s, loss=0.0641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.9446\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:22<00:00,  2.13it/s, loss=0.125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8729\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 47/47 [00:21<00:00,  2.17it/s, loss=0.138] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.7820\n",
      "Test Metrics: Precision=0.9338, Recall=0.9338, F1=0.9338\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 370, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 370/370 [00:00<00:00, 9094.22 examples/s]\n",
      "Map: 100%|██████████| 74/74 [00:00<00:00, 7119.10 examples/s]\n",
      "Map: 100%|██████████| 804/804 [00:00<00:00, 8957.88 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:21<00:00,  2.20it/s, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.8452\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:21<00:00,  2.23it/s, loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.8914\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:21<00:00,  2.16it/s, loss=0.161] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4860\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:20<00:00,  2.26it/s, loss=0.0255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2266\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 47/47 [00:21<00:00,  2.18it/s, loss=0.089] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.9508\n",
      "Test Metrics: Precision=0.9346, Recall=0.9346, F1=0.9346\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 370, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 370/370 [00:00<00:00, 8633.86 examples/s]\n",
      "Map: 100%|██████████| 74/74 [00:00<00:00, 6707.70 examples/s]\n",
      "Map: 100%|██████████| 804/804 [00:00<00:00, 9289.74 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:22<00:00,  2.06it/s, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.8653\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:22<00:00,  2.09it/s, loss=0.297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9125\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:22<00:00,  2.10it/s, loss=0.0464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6830\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:23<00:00,  2.03it/s, loss=0.072] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9487\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 47/47 [00:22<00:00,  2.08it/s, loss=0.0697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5594\n",
      "Test Metrics: Precision=0.9370, Recall=0.9370, F1=0.9370\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 375, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 375/375 [00:00<00:00, 9000.55 examples/s]\n",
      "Map: 100%|██████████| 75/75 [00:00<00:00, 7856.66 examples/s]\n",
      "Map: 100%|██████████| 798/798 [00:00<00:00, 9116.98 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:21<00:00,  2.19it/s, loss=0.354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.5326\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:21<00:00,  2.20it/s, loss=0.297] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.4225\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:21<00:00,  2.18it/s, loss=0.146] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1844\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:21<00:00,  2.21it/s, loss=0.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7171\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 47/47 [00:20<00:00,  2.24it/s, loss=0.0568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1185\n",
      "Test Metrics: Precision=0.9330, Recall=0.9330, F1=0.9330\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 375, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 375/375 [00:00<00:00, 8885.63 examples/s]\n",
      "Map: 100%|██████████| 75/75 [00:00<00:00, 6970.68 examples/s]\n",
      "Map: 100%|██████████| 798/798 [00:00<00:00, 8990.18 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:21<00:00,  2.14it/s, loss=0.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.4124\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:21<00:00,  2.16it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.5567\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:21<00:00,  2.15it/s, loss=0.117] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2434\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:21<00:00,  2.15it/s, loss=0.0681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6439\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 47/47 [00:22<00:00,  2.07it/s, loss=0.0957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.3519\n",
      "Test Metrics: Precision=0.9348, Recall=0.9348, F1=0.9348\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 375, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 375/375 [00:00<00:00, 8678.06 examples/s]\n",
      "Map: 100%|██████████| 75/75 [00:00<00:00, 9068.37 examples/s]\n",
      "Map: 100%|██████████| 798/798 [00:00<00:00, 9090.32 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:22<00:00,  2.12it/s, loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.8588\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:22<00:00,  2.08it/s, loss=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.1413\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:22<00:00,  2.11it/s, loss=0.19]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.9823\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:22<00:00,  2.12it/s, loss=0.0441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.2266\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 47/47 [00:22<00:00,  2.07it/s, loss=0.0524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.0187\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 375, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 375/375 [00:00<00:00, 8453.58 examples/s]\n",
      "Map: 100%|██████████| 75/75 [00:00<00:00, 7060.80 examples/s]\n",
      "Map: 100%|██████████| 798/798 [00:00<00:00, 9025.16 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:21<00:00,  2.17it/s, loss=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.8157\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:22<00:00,  2.11it/s, loss=0.133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.5125\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:22<00:00,  2.13it/s, loss=0.135] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.5438\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:21<00:00,  2.20it/s, loss=0.0748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9982\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 47/47 [00:21<00:00,  2.19it/s, loss=0.0525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.6048\n",
      "Test Metrics: Precision=0.9351, Recall=0.9351, F1=0.9351\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 375, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 375/375 [00:00<00:00, 8067.62 examples/s]\n",
      "Map: 100%|██████████| 75/75 [00:00<00:00, 6873.50 examples/s]\n",
      "Map: 100%|██████████| 798/798 [00:00<00:00, 9289.63 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 47/47 [00:23<00:00,  2.01it/s, loss=0.939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 46.9598\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 47/47 [00:22<00:00,  2.05it/s, loss=0.912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 40.5876\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 47/47 [00:23<00:00,  2.02it/s, loss=0.623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 29.6969\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 47/47 [00:22<00:00,  2.05it/s, loss=0.406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 21.3968\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 47/47 [00:22<00:00,  2.09it/s, loss=0.365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 17.6996\n",
      "Test Metrics: Precision=0.8839, Recall=0.8839, F1=0.8839\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8466.45 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 6921.44 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9276.40 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:21<00:00,  2.21it/s, loss=0.334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.8210\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:21<00:00,  2.25it/s, loss=0.52] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5260\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:21<00:00,  2.23it/s, loss=0.16]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5147\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 48/48 [00:21<00:00,  2.19it/s, loss=0.104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8710\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 48/48 [00:21<00:00,  2.25it/s, loss=0.0422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.3669\n",
      "Test Metrics: Precision=0.9343, Recall=0.9343, F1=0.9343\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8958.41 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7836.54 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9376.93 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:22<00:00,  2.10it/s, loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.0517\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:22<00:00,  2.11it/s, loss=0.276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.6855\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:22<00:00,  2.13it/s, loss=0.175] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8345\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 48/48 [00:22<00:00,  2.16it/s, loss=0.0891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2900\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 48/48 [00:21<00:00,  2.19it/s, loss=0.117] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.7914\n",
      "Test Metrics: Precision=0.9351, Recall=0.9351, F1=0.9351\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8610.49 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 8084.38 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9245.39 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:21<00:00,  2.22it/s, loss=0.631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.1704\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:22<00:00,  2.17it/s, loss=0.114] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.4780\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:22<00:00,  2.18it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4374\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 48/48 [00:22<00:00,  2.15it/s, loss=0.137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0073\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 48/48 [00:22<00:00,  2.13it/s, loss=0.078] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.8455\n",
      "Test Metrics: Precision=0.9353, Recall=0.9353, F1=0.9353\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 9189.28 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7781.26 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 3895.31 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:22<00:00,  2.17it/s, loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.9448\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:21<00:00,  2.19it/s, loss=0.406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.0946\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:21<00:00,  2.18it/s, loss=0.0736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0849\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 48/48 [00:22<00:00,  2.15it/s, loss=0.111] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1635\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 48/48 [00:21<00:00,  2.20it/s, loss=0.0867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.6977\n",
      "Test Metrics: Precision=0.9317, Recall=0.9317, F1=0.9317\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8826.45 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 6710.89 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9261.30 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:22<00:00,  2.10it/s, loss=0.139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.9395\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:23<00:00,  2.02it/s, loss=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.9476\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:24<00:00,  1.99it/s, loss=0.379] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9446\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 48/48 [00:23<00:00,  2.06it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9731\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 48/48 [00:23<00:00,  2.04it/s, loss=0.0902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1488\n",
      "Test Metrics: Precision=0.9374, Recall=0.9374, F1=0.9374\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 385, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 385/385 [00:00<00:00, 8608.22 examples/s]\n",
      "Map: 100%|██████████| 77/77 [00:00<00:00, 7604.64 examples/s]\n",
      "Map: 100%|██████████| 786/786 [00:00<00:00, 9368.48 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:22<00:00,  2.21it/s, loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.7323\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:21<00:00,  2.26it/s, loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.2301\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:21<00:00,  2.23it/s, loss=0.473] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.1572\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:21<00:00,  2.25it/s, loss=0.286] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.4259\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 49/49 [00:22<00:00,  2.21it/s, loss=0.015] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.8170\n",
      "Test Metrics: Precision=0.9358, Recall=0.9358, F1=0.9358\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 385, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 385/385 [00:00<00:00, 8851.07 examples/s]\n",
      "Map: 100%|██████████| 77/77 [00:00<00:00, 7113.69 examples/s]\n",
      "Map: 100%|██████████| 786/786 [00:00<00:00, 9112.15 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:23<00:00,  2.11it/s, loss=0.142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.3466\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:21<00:00,  2.24it/s, loss=1.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.9877\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:22<00:00,  2.20it/s, loss=0.208] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5377\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:23<00:00,  2.12it/s, loss=0.0141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7931\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 49/49 [00:22<00:00,  2.16it/s, loss=0.208] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.4833\n",
      "Test Metrics: Precision=0.9325, Recall=0.9325, F1=0.9325\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 385, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 385/385 [00:00<00:00, 8826.98 examples/s]\n",
      "Map: 100%|██████████| 77/77 [00:00<00:00, 7595.34 examples/s]\n",
      "Map: 100%|██████████| 786/786 [00:00<00:00, 9076.86 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:22<00:00,  2.19it/s, loss=0.362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.5615\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:22<00:00,  2.16it/s, loss=0.329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.6947\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:23<00:00,  2.11it/s, loss=0.103] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0105\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:22<00:00,  2.22it/s, loss=0.0585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1382\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 49/49 [00:22<00:00,  2.17it/s, loss=0.305] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.8686\n",
      "Test Metrics: Precision=0.9373, Recall=0.9373, F1=0.9373\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 385, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 385/385 [00:00<00:00, 8955.93 examples/s]\n",
      "Map: 100%|██████████| 77/77 [00:00<00:00, 6786.33 examples/s]\n",
      "Map: 100%|██████████| 786/786 [00:00<00:00, 9083.51 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:22<00:00,  2.20it/s, loss=0.597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.4131\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:23<00:00,  2.13it/s, loss=0.0705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.8273\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:22<00:00,  2.15it/s, loss=0.145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9595\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:23<00:00,  2.13it/s, loss=0.0411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1825\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 49/49 [00:22<00:00,  2.15it/s, loss=0.0412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.8866\n",
      "Test Metrics: Precision=0.9319, Recall=0.9319, F1=0.9319\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 385, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 385/385 [00:00<00:00, 8469.08 examples/s]\n",
      "Map: 100%|██████████| 77/77 [00:00<00:00, 6529.88 examples/s]\n",
      "Map: 100%|██████████| 786/786 [00:00<00:00, 9243.48 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:24<00:00,  2.01it/s, loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.6499\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:23<00:00,  2.06it/s, loss=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2476\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:23<00:00,  2.04it/s, loss=0.202] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5920\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:22<00:00,  2.14it/s, loss=0.0543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1015\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 49/49 [00:23<00:00,  2.07it/s, loss=0.162] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.7788\n",
      "Test Metrics: Precision=0.9365, Recall=0.9365, F1=0.9365\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 390, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 390/390 [00:00<00:00, 8735.57 examples/s]\n",
      "Map: 100%|██████████| 78/78 [00:00<00:00, 7819.77 examples/s]\n",
      "Map: 100%|██████████| 780/780 [00:00<00:00, 9122.71 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:21<00:00,  2.30it/s, loss=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.2917\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:21<00:00,  2.26it/s, loss=0.192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.9885\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:22<00:00,  2.21it/s, loss=0.162] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2462\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:21<00:00,  2.24it/s, loss=0.114] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3984\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 49/49 [00:22<00:00,  2.22it/s, loss=0.138] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.5645\n",
      "Test Metrics: Precision=0.9347, Recall=0.9347, F1=0.9347\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 390, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 390/390 [00:00<00:00, 2466.59 examples/s]\n",
      "Map: 100%|██████████| 78/78 [00:00<00:00, 7275.46 examples/s]\n",
      "Map: 100%|██████████| 780/780 [00:00<00:00, 8830.76 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:21<00:00,  2.26it/s, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.6161\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:22<00:00,  2.21it/s, loss=0.287] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.3764\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:22<00:00,  2.20it/s, loss=0.126] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0104\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:22<00:00,  2.22it/s, loss=0.0387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6911\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 49/49 [00:22<00:00,  2.15it/s, loss=0.0707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1339\n",
      "Test Metrics: Precision=0.9323, Recall=0.9323, F1=0.9323\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 390, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 390/390 [00:00<00:00, 8414.97 examples/s]\n",
      "Map: 100%|██████████| 78/78 [00:00<00:00, 7556.43 examples/s]\n",
      "Map: 100%|██████████| 780/780 [00:00<00:00, 8828.61 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:23<00:00,  2.12it/s, loss=0.372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.4227\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:22<00:00,  2.14it/s, loss=0.269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.1761\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:23<00:00,  2.08it/s, loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8223\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:23<00:00,  2.13it/s, loss=0.117] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.7412\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 49/49 [00:23<00:00,  2.11it/s, loss=0.0579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.5512\n",
      "Test Metrics: Precision=0.9360, Recall=0.9360, F1=0.9360\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 390, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 390/390 [00:00<00:00, 8637.50 examples/s]\n",
      "Map: 100%|██████████| 78/78 [00:00<00:00, 6846.70 examples/s]\n",
      "Map: 100%|██████████| 780/780 [00:00<00:00, 9218.22 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:22<00:00,  2.14it/s, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.3470\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:22<00:00,  2.19it/s, loss=0.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8778\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:22<00:00,  2.17it/s, loss=0.134] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5261\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:22<00:00,  2.15it/s, loss=0.0654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8144\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 49/49 [00:22<00:00,  2.21it/s, loss=0.0401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.2788\n",
      "Test Metrics: Precision=0.9327, Recall=0.9327, F1=0.9327\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 390, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 390/390 [00:00<00:00, 8704.51 examples/s]\n",
      "Map: 100%|██████████| 78/78 [00:00<00:00, 7356.44 examples/s]\n",
      "Map: 100%|██████████| 780/780 [00:00<00:00, 9096.17 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 49/49 [00:23<00:00,  2.08it/s, loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.9177\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 49/49 [00:24<00:00,  2.03it/s, loss=0.433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.9344\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 49/49 [00:24<00:00,  2.03it/s, loss=0.159] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7692\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 49/49 [00:23<00:00,  2.05it/s, loss=0.105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0636\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 49/49 [00:23<00:00,  2.06it/s, loss=0.21]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.5185\n",
      "Test Metrics: Precision=0.9363, Recall=0.9363, F1=0.9363\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 395, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 395/395 [00:00<00:00, 8514.19 examples/s]\n",
      "Map: 100%|██████████| 79/79 [00:00<00:00, 6811.45 examples/s]\n",
      "Map: 100%|██████████| 774/774 [00:00<00:00, 9287.11 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:22<00:00,  2.22it/s, loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.8646\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:22<00:00,  2.21it/s, loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1130\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:21<00:00,  2.28it/s, loss=0.101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7047\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:22<00:00,  2.24it/s, loss=0.0956]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.7737\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 50/50 [00:21<00:00,  2.28it/s, loss=0.143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.9462\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 395, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 395/395 [00:00<00:00, 8020.05 examples/s]\n",
      "Map: 100%|██████████| 79/79 [00:00<00:00, 7683.66 examples/s]\n",
      "Map: 100%|██████████| 774/774 [00:00<00:00, 9229.89 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:22<00:00,  2.18it/s, loss=0.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.1361\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:22<00:00,  2.22it/s, loss=0.42] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.9848\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:23<00:00,  2.17it/s, loss=0.292] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4411\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:22<00:00,  2.18it/s, loss=0.221] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9428\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 50/50 [00:22<00:00,  2.23it/s, loss=0.0347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.3980\n",
      "Test Metrics: Precision=0.9320, Recall=0.9320, F1=0.9320\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 395, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 395/395 [00:00<00:00, 8416.39 examples/s]\n",
      "Map: 100%|██████████| 79/79 [00:00<00:00, 7284.82 examples/s]\n",
      "Map: 100%|██████████| 774/774 [00:00<00:00, 9286.95 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:23<00:00,  2.13it/s, loss=0.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.2422\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:23<00:00,  2.11it/s, loss=0.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.8469\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:23<00:00,  2.11it/s, loss=0.105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.1595\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:23<00:00,  2.17it/s, loss=0.215] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3214\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 50/50 [00:23<00:00,  2.15it/s, loss=0.19]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.7386\n",
      "Test Metrics: Precision=0.9327, Recall=0.9327, F1=0.9327\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 395, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 395/395 [00:00<00:00, 8875.05 examples/s]\n",
      "Map: 100%|██████████| 79/79 [00:00<00:00, 7577.18 examples/s]\n",
      "Map: 100%|██████████| 774/774 [00:00<00:00, 9102.81 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:22<00:00,  2.19it/s, loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.2020\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:22<00:00,  2.21it/s, loss=0.0966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8748\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:23<00:00,  2.15it/s, loss=0.115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8240\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:22<00:00,  2.23it/s, loss=0.085] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6116\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 50/50 [00:23<00:00,  2.16it/s, loss=0.0989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1160\n",
      "Test Metrics: Precision=0.9369, Recall=0.9369, F1=0.9369\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 395, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 395/395 [00:00<00:00, 7799.52 examples/s]\n",
      "Map: 100%|██████████| 79/79 [00:00<00:00, 7876.91 examples/s]\n",
      "Map: 100%|██████████| 774/774 [00:00<00:00, 3817.00 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:23<00:00,  2.16it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.7526\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:24<00:00,  2.05it/s, loss=0.0717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.2245\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:24<00:00,  2.06it/s, loss=0.0723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9458\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:24<00:00,  2.03it/s, loss=0.138] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3186\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 50/50 [00:24<00:00,  2.07it/s, loss=0.138] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.8303\n",
      "Test Metrics: Precision=0.9369, Recall=0.9369, F1=0.9369\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8765.89 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7315.59 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9156.05 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:22<00:00,  2.18it/s, loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.2702\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:22<00:00,  2.22it/s, loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.1909\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:22<00:00,  2.20it/s, loss=0.136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4692\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:22<00:00,  2.23it/s, loss=0.0689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0162\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 50/50 [00:22<00:00,  2.23it/s, loss=0.0461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.2629\n",
      "Test Metrics: Precision=0.9393, Recall=0.9393, F1=0.9393\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8881.77 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7503.73 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9162.40 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:22<00:00,  2.22it/s, loss=0.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.9035\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:22<00:00,  2.18it/s, loss=0.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.8535\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:23<00:00,  2.15it/s, loss=0.138] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4674\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:22<00:00,  2.21it/s, loss=0.0996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7952\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 50/50 [00:23<00:00,  2.17it/s, loss=0.0351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.5248\n",
      "Test Metrics: Precision=0.9334, Recall=0.9334, F1=0.9334\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8351.24 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6596.11 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9155.03 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:23<00:00,  2.12it/s, loss=0.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.3865\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:23<00:00,  2.10it/s, loss=0.323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5737\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:23<00:00,  2.16it/s, loss=0.173] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6627\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:23<00:00,  2.14it/s, loss=0.165] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1665\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 50/50 [00:23<00:00,  2.14it/s, loss=0.085] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.8864\n",
      "Test Metrics: Precision=0.9339, Recall=0.9339, F1=0.9339\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8716.98 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6546.18 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9032.25 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:23<00:00,  2.15it/s, loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.4572\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:23<00:00,  2.15it/s, loss=0.149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5205\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:23<00:00,  2.17it/s, loss=0.204] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6857\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:22<00:00,  2.19it/s, loss=0.106] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0735\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 50/50 [00:23<00:00,  2.17it/s, loss=0.0745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.6787\n",
      "Test Metrics: Precision=0.9341, Recall=0.9341, F1=0.9341\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8598.06 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7103.87 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9034.58 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:24<00:00,  2.03it/s, loss=0.407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.9627\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:23<00:00,  2.09it/s, loss=0.421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.6364\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:24<00:00,  2.07it/s, loss=0.0439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.1003\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 50/50 [00:24<00:00,  2.04it/s, loss=0.173] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1287\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 50/50 [00:23<00:00,  2.09it/s, loss=0.104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.5067\n",
      "Test Metrics: Precision=0.9356, Recall=0.9356, F1=0.9356\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 405, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 405/405 [00:00<00:00, 7978.87 examples/s]\n",
      "Map: 100%|██████████| 81/81 [00:00<00:00, 7016.93 examples/s]\n",
      "Map: 100%|██████████| 762/762 [00:00<00:00, 9014.89 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 51/51 [00:23<00:00,  2.19it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.9941\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 51/51 [00:22<00:00,  2.23it/s, loss=0.229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5072\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 51/51 [00:23<00:00,  2.21it/s, loss=0.139] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.2913\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 51/51 [00:22<00:00,  2.29it/s, loss=0.0646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2991\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 51/51 [00:22<00:00,  2.25it/s, loss=0.0964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.8276\n",
      "Test Metrics: Precision=0.9357, Recall=0.9357, F1=0.9357\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 405, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 405/405 [00:00<00:00, 8830.62 examples/s]\n",
      "Map: 100%|██████████| 81/81 [00:00<00:00, 7883.48 examples/s]\n",
      "Map: 100%|██████████| 762/762 [00:00<00:00, 8942.88 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 51/51 [00:22<00:00,  2.24it/s, loss=0.307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.6178\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 51/51 [00:23<00:00,  2.21it/s, loss=0.149] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.1606\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 51/51 [00:23<00:00,  2.18it/s, loss=0.0445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8149\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 51/51 [00:23<00:00,  2.20it/s, loss=0.0964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.0254\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 51/51 [00:23<00:00,  2.18it/s, loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.7635\n",
      "Test Metrics: Precision=0.9341, Recall=0.9341, F1=0.9341\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 405, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 405/405 [00:00<00:00, 7861.23 examples/s]\n",
      "Map: 100%|██████████| 81/81 [00:00<00:00, 6541.74 examples/s]\n",
      "Map: 100%|██████████| 762/762 [00:00<00:00, 9095.81 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 51/51 [00:23<00:00,  2.19it/s, loss=0.247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.6024\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 51/51 [00:23<00:00,  2.21it/s, loss=0.418] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9010\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 51/51 [00:24<00:00,  2.09it/s, loss=0.0698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1303\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 51/51 [00:23<00:00,  2.15it/s, loss=0.304] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3393\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 51/51 [00:24<00:00,  2.12it/s, loss=0.0472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1004\n",
      "Test Metrics: Precision=0.9339, Recall=0.9339, F1=0.9339\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 405, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 405/405 [00:00<00:00, 8864.21 examples/s]\n",
      "Map: 100%|██████████| 81/81 [00:00<00:00, 6776.34 examples/s]\n",
      "Map: 100%|██████████| 762/762 [00:00<00:00, 9232.31 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 51/51 [00:23<00:00,  2.15it/s, loss=0.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.4744\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 51/51 [00:23<00:00,  2.15it/s, loss=0.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5439\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 51/51 [00:23<00:00,  2.17it/s, loss=0.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6174\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 51/51 [00:23<00:00,  2.20it/s, loss=0.0457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8074\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 51/51 [00:23<00:00,  2.18it/s, loss=0.0385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.3358\n",
      "Test Metrics: Precision=0.9315, Recall=0.9315, F1=0.9315\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 405, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 405/405 [00:00<00:00, 8366.75 examples/s]\n",
      "Map: 100%|██████████| 81/81 [00:00<00:00, 7072.58 examples/s]\n",
      "Map: 100%|██████████| 762/762 [00:00<00:00, 9010.80 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 51/51 [00:24<00:00,  2.08it/s, loss=0.252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.1961\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 51/51 [00:24<00:00,  2.12it/s, loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9515\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 51/51 [00:24<00:00,  2.05it/s, loss=0.0738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1355\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 51/51 [00:24<00:00,  2.07it/s, loss=0.0868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1524\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 51/51 [00:24<00:00,  2.08it/s, loss=0.0201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.8814\n",
      "Test Metrics: Precision=0.9384, Recall=0.9384, F1=0.9384\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 410, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 410/410 [00:00<00:00, 8618.06 examples/s]\n",
      "Map: 100%|██████████| 82/82 [00:00<00:00, 7271.77 examples/s]\n",
      "Map: 100%|██████████| 756/756 [00:00<00:00, 9121.78 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:23<00:00,  2.24it/s, loss=0.825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.1672\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:23<00:00,  2.22it/s, loss=0.416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 19.3472\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:23<00:00,  2.19it/s, loss=0.221] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.8009\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:23<00:00,  2.24it/s, loss=0.116] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.7209\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 52/52 [00:23<00:00,  2.18it/s, loss=0.0143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 6.0231\n",
      "Test Metrics: Precision=0.9316, Recall=0.9316, F1=0.9316\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 410, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 410/410 [00:00<00:00, 8893.22 examples/s]\n",
      "Map: 100%|██████████| 82/82 [00:00<00:00, 8247.00 examples/s]\n",
      "Map: 100%|██████████| 756/756 [00:00<00:00, 9200.73 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:23<00:00,  2.23it/s, loss=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.7267\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:23<00:00,  2.22it/s, loss=0.314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.6393\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:23<00:00,  2.21it/s, loss=0.206] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5228\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:23<00:00,  2.19it/s, loss=0.103] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9266\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 52/52 [00:23<00:00,  2.19it/s, loss=0.0616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0819\n",
      "Test Metrics: Precision=0.9345, Recall=0.9345, F1=0.9345\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 410, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 410/410 [00:00<00:00, 8780.61 examples/s]\n",
      "Map: 100%|██████████| 82/82 [00:00<00:00, 7630.41 examples/s]\n",
      "Map: 100%|██████████| 756/756 [00:00<00:00, 3886.21 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:24<00:00,  2.12it/s, loss=0.208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.0241\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:23<00:00,  2.18it/s, loss=0.0912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.1612\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:24<00:00,  2.11it/s, loss=0.0426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6856\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:24<00:00,  2.11it/s, loss=0.173] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3658\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 52/52 [00:24<00:00,  2.14it/s, loss=0.291] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0527\n",
      "Test Metrics: Precision=0.9363, Recall=0.9363, F1=0.9363\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 410, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 410/410 [00:00<00:00, 8991.76 examples/s]\n",
      "Map: 100%|██████████| 82/82 [00:00<00:00, 8006.63 examples/s]\n",
      "Map: 100%|██████████| 756/756 [00:00<00:00, 9063.50 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:22<00:00,  2.35it/s, loss=0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.0339\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:23<00:00,  2.20it/s, loss=0.337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.2697\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:22<00:00,  2.27it/s, loss=0.172] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9503\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:23<00:00,  2.20it/s, loss=0.0979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2130\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 52/52 [00:24<00:00,  2.16it/s, loss=0.113] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.7363\n",
      "Test Metrics: Precision=0.9342, Recall=0.9342, F1=0.9342\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 410, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 410/410 [00:00<00:00, 8388.73 examples/s]\n",
      "Map: 100%|██████████| 82/82 [00:00<00:00, 8059.54 examples/s]\n",
      "Map: 100%|██████████| 756/756 [00:00<00:00, 9011.37 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:25<00:00,  2.07it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.7508\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:24<00:00,  2.10it/s, loss=0.0677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.2573\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:25<00:00,  2.06it/s, loss=0.236] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0832\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:25<00:00,  2.07it/s, loss=0.113] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7593\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 52/52 [00:24<00:00,  2.10it/s, loss=0.034] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.6824\n",
      "Test Metrics: Precision=0.9379, Recall=0.9379, F1=0.9379\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 415, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 415/415 [00:00<00:00, 8733.75 examples/s]\n",
      "Map: 100%|██████████| 83/83 [00:00<00:00, 7243.29 examples/s]\n",
      "Map: 100%|██████████| 750/750 [00:00<00:00, 8991.62 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:23<00:00,  2.22it/s, loss=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.6535\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:24<00:00,  2.16it/s, loss=0.15] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.5549\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:23<00:00,  2.20it/s, loss=0.101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0485\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:23<00:00,  2.22it/s, loss=0.0762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9497\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 52/52 [00:23<00:00,  2.25it/s, loss=0.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.3441\n",
      "Test Metrics: Precision=0.9356, Recall=0.9356, F1=0.9356\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 415, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 415/415 [00:00<00:00, 9173.46 examples/s]\n",
      "Map: 100%|██████████| 83/83 [00:00<00:00, 6824.55 examples/s]\n",
      "Map: 100%|██████████| 750/750 [00:00<00:00, 9261.08 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:23<00:00,  2.20it/s, loss=0.474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.4376\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:23<00:00,  2.24it/s, loss=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.5214\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:23<00:00,  2.24it/s, loss=0.152] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8704\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:23<00:00,  2.20it/s, loss=0.106] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0694\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 52/52 [00:24<00:00,  2.13it/s, loss=0.114] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.3936\n",
      "Test Metrics: Precision=0.9330, Recall=0.9330, F1=0.9330\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 415, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 415/415 [00:00<00:00, 8997.02 examples/s]\n",
      "Map: 100%|██████████| 83/83 [00:00<00:00, 6965.47 examples/s]\n",
      "Map: 100%|██████████| 750/750 [00:00<00:00, 9262.25 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:24<00:00,  2.13it/s, loss=0.478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.1585\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:24<00:00,  2.14it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8654\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:24<00:00,  2.16it/s, loss=0.129] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3449\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:24<00:00,  2.11it/s, loss=0.089] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.9608\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 52/52 [00:24<00:00,  2.13it/s, loss=0.0529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5936\n",
      "Test Metrics: Precision=0.9361, Recall=0.9361, F1=0.9361\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 415, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 415/415 [00:00<00:00, 8446.66 examples/s]\n",
      "Map: 100%|██████████| 83/83 [00:00<00:00, 7472.14 examples/s]\n",
      "Map: 100%|██████████| 750/750 [00:00<00:00, 9090.83 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:24<00:00,  2.15it/s, loss=0.252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.2212\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:23<00:00,  2.20it/s, loss=0.175] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.2722\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:24<00:00,  2.14it/s, loss=0.24]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.7337\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:23<00:00,  2.18it/s, loss=0.227] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1581\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 52/52 [00:24<00:00,  2.13it/s, loss=0.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.7700\n",
      "Test Metrics: Precision=0.9345, Recall=0.9345, F1=0.9345\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 415, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 415/415 [00:00<00:00, 8376.90 examples/s]\n",
      "Map: 100%|██████████| 83/83 [00:00<00:00, 7061.26 examples/s]\n",
      "Map: 100%|██████████| 750/750 [00:00<00:00, 9120.72 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 52/52 [00:25<00:00,  2.05it/s, loss=0.39] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.7929\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 52/52 [00:25<00:00,  2.00it/s, loss=0.315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.7683\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 52/52 [00:25<00:00,  2.06it/s, loss=0.111] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5039\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 52/52 [00:26<00:00,  1.97it/s, loss=0.087] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8075\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 52/52 [00:25<00:00,  2.07it/s, loss=0.0855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0852\n",
      "Test Metrics: Precision=0.9387, Recall=0.9387, F1=0.9387\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8837.87 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 8217.60 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9000.06 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:24<00:00,  2.20it/s, loss=0.751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.1140\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:23<00:00,  2.24it/s, loss=0.158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.1754\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:23<00:00,  2.23it/s, loss=0.104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6468\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 53/53 [00:23<00:00,  2.24it/s, loss=0.0363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.6823\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 53/53 [00:23<00:00,  2.21it/s, loss=0.0264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.2892\n",
      "Test Metrics: Precision=0.9349, Recall=0.9349, F1=0.9349\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 9064.24 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7562.82 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 8809.10 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:24<00:00,  2.17it/s, loss=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.8729\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:23<00:00,  2.23it/s, loss=0.149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.3012\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:23<00:00,  2.25it/s, loss=0.0711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8991\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 53/53 [00:23<00:00,  2.24it/s, loss=0.0769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7746\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 53/53 [00:24<00:00,  2.19it/s, loss=0.0762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.3502\n",
      "Test Metrics: Precision=0.9345, Recall=0.9345, F1=0.9345\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8901.23 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7158.24 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9391.53 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:24<00:00,  2.17it/s, loss=0.325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.5865\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:24<00:00,  2.16it/s, loss=0.318] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9715\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:25<00:00,  2.09it/s, loss=0.183] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6673\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 53/53 [00:24<00:00,  2.14it/s, loss=0.0901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1030\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 53/53 [00:24<00:00,  2.18it/s, loss=0.0543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5529\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8404.62 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7658.16 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9192.26 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:23<00:00,  2.22it/s, loss=0.487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.5541\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:24<00:00,  2.18it/s, loss=0.179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.0246\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:24<00:00,  2.18it/s, loss=0.0578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.7522\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 53/53 [00:24<00:00,  2.20it/s, loss=0.0536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1100\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 53/53 [00:24<00:00,  2.19it/s, loss=0.18]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.8149\n",
      "Test Metrics: Precision=0.9330, Recall=0.9330, F1=0.9330\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8592.62 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7483.62 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9145.52 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:26<00:00,  2.01it/s, loss=0.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.7253\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:26<00:00,  2.03it/s, loss=0.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.9643\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:26<00:00,  1.99it/s, loss=0.143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.6920\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 53/53 [00:25<00:00,  2.05it/s, loss=0.0536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3916\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 53/53 [00:26<00:00,  2.02it/s, loss=0.0917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.7204\n",
      "Test Metrics: Precision=0.9378, Recall=0.9378, F1=0.9378\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 425, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 425/425 [00:00<00:00, 8530.80 examples/s]\n",
      "Map: 100%|██████████| 85/85 [00:00<00:00, 8333.70 examples/s]\n",
      "Map: 100%|██████████| 738/738 [00:00<00:00, 8847.81 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:23<00:00,  2.32it/s, loss=0.0292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.8062\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:24<00:00,  2.20it/s, loss=0.0499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1062\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:23<00:00,  2.29it/s, loss=0.109] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2910\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:24<00:00,  2.24it/s, loss=0.148] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.0683\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 54/54 [00:23<00:00,  2.29it/s, loss=0.0894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.6484\n",
      "Test Metrics: Precision=0.9353, Recall=0.9353, F1=0.9353\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 425, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 425/425 [00:00<00:00, 8869.26 examples/s]\n",
      "Map: 100%|██████████| 85/85 [00:00<00:00, 6060.82 examples/s]\n",
      "Map: 100%|██████████| 738/738 [00:00<00:00, 9045.26 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:24<00:00,  2.22it/s, loss=0.832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.6218\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:24<00:00,  2.25it/s, loss=0.0139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.9695\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:24<00:00,  2.22it/s, loss=0.0932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0684\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:24<00:00,  2.21it/s, loss=0.0582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1829\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 54/54 [00:23<00:00,  2.28it/s, loss=0.172] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.5432\n",
      "Test Metrics: Precision=0.9347, Recall=0.9347, F1=0.9347\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 425, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 425/425 [00:00<00:00, 8440.56 examples/s]\n",
      "Map: 100%|██████████| 85/85 [00:00<00:00, 6692.62 examples/s]\n",
      "Map: 100%|██████████| 738/738 [00:00<00:00, 9511.42 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:25<00:00,  2.14it/s, loss=0.566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.9077\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:25<00:00,  2.14it/s, loss=0.0095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.7226\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:24<00:00,  2.18it/s, loss=0.0787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.4620\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:25<00:00,  2.08it/s, loss=0.128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.7610\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 54/54 [00:24<00:00,  2.19it/s, loss=0.0339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.0173\n",
      "Test Metrics: Precision=0.9350, Recall=0.9350, F1=0.9350\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 425, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 425/425 [00:00<00:00, 8718.26 examples/s]\n",
      "Map: 100%|██████████| 85/85 [00:00<00:00, 8275.09 examples/s]\n",
      "Map: 100%|██████████| 738/738 [00:00<00:00, 8928.07 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:23<00:00,  2.26it/s, loss=0.0685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.6970\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:24<00:00,  2.19it/s, loss=0.0419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.7922\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:24<00:00,  2.22it/s, loss=0.169] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.1588\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:24<00:00,  2.19it/s, loss=0.00966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.1623\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 54/54 [00:23<00:00,  2.26it/s, loss=0.223] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 6.0646\n",
      "Test Metrics: Precision=0.9326, Recall=0.9326, F1=0.9326\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 425, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 425/425 [00:00<00:00, 8137.70 examples/s]\n",
      "Map: 100%|██████████| 85/85 [00:00<00:00, 7402.43 examples/s]\n",
      "Map: 100%|██████████| 738/738 [00:00<00:00, 9284.69 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:26<00:00,  2.04it/s, loss=0.129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.5956\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:26<00:00,  2.04it/s, loss=0.478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.4693\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:26<00:00,  2.06it/s, loss=0.0187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9860\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:25<00:00,  2.10it/s, loss=0.134] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7117\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 54/54 [00:26<00:00,  2.04it/s, loss=0.186] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.9698\n",
      "Test Metrics: Precision=0.9385, Recall=0.9385, F1=0.9385\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 430, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 430/430 [00:00<00:00, 8721.11 examples/s]\n",
      "Map: 100%|██████████| 86/86 [00:00<00:00, 7324.06 examples/s]\n",
      "Map: 100%|██████████| 732/732 [00:00<00:00, 9347.35 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:24<00:00,  2.22it/s, loss=0.455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.8584\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:24<00:00,  2.22it/s, loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.9841\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:24<00:00,  2.20it/s, loss=0.152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.9491\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:23<00:00,  2.28it/s, loss=0.115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.2684\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 54/54 [00:24<00:00,  2.22it/s, loss=0.12]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.6201\n",
      "Test Metrics: Precision=0.9300, Recall=0.9300, F1=0.9300\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 430, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 430/430 [00:00<00:00, 8839.55 examples/s]\n",
      "Map: 100%|██████████| 86/86 [00:00<00:00, 6542.66 examples/s]\n",
      "Map: 100%|██████████| 732/732 [00:00<00:00, 8998.88 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:24<00:00,  2.18it/s, loss=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 34.5714\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:24<00:00,  2.20it/s, loss=0.155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 19.8217\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:24<00:00,  2.18it/s, loss=0.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.6181\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:24<00:00,  2.22it/s, loss=0.132] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 9.7902\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 54/54 [00:24<00:00,  2.18it/s, loss=0.168] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 8.2625\n",
      "Test Metrics: Precision=0.9263, Recall=0.9263, F1=0.9263\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 430, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 430/430 [00:00<00:00, 8559.03 examples/s]\n",
      "Map: 100%|██████████| 86/86 [00:00<00:00, 7400.40 examples/s]\n",
      "Map: 100%|██████████| 732/732 [00:00<00:00, 9384.95 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:25<00:00,  2.08it/s, loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.6293\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:25<00:00,  2.13it/s, loss=0.0991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.0826\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:26<00:00,  2.05it/s, loss=0.103] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4008\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:25<00:00,  2.11it/s, loss=0.0655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5753\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 54/54 [00:25<00:00,  2.10it/s, loss=0.0848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.8517\n",
      "Test Metrics: Precision=0.9363, Recall=0.9363, F1=0.9363\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 430, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 430/430 [00:00<00:00, 8614.30 examples/s]\n",
      "Map: 100%|██████████| 86/86 [00:00<00:00, 6952.51 examples/s]\n",
      "Map: 100%|██████████| 732/732 [00:00<00:00, 9290.50 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:24<00:00,  2.18it/s, loss=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.0298\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:25<00:00,  2.15it/s, loss=0.384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.7275\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:25<00:00,  2.10it/s, loss=0.163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.8065\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:25<00:00,  2.11it/s, loss=0.195] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.9134\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 54/54 [00:25<00:00,  2.12it/s, loss=0.0575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 6.0781\n",
      "Test Metrics: Precision=0.9293, Recall=0.9293, F1=0.9293\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 430, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 430/430 [00:00<00:00, 8171.18 examples/s]\n",
      "Map: 100%|██████████| 86/86 [00:00<00:00, 6859.95 examples/s]\n",
      "Map: 100%|██████████| 732/732 [00:00<00:00, 9284.85 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 54/54 [00:26<00:00,  2.05it/s, loss=0.345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.8585\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 54/54 [00:26<00:00,  2.02it/s, loss=0.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.6319\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 54/54 [00:26<00:00,  2.03it/s, loss=0.108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3083\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 54/54 [00:26<00:00,  2.01it/s, loss=0.296] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6902\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 54/54 [00:26<00:00,  2.07it/s, loss=0.0887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.8727\n",
      "Test Metrics: Precision=0.9395, Recall=0.9395, F1=0.9395\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 435, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 435/435 [00:00<00:00, 9053.44 examples/s]\n",
      "Map: 100%|██████████| 87/87 [00:00<00:00, 7765.58 examples/s]\n",
      "Map: 100%|██████████| 726/726 [00:00<00:00, 9156.00 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:23<00:00,  2.30it/s, loss=0.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.4834\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:24<00:00,  2.25it/s, loss=0.283] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.0094\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:24<00:00,  2.23it/s, loss=0.935] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1509\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:23<00:00,  2.31it/s, loss=0.0134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.4275\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 55/55 [00:24<00:00,  2.26it/s, loss=0.111] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.4906\n",
      "Test Metrics: Precision=0.9351, Recall=0.9351, F1=0.9351\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 435, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 435/435 [00:00<00:00, 8447.57 examples/s]\n",
      "Map: 100%|██████████| 87/87 [00:00<00:00, 6842.51 examples/s]\n",
      "Map: 100%|██████████| 726/726 [00:00<00:00, 8880.92 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:24<00:00,  2.23it/s, loss=0.175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.4332\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:25<00:00,  2.16it/s, loss=0.161] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.7233\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:24<00:00,  2.20it/s, loss=0.1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4046\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:24<00:00,  2.26it/s, loss=0.0827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9047\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 55/55 [00:25<00:00,  2.17it/s, loss=0.0969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.9634\n",
      "Test Metrics: Precision=0.9342, Recall=0.9342, F1=0.9342\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 435, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 435/435 [00:00<00:00, 8536.70 examples/s]\n",
      "Map: 100%|██████████| 87/87 [00:00<00:00, 8027.99 examples/s]\n",
      "Map: 100%|██████████| 726/726 [00:00<00:00, 9285.49 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:26<00:00,  2.08it/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.8601\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:26<00:00,  2.11it/s, loss=0.405] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.8119\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:26<00:00,  2.11it/s, loss=0.125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0315\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:26<00:00,  2.11it/s, loss=0.0791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.4334\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 55/55 [00:26<00:00,  2.11it/s, loss=0.074] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1274\n",
      "Test Metrics: Precision=0.9356, Recall=0.9356, F1=0.9356\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 435, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 435/435 [00:00<00:00, 8486.39 examples/s]\n",
      "Map: 100%|██████████| 87/87 [00:00<00:00, 6955.20 examples/s]\n",
      "Map: 100%|██████████| 726/726 [00:00<00:00, 9416.92 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:25<00:00,  2.13it/s, loss=0.436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.5873\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:25<00:00,  2.14it/s, loss=0.095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2824\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:25<00:00,  2.13it/s, loss=0.103] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4510\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:25<00:00,  2.13it/s, loss=0.0394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.3712\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 55/55 [00:25<00:00,  2.17it/s, loss=0.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.7742\n",
      "Test Metrics: Precision=0.9350, Recall=0.9350, F1=0.9350\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 435, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 435/435 [00:00<00:00, 8627.07 examples/s]\n",
      "Map: 100%|██████████| 87/87 [00:00<00:00, 6922.34 examples/s]\n",
      "Map: 100%|██████████| 726/726 [00:00<00:00, 9273.87 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:26<00:00,  2.04it/s, loss=0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.6768\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:27<00:00,  2.03it/s, loss=0.098] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.4555\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:26<00:00,  2.04it/s, loss=0.139] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3942\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:26<00:00,  2.08it/s, loss=0.104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7371\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 55/55 [00:26<00:00,  2.08it/s, loss=0.0333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.9539\n",
      "Test Metrics: Precision=0.9387, Recall=0.9387, F1=0.9387\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8479.30 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7816.91 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 8928.56 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:24<00:00,  2.22it/s, loss=0.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.0255\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:24<00:00,  2.23it/s, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5428\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:24<00:00,  2.21it/s, loss=0.167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6912\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:24<00:00,  2.25it/s, loss=0.103] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7790\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 55/55 [00:24<00:00,  2.24it/s, loss=0.0784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.3551\n",
      "Test Metrics: Precision=0.9367, Recall=0.9367, F1=0.9367\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 7995.73 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7512.39 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9184.80 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:25<00:00,  2.17it/s, loss=0.175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.6119\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:25<00:00,  2.12it/s, loss=0.152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.6113\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:24<00:00,  2.21it/s, loss=0.085] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8574\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:25<00:00,  2.20it/s, loss=0.0819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2806\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 55/55 [00:24<00:00,  2.24it/s, loss=0.0957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5712\n",
      "Test Metrics: Precision=0.9351, Recall=0.9351, F1=0.9351\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8465.88 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7169.75 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9421.40 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:26<00:00,  2.09it/s, loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.6436\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:26<00:00,  2.05it/s, loss=0.152] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.0749\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:26<00:00,  2.10it/s, loss=0.0827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.2149\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:26<00:00,  2.11it/s, loss=0.0776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7216\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 55/55 [00:26<00:00,  2.07it/s, loss=0.0796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.9423\n",
      "Test Metrics: Precision=0.9351, Recall=0.9351, F1=0.9351\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8633.20 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7024.70 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9236.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:26<00:00,  2.12it/s, loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.4343\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:26<00:00,  2.07it/s, loss=0.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.1624\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:26<00:00,  2.11it/s, loss=0.125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3385\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:26<00:00,  2.10it/s, loss=0.135] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3295\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 55/55 [00:26<00:00,  2.09it/s, loss=0.233] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.6199\n",
      "Test Metrics: Precision=0.9333, Recall=0.9333, F1=0.9333\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8142.09 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7768.54 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9076.97 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:26<00:00,  2.07it/s, loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.1294\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:27<00:00,  2.03it/s, loss=0.28]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.8693\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:27<00:00,  2.01it/s, loss=0.108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.2412\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 55/55 [00:26<00:00,  2.07it/s, loss=0.156] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1107\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 55/55 [00:26<00:00,  2.07it/s, loss=0.0765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.3581\n",
      "Test Metrics: Precision=0.9387, Recall=0.9387, F1=0.9387\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 445, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 445/445 [00:00<00:00, 8474.42 examples/s]\n",
      "Map: 100%|██████████| 89/89 [00:00<00:00, 5489.20 examples/s]\n",
      "Map: 100%|██████████| 714/714 [00:00<00:00, 9370.84 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 56/56 [00:25<00:00,  2.22it/s, loss=0.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.5848\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 56/56 [00:25<00:00,  2.22it/s, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2944\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 56/56 [00:24<00:00,  2.26it/s, loss=0.18]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.1338\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 56/56 [00:24<00:00,  2.27it/s, loss=0.0825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9792\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 56/56 [00:24<00:00,  2.25it/s, loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.4624\n",
      "Test Metrics: Precision=0.9343, Recall=0.9343, F1=0.9343\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 445, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 445/445 [00:00<00:00, 8672.20 examples/s]\n",
      "Map: 100%|██████████| 89/89 [00:00<00:00, 6942.27 examples/s]\n",
      "Map: 100%|██████████| 714/714 [00:00<00:00, 9263.67 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 56/56 [00:25<00:00,  2.15it/s, loss=0.193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.3554\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 56/56 [00:25<00:00,  2.18it/s, loss=0.132] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.3285\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 56/56 [00:25<00:00,  2.22it/s, loss=0.13]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7692\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 56/56 [00:25<00:00,  2.20it/s, loss=0.117] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9121\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 56/56 [00:26<00:00,  2.12it/s, loss=0.0336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.4280\n",
      "Test Metrics: Precision=0.9350, Recall=0.9350, F1=0.9350\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 445, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 445/445 [00:00<00:00, 8666.76 examples/s]\n",
      "Map: 100%|██████████| 89/89 [00:00<00:00, 6583.30 examples/s]\n",
      "Map: 100%|██████████| 714/714 [00:00<00:00, 9439.84 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 56/56 [00:26<00:00,  2.11it/s, loss=0.244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.0995\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 56/56 [00:25<00:00,  2.16it/s, loss=0.159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2975\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 56/56 [00:25<00:00,  2.18it/s, loss=0.114] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9266\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 56/56 [00:26<00:00,  2.10it/s, loss=0.0954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1883\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 56/56 [00:25<00:00,  2.16it/s, loss=0.0251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1424\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 445, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 445/445 [00:00<00:00, 8708.41 examples/s]\n",
      "Map: 100%|██████████| 89/89 [00:00<00:00, 7971.92 examples/s]\n",
      "Map: 100%|██████████| 714/714 [00:00<00:00, 9299.40 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 56/56 [00:26<00:00,  2.13it/s, loss=0.16] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.8164\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 56/56 [00:25<00:00,  2.16it/s, loss=0.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.5082\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 56/56 [00:27<00:00,  2.07it/s, loss=0.171] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.6571\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 56/56 [00:26<00:00,  2.12it/s, loss=0.175] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 9.3682\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 56/56 [00:26<00:00,  2.09it/s, loss=0.0958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 7.6527\n",
      "Test Metrics: Precision=0.9303, Recall=0.9303, F1=0.9303\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 445, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 445/445 [00:00<00:00, 8705.41 examples/s]\n",
      "Map: 100%|██████████| 89/89 [00:00<00:00, 7879.04 examples/s]\n",
      "Map: 100%|██████████| 714/714 [00:00<00:00, 9418.61 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 56/56 [00:27<00:00,  2.00it/s, loss=0.372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.4972\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 56/56 [00:27<00:00,  2.06it/s, loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.3599\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 56/56 [00:27<00:00,  2.04it/s, loss=0.0989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.5286\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 56/56 [00:27<00:00,  2.00it/s, loss=0.0257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.5859\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 56/56 [00:28<00:00,  1.97it/s, loss=0.0572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.2310\n",
      "Test Metrics: Precision=0.9385, Recall=0.9385, F1=0.9385\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 450, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 450/450 [00:00<00:00, 9062.10 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7459.93 examples/s]\n",
      "Map: 100%|██████████| 708/708 [00:00<00:00, 9058.81 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:25<00:00,  2.21it/s, loss=0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.4407\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:24<00:00,  2.30it/s, loss=0.0215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.0992\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:25<00:00,  2.28it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.5716\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:25<00:00,  2.28it/s, loss=0.0642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.0052\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 57/57 [00:25<00:00,  2.27it/s, loss=0.319] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.4530\n",
      "Test Metrics: Precision=0.9343, Recall=0.9343, F1=0.9343\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 450, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 450/450 [00:00<00:00, 8374.98 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7068.26 examples/s]\n",
      "Map: 100%|██████████| 708/708 [00:00<00:00, 9098.02 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:26<00:00,  2.18it/s, loss=0.142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.5915\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:25<00:00,  2.22it/s, loss=0.0991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.8054\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:26<00:00,  2.15it/s, loss=0.121] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.2793\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:25<00:00,  2.20it/s, loss=0.00927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.4008\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 57/57 [00:26<00:00,  2.17it/s, loss=0.0478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.9362\n",
      "Test Metrics: Precision=0.9362, Recall=0.9362, F1=0.9362\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 450, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 450/450 [00:00<00:00, 2629.32 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 8254.34 examples/s]\n",
      "Map: 100%|██████████| 708/708 [00:00<00:00, 9043.74 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:26<00:00,  2.16it/s, loss=0.148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.4965\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:27<00:00,  2.11it/s, loss=0.223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.3899\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:26<00:00,  2.15it/s, loss=0.133] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.0197\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:26<00:00,  2.13it/s, loss=0.0869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.8359\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 57/57 [00:26<00:00,  2.16it/s, loss=0.0557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 6.1388\n",
      "Test Metrics: Precision=0.9349, Recall=0.9349, F1=0.9349\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 450, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 450/450 [00:00<00:00, 8725.25 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7226.30 examples/s]\n",
      "Map: 100%|██████████| 708/708 [00:00<00:00, 9239.73 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:26<00:00,  2.15it/s, loss=0.325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.5445\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:27<00:00,  2.08it/s, loss=0.223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.3415\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:26<00:00,  2.14it/s, loss=0.188] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0892\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:26<00:00,  2.18it/s, loss=0.0462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.5915\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 57/57 [00:26<00:00,  2.15it/s, loss=0.0117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.4182\n",
      "Test Metrics: Precision=0.9346, Recall=0.9346, F1=0.9346\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 450, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 450/450 [00:00<00:00, 8503.31 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7255.60 examples/s]\n",
      "Map: 100%|██████████| 708/708 [00:00<00:00, 9372.78 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:27<00:00,  2.06it/s, loss=0.0853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.9499\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:28<00:00,  2.04it/s, loss=0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.2978\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:27<00:00,  2.05it/s, loss=0.0925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0617\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:27<00:00,  2.11it/s, loss=0.0622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9460\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 57/57 [00:27<00:00,  2.06it/s, loss=0.0531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.9780\n",
      "Test Metrics: Precision=0.9398, Recall=0.9398, F1=0.9398\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 455, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 455/455 [00:00<00:00, 8942.74 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:00<00:00, 7519.49 examples/s]\n",
      "Map: 100%|██████████| 702/702 [00:00<00:00, 8935.48 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:25<00:00,  2.20it/s, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.1684\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:25<00:00,  2.21it/s, loss=0.308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.2015\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:25<00:00,  2.24it/s, loss=0.185] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3231\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:25<00:00,  2.23it/s, loss=0.0431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5757\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 57/57 [00:25<00:00,  2.24it/s, loss=0.0583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.9006\n",
      "Test Metrics: Precision=0.9352, Recall=0.9352, F1=0.9352\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 455, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 455/455 [00:00<00:00, 8740.69 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:00<00:00, 7682.34 examples/s]\n",
      "Map: 100%|██████████| 702/702 [00:00<00:00, 9346.80 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:26<00:00,  2.16it/s, loss=0.491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.6289\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:25<00:00,  2.21it/s, loss=0.314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.3061\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:24<00:00,  2.29it/s, loss=0.183] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.8620\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:25<00:00,  2.22it/s, loss=0.0865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.7043\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 57/57 [00:26<00:00,  2.19it/s, loss=0.175] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.7668\n",
      "Test Metrics: Precision=0.9368, Recall=0.9368, F1=0.9368\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 455, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 455/455 [00:00<00:00, 8450.75 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:00<00:00, 7711.83 examples/s]\n",
      "Map: 100%|██████████| 702/702 [00:00<00:00, 9172.02 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:26<00:00,  2.13it/s, loss=0.388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.8703\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:27<00:00,  2.08it/s, loss=0.146] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.6772\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:26<00:00,  2.17it/s, loss=0.217] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8462\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:26<00:00,  2.12it/s, loss=0.0615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9810\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 57/57 [00:26<00:00,  2.11it/s, loss=0.0565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.2795\n",
      "Test Metrics: Precision=0.9364, Recall=0.9364, F1=0.9364\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 455, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 455/455 [00:00<00:00, 8905.94 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:00<00:00, 7861.13 examples/s]\n",
      "Map: 100%|██████████| 702/702 [00:00<00:00, 9450.21 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:26<00:00,  2.16it/s, loss=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.7507\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:26<00:00,  2.17it/s, loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.3110\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:26<00:00,  2.13it/s, loss=0.0878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6019\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:27<00:00,  2.11it/s, loss=0.32]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9815\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 57/57 [00:26<00:00,  2.14it/s, loss=0.129] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.5887\n",
      "Test Metrics: Precision=0.9353, Recall=0.9353, F1=0.9353\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 455, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 455/455 [00:00<00:00, 8070.88 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:00<00:00, 8803.84 examples/s]\n",
      "Map: 100%|██████████| 702/702 [00:00<00:00, 9131.57 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 57/57 [00:27<00:00,  2.06it/s, loss=0.36] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.9625\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 57/57 [00:27<00:00,  2.04it/s, loss=0.221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.9416\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 57/57 [00:28<00:00,  2.03it/s, loss=0.175] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.1215\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 57/57 [00:28<00:00,  2.01it/s, loss=0.0651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.6251\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 57/57 [00:28<00:00,  2.02it/s, loss=0.0665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.6116\n",
      "Test Metrics: Precision=0.9386, Recall=0.9386, F1=0.9386\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8825.02 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7137.26 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9163.79 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:25<00:00,  2.26it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.8572\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:25<00:00,  2.24it/s, loss=0.193] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.7531\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:25<00:00,  2.31it/s, loss=0.177] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.6829\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 58/58 [00:25<00:00,  2.27it/s, loss=0.0828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.2441\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 58/58 [00:25<00:00,  2.29it/s, loss=0.132] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.7101\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8070.90 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7379.25 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9453.73 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:26<00:00,  2.20it/s, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.4293\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:27<00:00,  2.14it/s, loss=0.135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.8665\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:26<00:00,  2.20it/s, loss=0.0502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.1390\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 58/58 [00:26<00:00,  2.20it/s, loss=0.127] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7372\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 58/58 [00:26<00:00,  2.23it/s, loss=0.0798]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.2682\n",
      "Test Metrics: Precision=0.9351, Recall=0.9351, F1=0.9351\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8587.50 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7780.23 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9417.38 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:28<00:00,  2.06it/s, loss=0.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.7179\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:27<00:00,  2.07it/s, loss=0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.0673\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:26<00:00,  2.15it/s, loss=0.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3812\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 58/58 [00:27<00:00,  2.14it/s, loss=0.0566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1755\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 58/58 [00:27<00:00,  2.09it/s, loss=0.069] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0554\n",
      "Test Metrics: Precision=0.9349, Recall=0.9349, F1=0.9349\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8655.15 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7856.74 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9169.52 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:26<00:00,  2.17it/s, loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.1747\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:26<00:00,  2.21it/s, loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.9137\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:26<00:00,  2.18it/s, loss=0.0514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2763\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 58/58 [00:26<00:00,  2.22it/s, loss=0.0907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.4381\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 58/58 [00:26<00:00,  2.15it/s, loss=0.0392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.0399\n",
      "Test Metrics: Precision=0.9335, Recall=0.9335, F1=0.9335\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8267.08 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 6452.45 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9346.16 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:27<00:00,  2.11it/s, loss=0.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.7816\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:28<00:00,  2.04it/s, loss=0.286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.1864\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:28<00:00,  2.03it/s, loss=0.184] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.2595\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 58/58 [00:27<00:00,  2.11it/s, loss=0.0748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.0926\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 58/58 [00:28<00:00,  2.07it/s, loss=0.0553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.5873\n",
      "Test Metrics: Precision=0.9387, Recall=0.9387, F1=0.9387\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 465, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 465/465 [00:00<00:00, 8071.41 examples/s]\n",
      "Map: 100%|██████████| 93/93 [00:00<00:00, 7679.76 examples/s]\n",
      "Map: 100%|██████████| 690/690 [00:00<00:00, 8395.28 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:25<00:00,  2.28it/s, loss=0.387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.1732\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:25<00:00,  2.34it/s, loss=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.3858\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:25<00:00,  2.30it/s, loss=0.125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.7810\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:26<00:00,  2.27it/s, loss=0.166] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.7742\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 59/59 [00:25<00:00,  2.28it/s, loss=0.0394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.7902\n",
      "Test Metrics: Precision=0.9362, Recall=0.9362, F1=0.9362\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 465, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 465/465 [00:00<00:00, 8841.44 examples/s]\n",
      "Map: 100%|██████████| 93/93 [00:00<00:00, 7884.67 examples/s]\n",
      "Map: 100%|██████████| 690/690 [00:00<00:00, 9217.55 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:27<00:00,  2.18it/s, loss=0.468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.8551\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:26<00:00,  2.22it/s, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.1902\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:27<00:00,  2.18it/s, loss=0.367] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.3261\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:26<00:00,  2.20it/s, loss=0.0337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 9.2938\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 59/59 [00:26<00:00,  2.20it/s, loss=0.134] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 6.9262\n",
      "Test Metrics: Precision=0.9279, Recall=0.9279, F1=0.9279\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 465, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 465/465 [00:00<00:00, 7499.71 examples/s]\n",
      "Map: 100%|██████████| 93/93 [00:00<00:00, 7898.72 examples/s]\n",
      "Map: 100%|██████████| 690/690 [00:00<00:00, 9333.75 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:27<00:00,  2.13it/s, loss=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.4995\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:27<00:00,  2.13it/s, loss=0.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.0785\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:27<00:00,  2.11it/s, loss=0.217] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.7006\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:28<00:00,  2.10it/s, loss=1.63]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 8.5271\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 59/59 [00:28<00:00,  2.09it/s, loss=0.838] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 6.6455\n",
      "Test Metrics: Precision=0.9345, Recall=0.9345, F1=0.9345\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 465, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 465/465 [00:00<00:00, 8240.25 examples/s]\n",
      "Map: 100%|██████████| 93/93 [00:00<00:00, 8248.82 examples/s]\n",
      "Map: 100%|██████████| 690/690 [00:00<00:00, 8988.88 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:28<00:00,  2.09it/s, loss=0.0867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.8197\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:27<00:00,  2.14it/s, loss=0.346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.0106\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:26<00:00,  2.25it/s, loss=0.00431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.7205\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:26<00:00,  2.22it/s, loss=0.0484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.9005\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 59/59 [00:27<00:00,  2.15it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.8909\n",
      "Test Metrics: Precision=0.9344, Recall=0.9344, F1=0.9344\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 465, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 465/465 [00:00<00:00, 8601.33 examples/s]\n",
      "Map: 100%|██████████| 93/93 [00:00<00:00, 7360.37 examples/s]\n",
      "Map: 100%|██████████| 690/690 [00:00<00:00, 9393.04 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:28<00:00,  2.05it/s, loss=0.347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.0124\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:28<00:00,  2.09it/s, loss=0.218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.0655\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:28<00:00,  2.06it/s, loss=0.246] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.1656\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:28<00:00,  2.08it/s, loss=0.0084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.9750\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 59/59 [00:28<00:00,  2.10it/s, loss=0.000976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.0099\n",
      "Test Metrics: Precision=0.9403, Recall=0.9403, F1=0.9403\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 470, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 470/470 [00:00<00:00, 8390.00 examples/s]\n",
      "Map: 100%|██████████| 94/94 [00:00<00:00, 7163.24 examples/s]\n",
      "Map: 100%|██████████| 684/684 [00:00<00:00, 9068.39 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:26<00:00,  2.27it/s, loss=0.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.9521\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:26<00:00,  2.20it/s, loss=0.167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.7327\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:25<00:00,  2.27it/s, loss=0.074] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.4221\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:27<00:00,  2.18it/s, loss=0.118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.6819\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 59/59 [00:25<00:00,  2.28it/s, loss=0.0615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.7096\n",
      "Test Metrics: Precision=0.9357, Recall=0.9357, F1=0.9357\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 470, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 470/470 [00:00<00:00, 8664.20 examples/s]\n",
      "Map: 100%|██████████| 94/94 [00:00<00:00, 7165.97 examples/s]\n",
      "Map: 100%|██████████| 684/684 [00:00<00:00, 9157.75 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:26<00:00,  2.21it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.7055\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:27<00:00,  2.17it/s, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.4337\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:27<00:00,  2.14it/s, loss=0.143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5569\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:27<00:00,  2.16it/s, loss=0.17]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.6399\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 59/59 [00:27<00:00,  2.17it/s, loss=0.0874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.7124\n",
      "Test Metrics: Precision=0.9371, Recall=0.9371, F1=0.9371\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 470, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 470/470 [00:00<00:00, 8490.38 examples/s]\n",
      "Map: 100%|██████████| 94/94 [00:00<00:00, 8393.25 examples/s]\n",
      "Map: 100%|██████████| 684/684 [00:00<00:00, 9608.46 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:28<00:00,  2.04it/s, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.1664\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:27<00:00,  2.13it/s, loss=0.147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.2803\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:28<00:00,  2.10it/s, loss=0.139] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.7497\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:28<00:00,  2.08it/s, loss=0.133] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.5910\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 59/59 [00:28<00:00,  2.06it/s, loss=0.0502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.5343\n",
      "Test Metrics: Precision=0.9362, Recall=0.9362, F1=0.9362\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 470, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 470/470 [00:00<00:00, 8675.64 examples/s]\n",
      "Map: 100%|██████████| 94/94 [00:00<00:00, 7639.01 examples/s]\n",
      "Map: 100%|██████████| 684/684 [00:00<00:00, 9158.07 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:26<00:00,  2.21it/s, loss=0.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.9189\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:27<00:00,  2.15it/s, loss=0.168] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.9667\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:27<00:00,  2.17it/s, loss=0.204] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9925\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:27<00:00,  2.18it/s, loss=0.0419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3472\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 59/59 [00:27<00:00,  2.13it/s, loss=0.088] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.4534\n",
      "Test Metrics: Precision=0.9326, Recall=0.9326, F1=0.9326\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 470, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 470/470 [00:00<00:00, 8687.65 examples/s]\n",
      "Map: 100%|██████████| 94/94 [00:00<00:00, 7544.58 examples/s]\n",
      "Map: 100%|██████████| 684/684 [00:00<00:00, 9422.24 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 59/59 [00:28<00:00,  2.06it/s, loss=0.569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 38.1551\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 59/59 [00:28<00:00,  2.07it/s, loss=0.498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 31.2048\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 59/59 [00:28<00:00,  2.06it/s, loss=0.215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 18.2669\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 59/59 [00:28<00:00,  2.04it/s, loss=0.279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 14.5665\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 59/59 [00:28<00:00,  2.08it/s, loss=0.146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 13.8814\n",
      "Test Metrics: Precision=0.9221, Recall=0.9221, F1=0.9221\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 475, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 475/475 [00:00<00:00, 8496.54 examples/s]\n",
      "Map: 100%|██████████| 95/95 [00:00<00:00, 6775.82 examples/s]\n",
      "Map: 100%|██████████| 678/678 [00:00<00:00, 9182.23 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:25<00:00,  2.33it/s, loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.8645\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:26<00:00,  2.28it/s, loss=0.467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1963\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:25<00:00,  2.31it/s, loss=0.112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1700\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:26<00:00,  2.25it/s, loss=0.192] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.9788\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 60/60 [00:26<00:00,  2.29it/s, loss=0.0606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.0868\n",
      "Test Metrics: Precision=0.9350, Recall=0.9350, F1=0.9350\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 475, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 475/475 [00:00<00:00, 8344.09 examples/s]\n",
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7292.44 examples/s]\n",
      "Map: 100%|██████████| 678/678 [00:00<00:00, 9090.07 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:26<00:00,  2.25it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.4364\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:27<00:00,  2.19it/s, loss=0.172] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.6246\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:27<00:00,  2.21it/s, loss=0.0548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.7586\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:27<00:00,  2.16it/s, loss=0.189] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.2981\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 60/60 [00:27<00:00,  2.22it/s, loss=0.0217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.9607\n",
      "Test Metrics: Precision=0.9355, Recall=0.9355, F1=0.9355\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 475, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 475/475 [00:00<00:00, 7987.32 examples/s]\n",
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7503.37 examples/s]\n",
      "Map: 100%|██████████| 678/678 [00:00<00:00, 9347.39 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:28<00:00,  2.13it/s, loss=0.28]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.1476\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:28<00:00,  2.11it/s, loss=0.218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.5292\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:27<00:00,  2.16it/s, loss=0.112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.4993\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:28<00:00,  2.08it/s, loss=0.14]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3553\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 60/60 [00:28<00:00,  2.12it/s, loss=0.0368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.7834\n",
      "Test Metrics: Precision=0.9356, Recall=0.9356, F1=0.9356\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 475, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 475/475 [00:00<00:00, 8349.34 examples/s]\n",
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7282.18 examples/s]\n",
      "Map: 100%|██████████| 678/678 [00:00<00:00, 9595.29 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:27<00:00,  2.16it/s, loss=0.198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.9464\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:27<00:00,  2.22it/s, loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.7187\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:27<00:00,  2.18it/s, loss=0.126] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7459\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:27<00:00,  2.17it/s, loss=0.0994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.5659\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 60/60 [00:27<00:00,  2.15it/s, loss=0.263] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.7746\n",
      "Test Metrics: Precision=0.9357, Recall=0.9357, F1=0.9357\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 475, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 475/475 [00:00<00:00, 8366.34 examples/s]\n",
      "Map: 100%|██████████| 95/95 [00:00<00:00, 6959.37 examples/s]\n",
      "Map: 100%|██████████| 678/678 [00:00<00:00, 9564.25 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:28<00:00,  2.08it/s, loss=0.465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.0532\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:29<00:00,  2.05it/s, loss=0.122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.0075\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:29<00:00,  2.05it/s, loss=0.18]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.7118\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:28<00:00,  2.11it/s, loss=0.168] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.2247\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 60/60 [00:29<00:00,  2.03it/s, loss=0.059] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.2720\n",
      "Test Metrics: Precision=0.9377, Recall=0.9377, F1=0.9377\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8642.85 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7109.12 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9098.54 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:27<00:00,  2.22it/s, loss=0.52] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.3258\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:26<00:00,  2.30it/s, loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 21.6414\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:27<00:00,  2.18it/s, loss=0.21]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.1716\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:27<00:00,  2.21it/s, loss=0.0843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.7994\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 60/60 [00:27<00:00,  2.19it/s, loss=0.0831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.5328\n",
      "Test Metrics: Precision=0.9325, Recall=0.9325, F1=0.9325\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 7839.09 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7330.56 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 8943.25 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:27<00:00,  2.17it/s, loss=0.34] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.0926\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:26<00:00,  2.23it/s, loss=0.172] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2894\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:28<00:00,  2.14it/s, loss=0.0889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1377\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:28<00:00,  2.12it/s, loss=0.0779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.1205\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 60/60 [00:27<00:00,  2.17it/s, loss=0.0606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.2642\n",
      "Test Metrics: Precision=0.9388, Recall=0.9388, F1=0.9388\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8568.29 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7115.90 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9539.45 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:28<00:00,  2.10it/s, loss=0.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.8472\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:30<00:00,  1.99it/s, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1621\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:29<00:00,  2.03it/s, loss=0.136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6613\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:28<00:00,  2.07it/s, loss=0.0728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.8032\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 60/60 [00:28<00:00,  2.12it/s, loss=0.0529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.2608\n",
      "Test Metrics: Precision=0.9382, Recall=0.9382, F1=0.9382\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 9003.19 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7469.82 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9444.92 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:28<00:00,  2.12it/s, loss=0.253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.9940\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:28<00:00,  2.11it/s, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.0690\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:28<00:00,  2.12it/s, loss=0.143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.6223\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:28<00:00,  2.11it/s, loss=0.108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.9067\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 60/60 [00:27<00:00,  2.20it/s, loss=0.0842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.0543\n",
      "Test Metrics: Precision=0.9354, Recall=0.9354, F1=0.9354\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8443.74 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7503.79 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9038.46 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:29<00:00,  2.04it/s, loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.2836\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:29<00:00,  2.06it/s, loss=0.317] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.5040\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:29<00:00,  2.05it/s, loss=0.0743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7337\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 60/60 [00:29<00:00,  2.06it/s, loss=0.0926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2953\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 60/60 [00:29<00:00,  2.05it/s, loss=0.0342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.5638\n",
      "Test Metrics: Precision=0.9424, Recall=0.9424, F1=0.9424\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 485, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 485/485 [00:00<00:00, 8039.45 examples/s]\n",
      "Map: 100%|██████████| 97/97 [00:00<00:00, 7442.15 examples/s]\n",
      "Map: 100%|██████████| 666/666 [00:00<00:00, 9184.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 61/61 [00:27<00:00,  2.25it/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.6040\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 61/61 [00:26<00:00,  2.27it/s, loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.0723\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 61/61 [00:26<00:00,  2.31it/s, loss=0.0895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9613\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 61/61 [00:27<00:00,  2.26it/s, loss=0.157] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.9950\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 61/61 [00:26<00:00,  2.27it/s, loss=0.0482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.2210\n",
      "Test Metrics: Precision=0.9362, Recall=0.9362, F1=0.9362\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 485, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 485/485 [00:00<00:00, 8638.10 examples/s]\n",
      "Map: 100%|██████████| 97/97 [00:00<00:00, 7341.30 examples/s]\n",
      "Map: 100%|██████████| 666/666 [00:00<00:00, 8960.81 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 61/61 [00:28<00:00,  2.18it/s, loss=0.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.2143\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 61/61 [00:28<00:00,  2.18it/s, loss=0.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.8576\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 61/61 [00:27<00:00,  2.19it/s, loss=0.341] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.7771\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 61/61 [00:28<00:00,  2.15it/s, loss=0.178] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 9.1785\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 61/61 [00:28<00:00,  2.17it/s, loss=0.153] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 7.5687\n",
      "Test Metrics: Precision=0.9304, Recall=0.9304, F1=0.9304\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 485, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 485/485 [00:00<00:00, 7887.91 examples/s]\n",
      "Map: 100%|██████████| 97/97 [00:00<00:00, 8177.35 examples/s]\n",
      "Map: 100%|██████████| 666/666 [00:00<00:00, 3580.19 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 61/61 [00:29<00:00,  2.06it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.7399\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 61/61 [00:29<00:00,  2.07it/s, loss=0.297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.8126\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 61/61 [00:28<00:00,  2.11it/s, loss=0.214] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1357\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 61/61 [00:29<00:00,  2.09it/s, loss=0.0493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.2967\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 61/61 [00:29<00:00,  2.04it/s, loss=0.0796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.7355\n",
      "Test Metrics: Precision=0.9391, Recall=0.9391, F1=0.9391\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 485, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 485/485 [00:00<00:00, 8834.72 examples/s]\n",
      "Map: 100%|██████████| 97/97 [00:00<00:00, 7481.70 examples/s]\n",
      "Map: 100%|██████████| 666/666 [00:00<00:00, 9040.30 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 61/61 [00:29<00:00,  2.10it/s, loss=0.391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.7183\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 61/61 [00:28<00:00,  2.14it/s, loss=0.215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.0483\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 61/61 [00:28<00:00,  2.14it/s, loss=0.143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2037\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 61/61 [00:28<00:00,  2.18it/s, loss=0.0812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.1740\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 61/61 [00:27<00:00,  2.19it/s, loss=0.0819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.6432\n",
      "Test Metrics: Precision=0.9350, Recall=0.9350, F1=0.9350\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 485, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 485/485 [00:00<00:00, 8740.95 examples/s]\n",
      "Map: 100%|██████████| 97/97 [00:00<00:00, 7104.15 examples/s]\n",
      "Map: 100%|██████████| 666/666 [00:00<00:00, 9525.10 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 61/61 [00:30<00:00,  2.01it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.3424\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 61/61 [00:29<00:00,  2.08it/s, loss=0.436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.2232\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 61/61 [00:29<00:00,  2.08it/s, loss=0.109] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7630\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 61/61 [00:30<00:00,  2.03it/s, loss=0.101] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7235\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 61/61 [00:29<00:00,  2.05it/s, loss=0.0619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.9906\n",
      "Test Metrics: Precision=0.9389, Recall=0.9389, F1=0.9389\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 490, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 490/490 [00:00<00:00, 8648.56 examples/s]\n",
      "Map: 100%|██████████| 98/98 [00:00<00:00, 7212.27 examples/s]\n",
      "Map: 100%|██████████| 660/660 [00:00<00:00, 9311.93 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:27<00:00,  2.27it/s, loss=0.232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.2229\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:27<00:00,  2.26it/s, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.9325\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:27<00:00,  2.24it/s, loss=0.127] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.9549\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:27<00:00,  2.25it/s, loss=0.118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.8903\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 62/62 [00:26<00:00,  2.31it/s, loss=0.135] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.8591\n",
      "Test Metrics: Precision=0.9324, Recall=0.9324, F1=0.9324\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 490, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 490/490 [00:00<00:00, 8921.77 examples/s]\n",
      "Map: 100%|██████████| 98/98 [00:00<00:00, 7614.28 examples/s]\n",
      "Map: 100%|██████████| 660/660 [00:00<00:00, 9148.97 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:28<00:00,  2.18it/s, loss=0.0943]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.6818\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:28<00:00,  2.16it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.9811\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:28<00:00,  2.21it/s, loss=0.118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.4775\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:28<00:00,  2.20it/s, loss=0.158] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.2980\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 62/62 [00:28<00:00,  2.21it/s, loss=0.00921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.7183\n",
      "Test Metrics: Precision=0.9357, Recall=0.9357, F1=0.9357\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 490, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 490/490 [00:00<00:00, 8379.78 examples/s]\n",
      "Map: 100%|██████████| 98/98 [00:00<00:00, 8392.72 examples/s]\n",
      "Map: 100%|██████████| 660/660 [00:00<00:00, 9327.55 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:29<00:00,  2.09it/s, loss=0.272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.1209\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:29<00:00,  2.10it/s, loss=0.192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.9491\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:29<00:00,  2.11it/s, loss=0.193] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.5177\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:29<00:00,  2.13it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.0105\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 62/62 [00:29<00:00,  2.09it/s, loss=0.0709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.4480\n",
      "Test Metrics: Precision=0.9362, Recall=0.9362, F1=0.9362\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 490, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 490/490 [00:00<00:00, 8533.33 examples/s]\n",
      "Map: 100%|██████████| 98/98 [00:00<00:00, 8312.44 examples/s]\n",
      "Map: 100%|██████████| 660/660 [00:00<00:00, 9153.24 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:28<00:00,  2.19it/s, loss=0.476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.1125\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:28<00:00,  2.14it/s, loss=0.163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.0199\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:29<00:00,  2.13it/s, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.9841\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:26<00:00,  2.31it/s, loss=0.136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 8.0665\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 62/62 [00:28<00:00,  2.16it/s, loss=0.063] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 6.5682\n",
      "Test Metrics: Precision=0.9339, Recall=0.9339, F1=0.9339\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 490, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 490/490 [00:00<00:00, 8341.15 examples/s]\n",
      "Map: 100%|██████████| 98/98 [00:00<00:00, 7755.21 examples/s]\n",
      "Map: 100%|██████████| 660/660 [00:00<00:00, 9357.76 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:28<00:00,  2.14it/s, loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.8495\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:29<00:00,  2.10it/s, loss=0.124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.4301\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:29<00:00,  2.09it/s, loss=0.125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.2272\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:30<00:00,  2.02it/s, loss=0.0577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.9335\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 62/62 [00:29<00:00,  2.13it/s, loss=0.137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.9513\n",
      "Test Metrics: Precision=0.9386, Recall=0.9386, F1=0.9386\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 495, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 495/495 [00:00<00:00, 8715.57 examples/s]\n",
      "Map: 100%|██████████| 99/99 [00:00<00:00, 7477.42 examples/s]\n",
      "Map: 100%|██████████| 654/654 [00:00<00:00, 9042.19 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:26<00:00,  2.30it/s, loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.9231\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:27<00:00,  2.25it/s, loss=0.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.6492\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:27<00:00,  2.25it/s, loss=0.254] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2314\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:27<00:00,  2.28it/s, loss=0.0577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0020\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 62/62 [00:28<00:00,  2.21it/s, loss=0.0562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.1019\n",
      "Test Metrics: Precision=0.9362, Recall=0.9362, F1=0.9362\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 495, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 495/495 [00:00<00:00, 8967.10 examples/s]\n",
      "Map: 100%|██████████| 99/99 [00:00<00:00, 7812.97 examples/s]\n",
      "Map: 100%|██████████| 654/654 [00:00<00:00, 8830.28 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:28<00:00,  2.17it/s, loss=0.129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.5788\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:28<00:00,  2.19it/s, loss=0.246] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.3300\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:28<00:00,  2.20it/s, loss=0.176] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9033\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:28<00:00,  2.19it/s, loss=0.0841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5194\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 62/62 [00:27<00:00,  2.22it/s, loss=0.0887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.8156\n",
      "Test Metrics: Precision=0.9368, Recall=0.9368, F1=0.9368\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 495, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 495/495 [00:00<00:00, 7989.52 examples/s]\n",
      "Map: 100%|██████████| 99/99 [00:00<00:00, 7628.39 examples/s]\n",
      "Map: 100%|██████████| 654/654 [00:00<00:00, 9210.79 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:29<00:00,  2.13it/s, loss=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.9672\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:30<00:00,  2.05it/s, loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.6778\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:29<00:00,  2.09it/s, loss=0.23]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.7066\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:29<00:00,  2.13it/s, loss=0.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.5811\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 62/62 [00:29<00:00,  2.08it/s, loss=0.0774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.7881\n",
      "Test Metrics: Precision=0.9348, Recall=0.9348, F1=0.9348\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 495, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 495/495 [00:00<00:00, 8578.37 examples/s]\n",
      "Map: 100%|██████████| 99/99 [00:00<00:00, 7696.83 examples/s]\n",
      "Map: 100%|██████████| 654/654 [00:00<00:00, 9253.61 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:28<00:00,  2.16it/s, loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.0745\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:29<00:00,  2.12it/s, loss=0.307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.0478\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:28<00:00,  2.18it/s, loss=0.0911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2296\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:28<00:00,  2.18it/s, loss=0.0823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.3433\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 62/62 [00:28<00:00,  2.19it/s, loss=0.114] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.5060\n",
      "Test Metrics: Precision=0.9359, Recall=0.9359, F1=0.9359\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 495, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 495/495 [00:00<00:00, 8075.88 examples/s]\n",
      "Map: 100%|██████████| 99/99 [00:00<00:00, 7617.47 examples/s]\n",
      "Map: 100%|██████████| 654/654 [00:00<00:00, 9154.42 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 62/62 [00:30<00:00,  2.01it/s, loss=0.233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.9347\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 62/62 [00:30<00:00,  2.06it/s, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.6394\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 62/62 [00:29<00:00,  2.07it/s, loss=0.275] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2723\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 62/62 [00:29<00:00,  2.09it/s, loss=0.118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.0169\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 62/62 [00:30<00:00,  2.02it/s, loss=0.112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.2183\n",
      "Test Metrics: Precision=0.9417, Recall=0.9417, F1=0.9417\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8481.15 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7364.50 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9072.06 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:27<00:00,  2.29it/s, loss=0.657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.6950\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:27<00:00,  2.29it/s, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.8353\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:27<00:00,  2.30it/s, loss=0.0666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.1180\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 63/63 [00:27<00:00,  2.29it/s, loss=0.175] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 9.6055\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 63/63 [00:27<00:00,  2.28it/s, loss=0.118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 7.6197\n",
      "Test Metrics: Precision=0.9265, Recall=0.9265, F1=0.9265\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8429.88 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7763.78 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 8968.58 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:28<00:00,  2.18it/s, loss=0.358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.2347\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:28<00:00,  2.22it/s, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.3695\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:28<00:00,  2.25it/s, loss=0.163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.9625\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 63/63 [00:28<00:00,  2.19it/s, loss=0.0354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.2623\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 63/63 [00:28<00:00,  2.20it/s, loss=0.0351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.3831\n",
      "Test Metrics: Precision=0.9369, Recall=0.9369, F1=0.9369\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8001.28 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8118.58 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9447.81 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:30<00:00,  2.07it/s, loss=0.213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.9661\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:30<00:00,  2.09it/s, loss=0.255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.7740\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:30<00:00,  2.09it/s, loss=0.167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0669\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 63/63 [00:30<00:00,  2.05it/s, loss=0.0569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.7718\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 63/63 [00:29<00:00,  2.12it/s, loss=0.0364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.2027\n",
      "Test Metrics: Precision=0.9347, Recall=0.9347, F1=0.9347\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8728.17 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7380.57 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 8975.83 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:28<00:00,  2.17it/s, loss=0.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.2752\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:28<00:00,  2.21it/s, loss=0.115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.6193\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:29<00:00,  2.17it/s, loss=0.167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.0751\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 63/63 [00:29<00:00,  2.10it/s, loss=0.0943]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.9035\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 63/63 [00:28<00:00,  2.19it/s, loss=0.142] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.1557\n",
      "Test Metrics: Precision=0.9356, Recall=0.9356, F1=0.9356\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8894.38 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8260.08 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9008.89 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:29<00:00,  2.11it/s, loss=0.124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.4851\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:30<00:00,  2.04it/s, loss=0.269] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.3291\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:30<00:00,  2.09it/s, loss=0.0345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2853\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 63/63 [00:31<00:00,  2.00it/s, loss=0.0735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 6.2381\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 63/63 [00:30<00:00,  2.04it/s, loss=0.0638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.3945\n",
      "Test Metrics: Precision=0.9398, Recall=0.9398, F1=0.9398\n",
      "Results saved to Experiments_epoch5_roberta.xlsx\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"large\": \"roberta-large\"\n",
    "}\n",
    "\n",
    "iterate_and_finetune_with_torch(dataset=dataset, file_name='Experiments_epoch5_roberta.xlsx', models=models, start_size=210, end_size=500, step_size=5, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bba81e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 360, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8564.61 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7287.58 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9336.01 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:07<00:00,  6.01it/s, loss=0.439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.2289\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:07<00:00,  6.29it/s, loss=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.9895\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:07<00:00,  6.29it/s, loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.5540\n",
      "Test Metrics: Precision=0.9055, Recall=0.9055, F1=0.9055\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 360, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9307.46 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 6697.05 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9470.50 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:21<00:00,  2.08it/s, loss=0.482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.4418\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:22<00:00,  2.04it/s, loss=0.233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.3432\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:21<00:00,  2.11it/s, loss=0.161] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.1260\n",
      "Test Metrics: Precision=0.9128, Recall=0.9128, F1=0.9128\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 7691.27 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 6921.14 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9762.18 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:20<00:00,  2.22it/s, loss=0.407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.1315\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:19<00:00,  2.27it/s, loss=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5471\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:20<00:00,  2.22it/s, loss=0.205] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3056\n",
      "Test Metrics: Precision=0.9302, Recall=0.9302, F1=0.9302\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 360, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9278.52 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7139.07 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9529.09 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:07<00:00,  6.15it/s, loss=0.38] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.9351\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:07<00:00,  6.04it/s, loss=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.0023\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:07<00:00,  5.99it/s, loss=0.329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.7698\n",
      "Test Metrics: Precision=0.9048, Recall=0.9048, F1=0.9048\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 360, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9357.88 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7419.17 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9490.61 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:23<00:00,  1.93it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.6538\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:22<00:00,  1.97it/s, loss=0.335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.3011\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:23<00:00,  1.93it/s, loss=0.0839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3352\n",
      "Test Metrics: Precision=0.9144, Recall=0.9144, F1=0.9144\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9324.82 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 6978.71 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9642.32 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:21<00:00,  2.12it/s, loss=0.474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.9761\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:20<00:00,  2.14it/s, loss=0.386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5611\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:21<00:00,  2.13it/s, loss=0.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8770\n",
      "Test Metrics: Precision=0.9264, Recall=0.9264, F1=0.9264\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 360, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8807.76 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7620.62 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9317.43 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:07<00:00,  6.12it/s, loss=0.54] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.1288\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:07<00:00,  5.98it/s, loss=0.314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.2919\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:07<00:00,  6.03it/s, loss=0.281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.3931\n",
      "Test Metrics: Precision=0.9036, Recall=0.9036, F1=0.9036\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 360, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9158.98 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7717.21 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9268.05 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:22<00:00,  2.01it/s, loss=0.269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.8159\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:21<00:00,  2.06it/s, loss=0.218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5086\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:23<00:00,  1.95it/s, loss=0.157] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3061\n",
      "Test Metrics: Precision=0.9140, Recall=0.9140, F1=0.9140\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9433.12 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7706.77 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9641.97 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:21<00:00,  2.13it/s, loss=0.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.0083\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:21<00:00,  2.12it/s, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2893\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:21<00:00,  2.14it/s, loss=0.136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.1239\n",
      "Test Metrics: Precision=0.9241, Recall=0.9241, F1=0.9241\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 360, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9148.88 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7259.37 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9406.93 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:07<00:00,  5.89it/s, loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.1929\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:07<00:00,  6.04it/s, loss=0.482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.6972\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:07<00:00,  5.92it/s, loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.6722\n",
      "Test Metrics: Precision=0.9027, Recall=0.9027, F1=0.9027\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 360, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9231.10 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7293.56 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9310.38 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:21<00:00,  2.05it/s, loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.5380\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:23<00:00,  1.95it/s, loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.7974\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:23<00:00,  1.92it/s, loss=0.152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.7635\n",
      "Test Metrics: Precision=0.9118, Recall=0.9118, F1=0.9118\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9334.97 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7345.01 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9475.56 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:20<00:00,  2.18it/s, loss=0.325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.1188\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:21<00:00,  2.12it/s, loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8281\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:21<00:00,  2.09it/s, loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8584\n",
      "Test Metrics: Precision=0.9240, Recall=0.9240, F1=0.9240\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 360, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9087.43 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7634.88 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9470.02 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:07<00:00,  5.69it/s, loss=0.576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.0824\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:07<00:00,  5.77it/s, loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.4259\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:07<00:00,  5.71it/s, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.4743\n",
      "Test Metrics: Precision=0.9057, Recall=0.9057, F1=0.9057\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 360, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9038.04 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7452.12 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9466.25 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:24<00:00,  1.84it/s, loss=0.549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.3473\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:23<00:00,  1.88it/s, loss=0.386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.4781\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:22<00:00,  1.99it/s, loss=0.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.0947\n",
      "Test Metrics: Precision=0.9133, Recall=0.9133, F1=0.9133\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8773.42 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7563.74 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9653.64 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:23<00:00,  1.95it/s, loss=0.287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.1537\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:22<00:00,  2.00it/s, loss=0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2822\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:22<00:00,  1.97it/s, loss=0.199] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.4408\n",
      "Test Metrics: Precision=0.9275, Recall=0.9275, F1=0.9275\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 360, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9054.90 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 6754.57 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9183.62 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:07<00:00,  5.84it/s, loss=0.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.7831\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:07<00:00,  6.02it/s, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.4310\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:07<00:00,  5.85it/s, loss=0.175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.5585\n",
      "Test Metrics: Precision=0.9003, Recall=0.9003, F1=0.9003\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 360, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9081.47 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 6884.84 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9431.25 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:22<00:00,  1.98it/s, loss=0.557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.2126\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:23<00:00,  1.93it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.8802\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:22<00:00,  1.99it/s, loss=0.0966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7151\n",
      "Test Metrics: Precision=0.9130, Recall=0.9130, F1=0.9130\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9055.55 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 6805.71 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 3787.30 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:20<00:00,  2.16it/s, loss=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.3154\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:21<00:00,  2.12it/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.5544\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:21<00:00,  2.14it/s, loss=0.289] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1582\n",
      "Test Metrics: Precision=0.9260, Recall=0.9260, F1=0.9260\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 360, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8989.83 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 8334.20 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9141.99 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:07<00:00,  6.06it/s, loss=0.437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.4391\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:07<00:00,  5.92it/s, loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.8429\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:07<00:00,  6.19it/s, loss=0.145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.8072\n",
      "Test Metrics: Precision=0.9074, Recall=0.9074, F1=0.9074\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 360, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9023.94 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 8570.25 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9265.54 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:22<00:00,  2.03it/s, loss=0.32] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.6233\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:22<00:00,  1.99it/s, loss=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.9231\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:22<00:00,  2.00it/s, loss=0.185] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.9410\n",
      "Test Metrics: Precision=0.9144, Recall=0.9144, F1=0.9144\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 9085.46 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 8230.85 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9455.14 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:21<00:00,  2.10it/s, loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.8269\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:20<00:00,  2.15it/s, loss=0.313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.4592\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:20<00:00,  2.17it/s, loss=0.375] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1298\n",
      "Test Metrics: Precision=0.9293, Recall=0.9293, F1=0.9293\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 360, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8868.02 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7367.22 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 3735.99 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:07<00:00,  5.72it/s, loss=0.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.7454\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:07<00:00,  5.77it/s, loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.4873\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:08<00:00,  5.60it/s, loss=0.189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.9963\n",
      "Test Metrics: Precision=0.9021, Recall=0.9021, F1=0.9021\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 360, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8744.71 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7405.71 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9413.81 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:24<00:00,  1.86it/s, loss=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.3744\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:24<00:00,  1.85it/s, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.9178\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:24<00:00,  1.81it/s, loss=0.133] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.2333\n",
      "Test Metrics: Precision=0.9136, Recall=0.9136, F1=0.9136\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8835.59 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7185.10 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9635.43 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:22<00:00,  2.02it/s, loss=0.291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.6320\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:22<00:00,  1.96it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.6561\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:23<00:00,  1.93it/s, loss=0.0784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8362\n",
      "Test Metrics: Precision=0.9312, Recall=0.9312, F1=0.9312\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 360, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8993.74 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 8234.67 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9013.10 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:07<00:00,  5.81it/s, loss=0.335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.0512\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:07<00:00,  5.86it/s, loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.3648\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:07<00:00,  6.03it/s, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.8512\n",
      "Test Metrics: Precision=0.9011, Recall=0.9011, F1=0.9011\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 360, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8911.31 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7861.25 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9124.79 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:22<00:00,  1.97it/s, loss=0.335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.7762\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:23<00:00,  1.90it/s, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.0109\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:23<00:00,  1.95it/s, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.4012\n",
      "Test Metrics: Precision=0.9073, Recall=0.9073, F1=0.9073\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8978.82 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7457.28 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9272.32 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:21<00:00,  2.12it/s, loss=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 21.4407\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:20<00:00,  2.15it/s, loss=0.193] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.9384\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:21<00:00,  2.10it/s, loss=0.104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.2250\n",
      "Test Metrics: Precision=0.9325, Recall=0.9325, F1=0.9325\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 360, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8659.95 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7752.47 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9249.22 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:07<00:00,  5.70it/s, loss=0.324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.8352\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:07<00:00,  5.89it/s, loss=0.331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.6462\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:07<00:00,  5.75it/s, loss=0.236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.9444\n",
      "Test Metrics: Precision=0.9063, Recall=0.9063, F1=0.9063\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 360, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8371.82 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 7920.84 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9155.76 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:23<00:00,  1.92it/s, loss=0.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.5877\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:24<00:00,  1.85it/s, loss=0.404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.1396\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:23<00:00,  1.91it/s, loss=0.0995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5137\n",
      "Test Metrics: Precision=0.9180, Recall=0.9180, F1=0.9180\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 360, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 360/360 [00:00<00:00, 8541.31 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 8163.87 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:00<00:00, 9800.84 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 45/45 [00:22<00:00,  2.02it/s, loss=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 22.1302\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 45/45 [00:22<00:00,  1.98it/s, loss=0.266] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.5176\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 45/45 [00:22<00:00,  1.99it/s, loss=0.224] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.6164\n",
      "Test Metrics: Precision=0.9321, Recall=0.9321, F1=0.9321\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 380, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 9024.76 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7413.53 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9132.64 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:07<00:00,  6.30it/s, loss=0.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.5185\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:07<00:00,  6.20it/s, loss=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.9769\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:07<00:00,  6.11it/s, loss=0.95] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.2411\n",
      "Test Metrics: Precision=0.9044, Recall=0.9044, F1=0.9044\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 380, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 9221.82 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7713.48 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9498.38 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:23<00:00,  2.03it/s, loss=0.445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.9523\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:24<00:00,  1.97it/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.4270\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:24<00:00,  1.99it/s, loss=0.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.0468\n",
      "Test Metrics: Precision=0.9090, Recall=0.9090, F1=0.9090\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 9120.14 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7276.29 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9411.57 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:21<00:00,  2.21it/s, loss=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.4174\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:22<00:00,  2.12it/s, loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.2502\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:21<00:00,  2.24it/s, loss=0.229] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6684\n",
      "Test Metrics: Precision=0.9278, Recall=0.9278, F1=0.9278\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 380, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8914.57 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7723.76 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9028.15 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:07<00:00,  6.06it/s, loss=0.414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.6762\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:07<00:00,  6.14it/s, loss=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.0769\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:07<00:00,  6.15it/s, loss=0.164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.4212\n",
      "Test Metrics: Precision=0.9053, Recall=0.9053, F1=0.9053\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 380, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8986.09 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7756.83 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9107.73 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:24<00:00,  1.98it/s, loss=0.402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.8483\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:24<00:00,  1.98it/s, loss=0.334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.6245\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:24<00:00,  1.95it/s, loss=0.132] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.0877\n",
      "Test Metrics: Precision=0.9101, Recall=0.9101, F1=0.9101\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 9168.77 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7995.56 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9196.02 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:22<00:00,  2.17it/s, loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.7740\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:22<00:00,  2.10it/s, loss=0.22]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.7591\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:22<00:00,  2.14it/s, loss=0.192] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9693\n",
      "Test Metrics: Precision=0.9294, Recall=0.9294, F1=0.9294\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 380, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8654.29 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 8093.00 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9058.48 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:07<00:00,  6.07it/s, loss=0.598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.9749\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:07<00:00,  6.15it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.4628\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:07<00:00,  6.08it/s, loss=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.3492\n",
      "Test Metrics: Precision=0.9056, Recall=0.9056, F1=0.9056\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 380, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 9046.63 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 8344.25 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9147.61 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:24<00:00,  1.95it/s, loss=0.994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.4128\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:24<00:00,  1.99it/s, loss=0.286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.3489\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:24<00:00,  1.99it/s, loss=0.317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 18.0922\n",
      "Test Metrics: Precision=0.8700, Recall=0.8700, F1=0.8700\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 9108.31 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 8217.98 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9414.85 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:23<00:00,  2.06it/s, loss=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.8307\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:22<00:00,  2.11it/s, loss=0.167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.3968\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:22<00:00,  2.17it/s, loss=0.0894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3744\n",
      "Test Metrics: Precision=0.9304, Recall=0.9304, F1=0.9304\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 380, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 9166.98 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7561.43 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 8798.45 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:07<00:00,  6.22it/s, loss=0.565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.0349\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:07<00:00,  6.32it/s, loss=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.2542\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:07<00:00,  6.22it/s, loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.8890\n",
      "Test Metrics: Precision=0.9025, Recall=0.9025, F1=0.9025\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 380, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8901.37 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7746.28 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 8971.48 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:24<00:00,  1.97it/s, loss=0.421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.0366\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:24<00:00,  1.95it/s, loss=0.213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1392\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:24<00:00,  1.97it/s, loss=0.118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7838\n",
      "Test Metrics: Precision=0.9145, Recall=0.9145, F1=0.9145\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 9611.03 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7793.05 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9273.29 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:22<00:00,  2.18it/s, loss=0.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.2071\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:22<00:00,  2.17it/s, loss=0.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.2185\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:22<00:00,  2.17it/s, loss=0.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.4002\n",
      "Test Metrics: Precision=0.9237, Recall=0.9237, F1=0.9237\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 380, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8642.47 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 6709.90 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 3653.45 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:08<00:00,  5.87it/s, loss=0.543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.9436\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:08<00:00,  5.92it/s, loss=0.348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.5407\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:08<00:00,  5.92it/s, loss=0.279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.3461\n",
      "Test Metrics: Precision=0.9097, Recall=0.9097, F1=0.9097\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 380, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8921.80 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 6801.82 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9257.68 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:25<00:00,  1.87it/s, loss=0.689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.4607\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:25<00:00,  1.89it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.3324\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:25<00:00,  1.91it/s, loss=0.136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.9891\n",
      "Test Metrics: Precision=0.9100, Recall=0.9100, F1=0.9100\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8793.72 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 6991.27 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9641.21 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:23<00:00,  2.06it/s, loss=0.497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.3580\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:23<00:00,  2.05it/s, loss=0.218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2010\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:23<00:00,  2.06it/s, loss=0.2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7132\n",
      "Test Metrics: Precision=0.9322, Recall=0.9322, F1=0.9322\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 380, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8664.69 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7585.00 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9302.09 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:07<00:00,  6.01it/s, loss=0.42] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.9841\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:08<00:00,  5.93it/s, loss=0.583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.7562\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:07<00:00,  6.04it/s, loss=0.174] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.7317\n",
      "Test Metrics: Precision=0.9048, Recall=0.9048, F1=0.9048\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 380, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8886.04 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7504.29 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9240.82 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:25<00:00,  1.92it/s, loss=0.282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.7045\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:23<00:00,  2.01it/s, loss=0.297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 20.3423\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:24<00:00,  1.99it/s, loss=0.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 16.0806\n",
      "Test Metrics: Precision=0.8848, Recall=0.8848, F1=0.8848\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8712.86 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7606.54 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9315.27 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:22<00:00,  2.15it/s, loss=0.255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.5303\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:23<00:00,  2.05it/s, loss=0.287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8749\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:22<00:00,  2.13it/s, loss=0.0685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6568\n",
      "Test Metrics: Precision=0.9262, Recall=0.9262, F1=0.9262\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 380, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8858.28 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 6793.12 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9108.00 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:07<00:00,  6.06it/s, loss=0.463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.4077\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:07<00:00,  6.04it/s, loss=0.0946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.2132\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:07<00:00,  6.23it/s, loss=0.198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.2159\n",
      "Test Metrics: Precision=0.9090, Recall=0.9090, F1=0.9090\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 380, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8820.00 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 6784.88 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9343.06 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:23<00:00,  2.02it/s, loss=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.3295\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:24<00:00,  1.99it/s, loss=0.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.4393\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:24<00:00,  2.00it/s, loss=0.225] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0138\n",
      "Test Metrics: Precision=0.9177, Recall=0.9177, F1=0.9177\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 9008.94 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 6859.78 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9474.33 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:23<00:00,  2.06it/s, loss=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.9917\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:23<00:00,  2.06it/s, loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.0012\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:22<00:00,  2.13it/s, loss=0.136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3521\n",
      "Test Metrics: Precision=0.9301, Recall=0.9301, F1=0.9301\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 380, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8680.74 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 6892.11 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9069.01 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:08<00:00,  5.65it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.2578\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:08<00:00,  5.74it/s, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.2909\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:08<00:00,  5.85it/s, loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.3398\n",
      "Test Metrics: Precision=0.9088, Recall=0.9088, F1=0.9088\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 380, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8855.77 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7465.62 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9480.84 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:26<00:00,  1.83it/s, loss=0.475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.1246\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:26<00:00,  1.81it/s, loss=0.347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.6710\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:25<00:00,  1.88it/s, loss=0.0564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.4767\n",
      "Test Metrics: Precision=0.9144, Recall=0.9144, F1=0.9144\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8692.43 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7070.99 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9346.85 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:23<00:00,  2.06it/s, loss=0.545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.8691\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:23<00:00,  2.02it/s, loss=0.256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.2232\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:23<00:00,  2.00it/s, loss=0.153] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.5054\n",
      "Test Metrics: Precision=0.9306, Recall=0.9306, F1=0.9306\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 380, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8963.15 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7759.10 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9125.24 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:07<00:00,  6.04it/s, loss=0.628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.7812\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:07<00:00,  6.12it/s, loss=0.12] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.4894\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:07<00:00,  6.06it/s, loss=0.179] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.9680\n",
      "Test Metrics: Precision=0.9023, Recall=0.9023, F1=0.9023\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 380, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8679.13 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7927.36 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9201.40 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:24<00:00,  1.97it/s, loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.5603\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:23<00:00,  2.07it/s, loss=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.2771\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:24<00:00,  1.97it/s, loss=0.205] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9532\n",
      "Test Metrics: Precision=0.9155, Recall=0.9155, F1=0.9155\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 9052.59 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7557.30 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9376.32 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:22<00:00,  2.10it/s, loss=0.392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.3174\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:22<00:00,  2.11it/s, loss=0.336] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8929\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:22<00:00,  2.10it/s, loss=0.154] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8448\n",
      "Test Metrics: Precision=0.9296, Recall=0.9296, F1=0.9296\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 380, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8584.01 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 6837.41 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9284.18 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:08<00:00,  5.93it/s, loss=0.509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.7329\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:08<00:00,  5.91it/s, loss=0.519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.2957\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:08<00:00,  5.90it/s, loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.0054\n",
      "Test Metrics: Precision=0.9072, Recall=0.9072, F1=0.9072\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 380, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8852.13 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 6994.19 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 3872.01 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:25<00:00,  1.87it/s, loss=0.486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.0672\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:24<00:00,  1.98it/s, loss=0.18] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.1640\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:25<00:00,  1.91it/s, loss=0.105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9359\n",
      "Test Metrics: Precision=0.9168, Recall=0.9168, F1=0.9168\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 380, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 8676.72 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 7088.28 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 9737.41 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 48/48 [00:23<00:00,  2.04it/s, loss=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.0248\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 48/48 [00:23<00:00,  2.07it/s, loss=0.0891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.0392\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 48/48 [00:23<00:00,  2.04it/s, loss=0.194] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4868\n",
      "Test Metrics: Precision=0.9345, Recall=0.9345, F1=0.9345\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 400, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 9078.92 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7894.42 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9288.18 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:08<00:00,  6.21it/s, loss=0.691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.6487\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:08<00:00,  6.17it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.3544\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:07<00:00,  6.31it/s, loss=0.285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.7854\n",
      "Test Metrics: Precision=0.9092, Recall=0.9092, F1=0.9092\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 400, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8980.66 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7854.50 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9334.53 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:24<00:00,  2.04it/s, loss=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.0314\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:24<00:00,  2.01it/s, loss=0.269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.8181\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:25<00:00,  1.96it/s, loss=0.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.5714\n",
      "Test Metrics: Precision=0.9132, Recall=0.9132, F1=0.9132\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 9119.44 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7612.51 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9176.76 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:22<00:00,  2.24it/s, loss=0.346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.7829\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:22<00:00,  2.24it/s, loss=0.413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5650\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:22<00:00,  2.20it/s, loss=0.266] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.6141\n",
      "Test Metrics: Precision=0.9295, Recall=0.9295, F1=0.9295\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 400, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8887.27 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7847.89 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 8824.26 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:08<00:00,  6.14it/s, loss=0.547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.9514\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:07<00:00,  6.39it/s, loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.8711\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:07<00:00,  6.37it/s, loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.0820\n",
      "Test Metrics: Precision=0.9054, Recall=0.9054, F1=0.9054\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 400, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 9090.93 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7859.84 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9066.21 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:25<00:00,  1.97it/s, loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.5656\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:24<00:00,  2.03it/s, loss=0.301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.2613\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:25<00:00,  1.98it/s, loss=0.233] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.0073\n",
      "Test Metrics: Precision=0.9136, Recall=0.9136, F1=0.9136\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 9173.04 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7887.00 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9132.92 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:23<00:00,  2.15it/s, loss=0.345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.7573\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:22<00:00,  2.21it/s, loss=0.208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8609\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:23<00:00,  2.10it/s, loss=0.277] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.7205\n",
      "Test Metrics: Precision=0.9264, Recall=0.9264, F1=0.9264\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 400, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8933.84 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7320.70 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9091.31 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:08<00:00,  5.98it/s, loss=0.269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 34.3303\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:08<00:00,  5.97it/s, loss=0.208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.5407\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:08<00:00,  6.05it/s, loss=0.181] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.0318\n",
      "Test Metrics: Precision=0.9018, Recall=0.9018, F1=0.9018\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 400, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8653.22 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7040.52 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9261.42 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:25<00:00,  1.95it/s, loss=0.376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.8637\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:25<00:00,  1.94it/s, loss=0.133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2793\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:25<00:00,  1.97it/s, loss=0.247] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8185\n",
      "Test Metrics: Precision=0.9138, Recall=0.9138, F1=0.9138\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8556.40 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6906.19 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9359.48 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:23<00:00,  2.12it/s, loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.8084\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:22<00:00,  2.18it/s, loss=0.139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.2140\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:24<00:00,  2.06it/s, loss=0.186] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9173\n",
      "Test Metrics: Precision=0.9297, Recall=0.9297, F1=0.9297\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 400, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 9164.82 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7032.41 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9135.12 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:07<00:00,  6.31it/s, loss=0.368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.5113\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:08<00:00,  6.07it/s, loss=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.3435\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:08<00:00,  6.09it/s, loss=0.226] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.4494\n",
      "Test Metrics: Precision=0.9051, Recall=0.9051, F1=0.9051\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 400, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 9234.33 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7265.54 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9407.89 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:24<00:00,  2.04it/s, loss=0.379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.6054\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:24<00:00,  2.02it/s, loss=0.319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.4896\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:25<00:00,  1.99it/s, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2676\n",
      "Test Metrics: Precision=0.9160, Recall=0.9160, F1=0.9160\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 9331.46 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6977.71 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9221.23 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:23<00:00,  2.09it/s, loss=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.6294\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:23<00:00,  2.12it/s, loss=0.279] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.6950\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:23<00:00,  2.15it/s, loss=0.167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4802\n",
      "Test Metrics: Precision=0.9324, Recall=0.9324, F1=0.9324\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 400, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8493.46 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6984.98 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9078.81 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:08<00:00,  5.89it/s, loss=0.348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.6702\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:08<00:00,  5.83it/s, loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.5238\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:08<00:00,  5.86it/s, loss=0.238] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.1292\n",
      "Test Metrics: Precision=0.9088, Recall=0.9088, F1=0.9088\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 400, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8959.60 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7473.81 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9345.39 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:26<00:00,  1.88it/s, loss=0.735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.6900\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:26<00:00,  1.88it/s, loss=0.282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2833\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:26<00:00,  1.87it/s, loss=0.133] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7051\n",
      "Test Metrics: Precision=0.9203, Recall=0.9203, F1=0.9203\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 9088.32 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6991.96 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 8772.02 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:25<00:00,  1.98it/s, loss=0.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.8010\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:24<00:00,  2.04it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.1871\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:25<00:00,  1.97it/s, loss=0.0934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0733\n",
      "Test Metrics: Precision=0.9348, Recall=0.9348, F1=0.9348\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 400, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8874.49 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7866.10 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9232.20 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:08<00:00,  5.88it/s, loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.8686\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:08<00:00,  5.84it/s, loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.9287\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:08<00:00,  5.84it/s, loss=0.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.6833\n",
      "Test Metrics: Precision=0.9059, Recall=0.9059, F1=0.9059\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 400, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8872.42 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7775.15 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9234.05 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:24<00:00,  2.02it/s, loss=0.365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.9157\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:26<00:00,  1.91it/s, loss=0.409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.8888\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:25<00:00,  1.95it/s, loss=0.322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.2826\n",
      "Test Metrics: Precision=0.9098, Recall=0.9098, F1=0.9098\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8945.08 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7734.29 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9328.07 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:24<00:00,  2.05it/s, loss=0.297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.0496\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:23<00:00,  2.14it/s, loss=0.218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1310\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:24<00:00,  2.05it/s, loss=0.12]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2464\n",
      "Test Metrics: Precision=0.9289, Recall=0.9289, F1=0.9289\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 400, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8695.38 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7897.39 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9268.25 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:08<00:00,  5.90it/s, loss=0.355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.7552\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:08<00:00,  5.91it/s, loss=0.223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.1844\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:08<00:00,  6.02it/s, loss=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.7916\n",
      "Test Metrics: Precision=0.9086, Recall=0.9086, F1=0.9086\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 400, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8782.87 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7859.84 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9214.37 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:25<00:00,  1.94it/s, loss=0.286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.9324\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:25<00:00,  1.96it/s, loss=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.7128\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:25<00:00,  1.94it/s, loss=0.153] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.3022\n",
      "Test Metrics: Precision=0.9165, Recall=0.9165, F1=0.9165\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 9073.82 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7508.77 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9505.65 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:24<00:00,  2.04it/s, loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.7014\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:23<00:00,  2.16it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.9191\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:24<00:00,  2.01it/s, loss=0.173] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3230\n",
      "Test Metrics: Precision=0.9311, Recall=0.9311, F1=0.9311\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 400, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8593.74 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7241.86 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9457.36 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:08<00:00,  5.65it/s, loss=0.335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.5363\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:08<00:00,  5.67it/s, loss=0.273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.8586\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:08<00:00,  5.57it/s, loss=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.4386\n",
      "Test Metrics: Precision=0.9059, Recall=0.9059, F1=0.9059\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 400, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8659.88 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7130.90 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9509.41 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:27<00:00,  1.83it/s, loss=0.328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.1718\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:27<00:00,  1.85it/s, loss=0.421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.0657\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:27<00:00,  1.85it/s, loss=0.331] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.8827\n",
      "Test Metrics: Precision=0.9160, Recall=0.9160, F1=0.9160\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8651.22 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6870.56 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9430.70 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:24<00:00,  2.05it/s, loss=0.482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.9353\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:25<00:00,  2.00it/s, loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8449\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:25<00:00,  1.96it/s, loss=0.136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.2058\n",
      "Test Metrics: Precision=0.9317, Recall=0.9317, F1=0.9317\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 400, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8762.00 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7019.90 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9221.23 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:08<00:00,  6.12it/s, loss=0.328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.4823\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:08<00:00,  6.00it/s, loss=0.233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.1600\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:08<00:00,  6.11it/s, loss=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.4446\n",
      "Test Metrics: Precision=0.9055, Recall=0.9055, F1=0.9055\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 400, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8866.56 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7043.63 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9047.75 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:25<00:00,  1.95it/s, loss=0.347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.2925\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:25<00:00,  1.93it/s, loss=0.353] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5301\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:25<00:00,  1.98it/s, loss=0.143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9153\n",
      "Test Metrics: Precision=0.9167, Recall=0.9167, F1=0.9167\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8910.16 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7517.35 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9319.14 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:23<00:00,  2.11it/s, loss=0.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.3993\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:23<00:00,  2.12it/s, loss=0.243] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.4614\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:23<00:00,  2.10it/s, loss=0.105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6254\n",
      "Test Metrics: Precision=0.9332, Recall=0.9332, F1=0.9332\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 400, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8544.85 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7660.83 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9174.14 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:08<00:00,  5.90it/s, loss=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.4717\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:08<00:00,  5.70it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.9304\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:08<00:00,  5.82it/s, loss=0.166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.4369\n",
      "Test Metrics: Precision=0.9047, Recall=0.9047, F1=0.9047\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 400, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8611.43 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7207.33 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9307.32 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:26<00:00,  1.86it/s, loss=0.313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.3445\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:26<00:00,  1.90it/s, loss=0.185] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5016\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:25<00:00,  1.93it/s, loss=0.118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.1555\n",
      "Test Metrics: Precision=0.9175, Recall=0.9175, F1=0.9175\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 400, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 8720.47 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7782.18 examples/s]\n",
      "Map: 100%|██████████| 768/768 [00:00<00:00, 9342.41 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 50/50 [00:23<00:00,  2.09it/s, loss=0.326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 23.5910\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 50/50 [00:23<00:00,  2.08it/s, loss=0.178] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8364\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 50/50 [00:24<00:00,  2.04it/s, loss=0.159] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0030\n",
      "Test Metrics: Precision=0.9327, Recall=0.9327, F1=0.9327\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 420, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 9051.15 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 8104.94 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9075.15 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:08<00:00,  6.34it/s, loss=0.541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.2677\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:08<00:00,  6.19it/s, loss=0.272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.9960\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:08<00:00,  6.18it/s, loss=0.0646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.8344\n",
      "Test Metrics: Precision=0.9098, Recall=0.9098, F1=0.9098\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 420, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8802.05 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 8010.04 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9149.41 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:26<00:00,  2.03it/s, loss=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.5942\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:26<00:00,  2.00it/s, loss=0.198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.2260\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:25<00:00,  2.06it/s, loss=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.9145\n",
      "Test Metrics: Precision=0.9160, Recall=0.9160, F1=0.9160\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 9079.24 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 8240.86 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9287.17 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:24<00:00,  2.20it/s, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.5684\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:23<00:00,  2.25it/s, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.5234\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:23<00:00,  2.28it/s, loss=0.105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2715\n",
      "Test Metrics: Precision=0.9279, Recall=0.9279, F1=0.9279\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 420, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 9359.25 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7817.90 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 8892.01 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:08<00:00,  6.36it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.9421\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:08<00:00,  6.49it/s, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.5732\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:08<00:00,  6.04it/s, loss=0.261] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.8723\n",
      "Test Metrics: Precision=0.9070, Recall=0.9070, F1=0.9070\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 420, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8792.56 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7719.58 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 8838.19 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:25<00:00,  2.05it/s, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.9730\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:26<00:00,  2.01it/s, loss=0.136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.5536\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:26<00:00,  2.00it/s, loss=0.292] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8606\n",
      "Test Metrics: Precision=0.9162, Recall=0.9162, F1=0.9162\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 9097.71 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7770.31 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 8983.32 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:24<00:00,  2.19it/s, loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.1478\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:24<00:00,  2.14it/s, loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.1304\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:23<00:00,  2.24it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4469\n",
      "Test Metrics: Precision=0.9307, Recall=0.9307, F1=0.9307\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 420, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8678.34 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7257.63 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9269.65 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:08<00:00,  6.09it/s, loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.2286\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:08<00:00,  5.97it/s, loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.6080\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:08<00:00,  6.22it/s, loss=0.105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.8823\n",
      "Test Metrics: Precision=0.9044, Recall=0.9044, F1=0.9044\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 420, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8753.24 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7268.56 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 3448.91 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:27<00:00,  1.96it/s, loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.7286\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:27<00:00,  1.93it/s, loss=0.136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.2716\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:27<00:00,  1.94it/s, loss=0.222] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.2574\n",
      "Test Metrics: Precision=0.9129, Recall=0.9129, F1=0.9129\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8967.30 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7406.85 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9442.09 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:25<00:00,  2.08it/s, loss=0.236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 24.7929\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:25<00:00,  2.12it/s, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.2687\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:24<00:00,  2.15it/s, loss=0.0917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.2828\n",
      "Test Metrics: Precision=0.9294, Recall=0.9294, F1=0.9294\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 420, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8897.14 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7746.05 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9107.38 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:08<00:00,  6.13it/s, loss=0.424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.0174\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:08<00:00,  6.17it/s, loss=0.286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.0329\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:08<00:00,  6.05it/s, loss=0.41] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.7067\n",
      "Test Metrics: Precision=0.9061, Recall=0.9061, F1=0.9061\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 420, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8643.47 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7570.95 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9182.66 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:26<00:00,  2.01it/s, loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.2183\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:25<00:00,  2.07it/s, loss=0.148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.9959\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:25<00:00,  2.05it/s, loss=0.144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.0701\n",
      "Test Metrics: Precision=0.9140, Recall=0.9140, F1=0.9140\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8882.83 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7294.89 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9071.19 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:24<00:00,  2.21it/s, loss=0.354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.6198\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:24<00:00,  2.14it/s, loss=0.244] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.8412\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:24<00:00,  2.13it/s, loss=0.183] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0771\n",
      "Test Metrics: Precision=0.9292, Recall=0.9292, F1=0.9292\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 420, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8845.81 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7295.50 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9064.10 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:09<00:00,  5.74it/s, loss=0.252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 35.5645\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:09<00:00,  5.88it/s, loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 19.0379\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:09<00:00,  5.86it/s, loss=0.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.3020\n",
      "Test Metrics: Precision=0.9103, Recall=0.9103, F1=0.9103\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 420, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8935.05 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7808.89 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9384.75 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:28<00:00,  1.84it/s, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.9128\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:27<00:00,  1.94it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.8282\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:28<00:00,  1.83it/s, loss=0.129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.5537\n",
      "Test Metrics: Precision=0.9165, Recall=0.9165, F1=0.9165\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 9066.85 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7649.52 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9251.76 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:26<00:00,  2.00it/s, loss=0.345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.6790\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:26<00:00,  1.98it/s, loss=0.356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1333\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:26<00:00,  2.01it/s, loss=0.23]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4252\n",
      "Test Metrics: Precision=0.9351, Recall=0.9351, F1=0.9351\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 420, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8678.51 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 8080.58 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 8889.55 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:08<00:00,  6.10it/s, loss=0.572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.6340\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:09<00:00,  5.86it/s, loss=0.253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.8031\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:08<00:00,  6.18it/s, loss=0.281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.3363\n",
      "Test Metrics: Precision=0.9076, Recall=0.9076, F1=0.9076\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 420, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8520.97 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 8412.64 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9204.57 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:26<00:00,  1.97it/s, loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.4299\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:26<00:00,  1.99it/s, loss=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.7798\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:26<00:00,  1.98it/s, loss=0.144] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0214\n",
      "Test Metrics: Precision=0.9154, Recall=0.9154, F1=0.9154\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8939.58 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 8342.33 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9279.32 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:25<00:00,  2.05it/s, loss=0.556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.0295\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:24<00:00,  2.16it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.0692\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:25<00:00,  2.07it/s, loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.2942\n",
      "Test Metrics: Precision=0.9276, Recall=0.9276, F1=0.9276\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 420, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 2392.80 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7868.01 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9196.95 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:08<00:00,  6.15it/s, loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.1153\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:08<00:00,  6.14it/s, loss=0.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.6884\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:08<00:00,  6.13it/s, loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.6750\n",
      "Test Metrics: Precision=0.9098, Recall=0.9098, F1=0.9098\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 420, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8697.92 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7364.12 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9275.88 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:26<00:00,  1.97it/s, loss=0.726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.2423\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:25<00:00,  2.05it/s, loss=0.36] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.1284\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:26<00:00,  1.99it/s, loss=0.155] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9735\n",
      "Test Metrics: Precision=0.9194, Recall=0.9194, F1=0.9194\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8906.95 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7842.79 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9404.98 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:25<00:00,  2.09it/s, loss=0.448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 25.0203\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:25<00:00,  2.08it/s, loss=0.247] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.4571\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:25<00:00,  2.07it/s, loss=0.122] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6382\n",
      "Test Metrics: Precision=0.9337, Recall=0.9337, F1=0.9337\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 420, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8680.78 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7120.77 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9532.98 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:09<00:00,  5.69it/s, loss=0.547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 34.4745\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:09<00:00,  5.66it/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.5106\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:09<00:00,  5.67it/s, loss=0.431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.8047\n",
      "Test Metrics: Precision=0.9094, Recall=0.9094, F1=0.9094\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 420, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8526.45 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7385.58 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 3485.85 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:29<00:00,  1.82it/s, loss=0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.3351\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:28<00:00,  1.85it/s, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.4564\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:28<00:00,  1.84it/s, loss=0.198] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.7205\n",
      "Test Metrics: Precision=0.9159, Recall=0.9159, F1=0.9159\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8593.29 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 6974.32 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9423.81 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:26<00:00,  1.99it/s, loss=0.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.7002\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:26<00:00,  1.99it/s, loss=0.0741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.9815\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:26<00:00,  1.98it/s, loss=0.193] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2056\n",
      "Test Metrics: Precision=0.9287, Recall=0.9287, F1=0.9287\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 420, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8751.24 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7249.27 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9283.52 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:08<00:00,  6.25it/s, loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.0923\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:08<00:00,  6.07it/s, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.3451\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:08<00:00,  6.06it/s, loss=0.187] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.1154\n",
      "Test Metrics: Precision=0.9014, Recall=0.9014, F1=0.9014\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 420, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8746.98 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 6918.85 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9237.38 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:27<00:00,  1.96it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.1715\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:26<00:00,  1.98it/s, loss=0.365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.2287\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:27<00:00,  1.95it/s, loss=0.0903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.4293\n",
      "Test Metrics: Precision=0.9150, Recall=0.9150, F1=0.9150\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8708.33 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 6833.10 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9268.35 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:24<00:00,  2.13it/s, loss=0.441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.0537\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:25<00:00,  2.11it/s, loss=0.283] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.9914\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:25<00:00,  2.10it/s, loss=0.0764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.6988\n",
      "Test Metrics: Precision=0.9252, Recall=0.9252, F1=0.9252\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 420, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8435.40 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 6810.12 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9203.76 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:08<00:00,  6.10it/s, loss=0.368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.9137\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:09<00:00,  5.82it/s, loss=0.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.2271\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:08<00:00,  6.18it/s, loss=0.26]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.4140\n",
      "Test Metrics: Precision=0.9053, Recall=0.9053, F1=0.9053\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 420, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8677.70 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7244.20 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9283.69 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:26<00:00,  1.98it/s, loss=0.576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.8770\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:27<00:00,  1.92it/s, loss=0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.5208\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:27<00:00,  1.92it/s, loss=0.0616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2328\n",
      "Test Metrics: Precision=0.9188, Recall=0.9188, F1=0.9188\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 420, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 420/420 [00:00<00:00, 8615.69 examples/s]\n",
      "Map: 100%|██████████| 84/84 [00:00<00:00, 7263.16 examples/s]\n",
      "Map: 100%|██████████| 744/744 [00:00<00:00, 9659.36 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [00:25<00:00,  2.07it/s, loss=0.233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.5773\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [00:25<00:00,  2.06it/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.5443\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [00:25<00:00,  2.04it/s, loss=0.2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.7203\n",
      "Test Metrics: Precision=0.9311, Recall=0.9311, F1=0.9311\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 440, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8862.64 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 8082.57 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9055.96 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:08<00:00,  6.39it/s, loss=0.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.7265\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:08<00:00,  6.14it/s, loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.8618\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:08<00:00,  6.33it/s, loss=0.17]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.9467\n",
      "Test Metrics: Precision=0.9105, Recall=0.9105, F1=0.9105\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 440, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8840.52 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 8147.88 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 8921.34 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:27<00:00,  2.01it/s, loss=0.435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.0716\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:27<00:00,  2.02it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.4472\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:27<00:00,  2.02it/s, loss=0.261] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.6972\n",
      "Test Metrics: Precision=0.9108, Recall=0.9108, F1=0.9108\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 9081.26 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 8061.92 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9205.41 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:25<00:00,  2.18it/s, loss=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.0194\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:24<00:00,  2.21it/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2859\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:25<00:00,  2.14it/s, loss=0.135] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2293\n",
      "Test Metrics: Precision=0.9312, Recall=0.9312, F1=0.9312\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 440, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 2408.77 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7458.35 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9231.95 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:09<00:00,  5.96it/s, loss=0.549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 34.2124\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:09<00:00,  6.06it/s, loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.2835\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:09<00:00,  6.11it/s, loss=0.333] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.1629\n",
      "Test Metrics: Precision=0.9061, Recall=0.9061, F1=0.9061\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 440, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8638.98 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7478.14 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9021.84 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:27<00:00,  2.03it/s, loss=0.419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.8864\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:28<00:00,  1.96it/s, loss=0.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.2344\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:26<00:00,  2.06it/s, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.4815\n",
      "Test Metrics: Precision=0.9098, Recall=0.9098, F1=0.9098\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8757.46 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7779.84 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9219.60 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:25<00:00,  2.14it/s, loss=0.309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.7049\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:26<00:00,  2.11it/s, loss=0.11] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1144\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:26<00:00,  2.11it/s, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.3447\n",
      "Test Metrics: Precision=0.9310, Recall=0.9310, F1=0.9310\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 440, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8700.77 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7478.60 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9317.97 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:08<00:00,  6.12it/s, loss=0.409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.9913\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:09<00:00,  5.81it/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.4616\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:09<00:00,  5.94it/s, loss=0.204] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.4990\n",
      "Test Metrics: Precision=0.9059, Recall=0.9059, F1=0.9059\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 440, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8848.36 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7023.23 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9157.26 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:29<00:00,  1.89it/s, loss=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.0276\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:28<00:00,  1.92it/s, loss=0.387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.9890\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:28<00:00,  1.94it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.9977\n",
      "Test Metrics: Precision=0.9091, Recall=0.9091, F1=0.9091\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8689.67 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7351.39 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9504.97 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:26<00:00,  2.09it/s, loss=0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.7846\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:25<00:00,  2.16it/s, loss=0.322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.9716\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:26<00:00,  2.10it/s, loss=0.0786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.4614\n",
      "Test Metrics: Precision=0.9253, Recall=0.9253, F1=0.9253\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 440, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8849.72 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7052.35 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9370.71 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:09<00:00,  6.08it/s, loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.6160\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:09<00:00,  5.93it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.6365\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:09<00:00,  5.98it/s, loss=0.149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.9623\n",
      "Test Metrics: Precision=0.9090, Recall=0.9090, F1=0.9090\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 440, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8436.12 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7075.87 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9083.88 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:27<00:00,  2.00it/s, loss=0.307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.5437\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:28<00:00,  1.94it/s, loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.5182\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:27<00:00,  2.02it/s, loss=0.204] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.4860\n",
      "Test Metrics: Precision=0.9168, Recall=0.9168, F1=0.9168\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8602.42 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7238.37 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9285.57 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:25<00:00,  2.12it/s, loss=0.232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.8701\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:26<00:00,  2.06it/s, loss=0.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5197\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:25<00:00,  2.16it/s, loss=0.171] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3890\n",
      "Test Metrics: Precision=0.9307, Recall=0.9307, F1=0.9307\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 440, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8649.75 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 8149.67 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 8993.82 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:09<00:00,  5.89it/s, loss=0.448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.4720\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:09<00:00,  5.76it/s, loss=0.35] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.6007\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:09<00:00,  5.77it/s, loss=0.129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.6475\n",
      "Test Metrics: Precision=0.9136, Recall=0.9136, F1=0.9136\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 440, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8561.51 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 8047.33 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9152.15 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:29<00:00,  1.87it/s, loss=0.49] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.1323\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:28<00:00,  1.90it/s, loss=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.7811\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:29<00:00,  1.89it/s, loss=0.322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.3493\n",
      "Test Metrics: Precision=0.9139, Recall=0.9139, F1=0.9139\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8848.11 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7958.66 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9243.36 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:27<00:00,  1.97it/s, loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.2800\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:27<00:00,  2.02it/s, loss=0.121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.5002\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:28<00:00,  1.96it/s, loss=0.128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.3600\n",
      "Test Metrics: Precision=0.9342, Recall=0.9342, F1=0.9342\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 440, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8797.87 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7493.63 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9183.32 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:09<00:00,  5.89it/s, loss=0.446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.1664\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:09<00:00,  6.08it/s, loss=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.7344\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:09<00:00,  5.91it/s, loss=0.61]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.8532\n",
      "Test Metrics: Precision=0.9050, Recall=0.9050, F1=0.9050\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 440, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8715.52 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7231.42 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9098.68 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:28<00:00,  1.95it/s, loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.3670\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:28<00:00,  1.90it/s, loss=0.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.0377\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:27<00:00,  2.01it/s, loss=0.13]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0791\n",
      "Test Metrics: Precision=0.9151, Recall=0.9151, F1=0.9151\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8991.14 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7651.46 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9313.83 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:26<00:00,  2.05it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.3573\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:27<00:00,  2.02it/s, loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.4138\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:25<00:00,  2.15it/s, loss=0.148] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.6969\n",
      "Test Metrics: Precision=0.9288, Recall=0.9288, F1=0.9288\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 440, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8695.73 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 8048.56 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9106.53 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:09<00:00,  5.88it/s, loss=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 35.1405\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:09<00:00,  5.82it/s, loss=0.408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.1777\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:09<00:00,  5.93it/s, loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.9989\n",
      "Test Metrics: Precision=0.9108, Recall=0.9108, F1=0.9108\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 440, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8572.37 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 8260.57 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9028.07 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:27<00:00,  1.98it/s, loss=0.408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.4384\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:29<00:00,  1.88it/s, loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.5105\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:28<00:00,  1.91it/s, loss=0.147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.1932\n",
      "Test Metrics: Precision=0.9175, Recall=0.9175, F1=0.9175\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8658.36 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7747.34 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9162.98 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:26<00:00,  2.05it/s, loss=0.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.3465\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:26<00:00,  2.10it/s, loss=0.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.0713\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:27<00:00,  2.02it/s, loss=0.128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.3616\n",
      "Test Metrics: Precision=0.9323, Recall=0.9323, F1=0.9323\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 440, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8673.49 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 6829.60 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9146.44 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:09<00:00,  5.80it/s, loss=0.502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 37.8821\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:09<00:00,  5.64it/s, loss=0.265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.9105\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:09<00:00,  5.69it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.1776\n",
      "Test Metrics: Precision=0.9095, Recall=0.9095, F1=0.9095\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 440, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8741.61 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7103.38 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9278.32 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:29<00:00,  1.85it/s, loss=0.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.5727\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:28<00:00,  1.91it/s, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.7985\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:30<00:00,  1.83it/s, loss=0.173] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.0042\n",
      "Test Metrics: Precision=0.9209, Recall=0.9209, F1=0.9209\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8823.32 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 6958.74 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9523.82 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:28<00:00,  1.90it/s, loss=0.247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.4743\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:27<00:00,  1.99it/s, loss=0.298] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.6151\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:27<00:00,  2.00it/s, loss=0.123] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3390\n",
      "Test Metrics: Precision=0.9354, Recall=0.9354, F1=0.9354\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 440, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8790.28 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7995.03 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9008.57 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:08<00:00,  6.20it/s, loss=0.309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 34.1775\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:08<00:00,  6.19it/s, loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.9322\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:09<00:00,  5.93it/s, loss=0.279] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.8265\n",
      "Test Metrics: Precision=0.9080, Recall=0.9080, F1=0.9080\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 440, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8768.94 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7938.46 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9310.76 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:28<00:00,  1.93it/s, loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.2731\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:27<00:00,  2.02it/s, loss=0.2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.6767\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:26<00:00,  2.04it/s, loss=0.0634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0231\n",
      "Test Metrics: Precision=0.9179, Recall=0.9179, F1=0.9179\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8868.00 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 8070.91 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9554.56 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:26<00:00,  2.11it/s, loss=0.405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.3708\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:25<00:00,  2.18it/s, loss=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.5425\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:26<00:00,  2.10it/s, loss=0.169] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3965\n",
      "Test Metrics: Precision=0.9316, Recall=0.9316, F1=0.9316\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 440, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8687.54 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7966.05 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9113.04 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:09<00:00,  5.90it/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.9913\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:09<00:00,  5.81it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.9377\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:09<00:00,  5.86it/s, loss=0.2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.6826\n",
      "Test Metrics: Precision=0.9063, Recall=0.9063, F1=0.9063\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 440, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8885.47 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7982.58 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9253.70 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:28<00:00,  1.96it/s, loss=0.416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.6977\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:28<00:00,  1.91it/s, loss=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.2687\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:28<00:00,  1.94it/s, loss=0.187] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.9977\n",
      "Test Metrics: Precision=0.9173, Recall=0.9173, F1=0.9173\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 440, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 440/440 [00:00<00:00, 8635.91 examples/s]\n",
      "Map: 100%|██████████| 88/88 [00:00<00:00, 7991.92 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 9242.06 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 55/55 [00:26<00:00,  2.06it/s, loss=0.233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.7349\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 55/55 [00:25<00:00,  2.13it/s, loss=0.221] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.8288\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 55/55 [00:26<00:00,  2.08it/s, loss=0.198] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8236\n",
      "Test Metrics: Precision=0.9323, Recall=0.9323, F1=0.9323\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 460, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8716.82 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7086.27 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9318.06 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:09<00:00,  6.29it/s, loss=0.121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.0847\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:08<00:00,  6.49it/s, loss=0.415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.1194\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:09<00:00,  6.40it/s, loss=0.131] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.2270\n",
      "Test Metrics: Precision=0.9080, Recall=0.9080, F1=0.9080\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 460, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 9077.17 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7281.50 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 8914.19 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:27<00:00,  2.08it/s, loss=0.478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.9227\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:28<00:00,  2.05it/s, loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.7206\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:28<00:00,  2.03it/s, loss=0.202] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.5715\n",
      "Test Metrics: Precision=0.9189, Recall=0.9189, F1=0.9189\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8999.90 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7190.86 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9192.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:26<00:00,  2.19it/s, loss=0.394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.4796\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:25<00:00,  2.27it/s, loss=0.285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.3354\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:26<00:00,  2.21it/s, loss=0.134] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.1711\n",
      "Test Metrics: Precision=0.9306, Recall=0.9306, F1=0.9306\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 460, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 9096.64 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7332.14 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9100.26 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:09<00:00,  6.22it/s, loss=0.307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 35.1765\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:09<00:00,  6.06it/s, loss=0.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.2365\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:09<00:00,  6.22it/s, loss=0.139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.6279\n",
      "Test Metrics: Precision=0.9085, Recall=0.9085, F1=0.9085\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 460, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8674.92 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 6968.54 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9136.46 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:28<00:00,  2.02it/s, loss=0.365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.3056\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:27<00:00,  2.09it/s, loss=0.545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.7051\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:28<00:00,  2.01it/s, loss=0.212] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.1342\n",
      "Test Metrics: Precision=0.9155, Recall=0.9155, F1=0.9155\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 9236.61 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7302.72 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9401.15 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:26<00:00,  2.16it/s, loss=0.174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.2580\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:27<00:00,  2.15it/s, loss=0.0519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.1876\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:26<00:00,  2.18it/s, loss=0.155] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.9036\n",
      "Test Metrics: Precision=0.9339, Recall=0.9339, F1=0.9339\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 460, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8630.33 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7965.07 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9254.81 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:09<00:00,  5.92it/s, loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.1283\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:09<00:00,  6.02it/s, loss=0.321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.4140\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:09<00:00,  5.92it/s, loss=0.272] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.8463\n",
      "Test Metrics: Precision=0.9088, Recall=0.9088, F1=0.9088\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 460, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8800.83 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7998.09 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9315.71 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:30<00:00,  1.92it/s, loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.1030\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:30<00:00,  1.91it/s, loss=0.516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.3053\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:29<00:00,  1.95it/s, loss=0.274] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.3608\n",
      "Test Metrics: Precision=0.9185, Recall=0.9185, F1=0.9185\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8669.38 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7701.65 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9194.15 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:27<00:00,  2.08it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.9086\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:27<00:00,  2.12it/s, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.6613\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:27<00:00,  2.12it/s, loss=0.183] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2238\n",
      "Test Metrics: Precision=0.9326, Recall=0.9326, F1=0.9326\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 460, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8510.08 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7969.84 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9086.66 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:09<00:00,  6.22it/s, loss=0.455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.9042\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:09<00:00,  6.13it/s, loss=0.198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.6827\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:09<00:00,  6.02it/s, loss=0.276] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.2676\n",
      "Test Metrics: Precision=0.9076, Recall=0.9076, F1=0.9076\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 460, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8424.03 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7742.29 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9248.68 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:28<00:00,  2.06it/s, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.2510\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:29<00:00,  1.95it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.8693\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:29<00:00,  1.96it/s, loss=0.144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.7415\n",
      "Test Metrics: Precision=0.9135, Recall=0.9135, F1=0.9135\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 9081.44 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7623.14 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9374.91 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:27<00:00,  2.13it/s, loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.1766\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:26<00:00,  2.16it/s, loss=0.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.9662\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:27<00:00,  2.15it/s, loss=0.275] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7319\n",
      "Test Metrics: Precision=0.9306, Recall=0.9306, F1=0.9306\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 460, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8697.56 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7364.89 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9489.99 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:09<00:00,  5.99it/s, loss=0.319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 35.8654\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:09<00:00,  5.92it/s, loss=0.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.4300\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:09<00:00,  5.86it/s, loss=0.177] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.9423\n",
      "Test Metrics: Precision=0.9105, Recall=0.9105, F1=0.9105\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 460, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8796.42 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7106.24 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9261.65 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:29<00:00,  1.94it/s, loss=0.483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.4855\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:30<00:00,  1.91it/s, loss=0.17] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.1471\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:30<00:00,  1.90it/s, loss=0.193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.3878\n",
      "Test Metrics: Precision=0.9136, Recall=0.9136, F1=0.9136\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8956.95 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7235.90 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9562.64 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:28<00:00,  2.01it/s, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.9494\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:28<00:00,  2.05it/s, loss=0.0987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.7691\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:29<00:00,  1.98it/s, loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.6691\n",
      "Test Metrics: Precision=0.9328, Recall=0.9328, F1=0.9328\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 460, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 9010.03 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 6910.39 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 3290.48 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:09<00:00,  6.26it/s, loss=0.397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.1724\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:09<00:00,  6.17it/s, loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.0342\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:09<00:00,  6.01it/s, loss=0.236] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.0722\n",
      "Test Metrics: Precision=0.9069, Recall=0.9069, F1=0.9069\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 460, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8795.98 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 6666.71 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9356.88 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:28<00:00,  2.01it/s, loss=0.547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.2163\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:29<00:00,  1.97it/s, loss=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.1156\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:29<00:00,  1.97it/s, loss=0.0974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.4669\n",
      "Test Metrics: Precision=0.9179, Recall=0.9179, F1=0.9179\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 9071.92 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 6830.15 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9349.27 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:27<00:00,  2.12it/s, loss=0.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.7848\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:27<00:00,  2.11it/s, loss=0.366] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.3871\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:26<00:00,  2.17it/s, loss=0.144] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2042\n",
      "Test Metrics: Precision=0.9347, Recall=0.9347, F1=0.9347\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 460, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8520.64 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7562.93 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9423.79 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:09<00:00,  5.95it/s, loss=0.9]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.1862\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:09<00:00,  5.81it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 19.0527\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:09<00:00,  6.03it/s, loss=0.221] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.2876\n",
      "Test Metrics: Precision=0.9112, Recall=0.9112, F1=0.9112\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 460, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 2586.68 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7653.54 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9291.96 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:29<00:00,  1.96it/s, loss=0.654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.9270\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:30<00:00,  1.93it/s, loss=0.232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.0429\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:30<00:00,  1.90it/s, loss=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.9481\n",
      "Test Metrics: Precision=0.9188, Recall=0.9188, F1=0.9188\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8760.03 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7415.42 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9508.76 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:27<00:00,  2.08it/s, loss=0.9]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.8642\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:28<00:00,  2.02it/s, loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.7158\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:28<00:00,  2.01it/s, loss=0.235] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.6117\n",
      "Test Metrics: Precision=0.9309, Recall=0.9309, F1=0.9309\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 460, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8743.76 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7752.09 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9104.32 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:10<00:00,  5.76it/s, loss=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 35.3799\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:10<00:00,  5.72it/s, loss=0.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.2369\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:10<00:00,  5.60it/s, loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.0117\n",
      "Test Metrics: Precision=0.9106, Recall=0.9106, F1=0.9106\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 460, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8541.50 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7887.41 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9129.14 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:30<00:00,  1.88it/s, loss=0.324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.1241\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:30<00:00,  1.89it/s, loss=0.218] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.8657\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:30<00:00,  1.93it/s, loss=0.132] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.0513\n",
      "Test Metrics: Precision=0.9222, Recall=0.9222, F1=0.9222\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 9003.80 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 8037.41 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9544.17 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:28<00:00,  2.01it/s, loss=0.229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.2026\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:28<00:00,  2.03it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.1850\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:29<00:00,  1.98it/s, loss=0.176] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.9329\n",
      "Test Metrics: Precision=0.9290, Recall=0.9290, F1=0.9290\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 460, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8606.39 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 8090.66 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9093.23 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:09<00:00,  6.11it/s, loss=0.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 35.7582\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:09<00:00,  6.07it/s, loss=0.268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.2081\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:09<00:00,  5.92it/s, loss=0.147] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.0785\n",
      "Test Metrics: Precision=0.9095, Recall=0.9095, F1=0.9095\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 460, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8721.70 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 8165.13 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9109.77 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:29<00:00,  1.96it/s, loss=0.272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.0274\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:28<00:00,  2.02it/s, loss=0.262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.8555\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:29<00:00,  1.98it/s, loss=0.547] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.8378\n",
      "Test Metrics: Precision=0.9164, Recall=0.9164, F1=0.9164\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8676.13 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 8014.21 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9384.65 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:26<00:00,  2.19it/s, loss=0.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 26.6208\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:27<00:00,  2.12it/s, loss=0.191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.3540\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:26<00:00,  2.16it/s, loss=0.105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0494\n",
      "Test Metrics: Precision=0.9332, Recall=0.9332, F1=0.9332\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 460, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8413.85 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7652.32 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9293.02 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:09<00:00,  5.99it/s, loss=0.549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 34.0516\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:09<00:00,  6.15it/s, loss=0.337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.1236\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:09<00:00,  5.92it/s, loss=0.272] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.2520\n",
      "Test Metrics: Precision=0.9092, Recall=0.9092, F1=0.9092\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 460, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8327.56 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 7854.50 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9022.63 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:31<00:00,  1.86it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.7286\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:30<00:00,  1.92it/s, loss=0.222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.1485\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:29<00:00,  1.95it/s, loss=0.0804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.8795\n",
      "Test Metrics: Precision=0.9189, Recall=0.9189, F1=0.9189\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 460, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 460/460 [00:00<00:00, 8826.76 examples/s]\n",
      "Map: 100%|██████████| 92/92 [00:00<00:00, 8192.35 examples/s]\n",
      "Map: 100%|██████████| 696/696 [00:00<00:00, 9598.20 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 58/58 [00:28<00:00,  2.06it/s, loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.2771\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 58/58 [00:29<00:00,  1.99it/s, loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.0349\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 58/58 [00:27<00:00,  2.08it/s, loss=0.123] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.3687\n",
      "Test Metrics: Precision=0.9333, Recall=0.9333, F1=0.9333\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 480, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8791.32 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7433.69 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 8853.44 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:09<00:00,  6.20it/s, loss=0.379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 37.5886\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:09<00:00,  6.33it/s, loss=0.291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.8930\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:09<00:00,  6.41it/s, loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.8875\n",
      "Test Metrics: Precision=0.9053, Recall=0.9053, F1=0.9053\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 480, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8991.13 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7385.69 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9230.05 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:29<00:00,  2.02it/s, loss=0.426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.0985\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:28<00:00,  2.10it/s, loss=0.159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.4381\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:29<00:00,  2.03it/s, loss=0.183] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.6472\n",
      "Test Metrics: Precision=0.9169, Recall=0.9169, F1=0.9169\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 9009.80 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7593.93 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9387.57 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:27<00:00,  2.21it/s, loss=0.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.0112\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:27<00:00,  2.19it/s, loss=0.291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.1528\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:26<00:00,  2.23it/s, loss=0.0935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.3970\n",
      "Test Metrics: Precision=0.9301, Recall=0.9301, F1=0.9301\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 480, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8977.61 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7943.13 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 3349.82 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:09<00:00,  6.10it/s, loss=0.287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 37.7425\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:09<00:00,  6.41it/s, loss=0.209] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.4358\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:09<00:00,  6.29it/s, loss=0.135] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.6391\n",
      "Test Metrics: Precision=0.9121, Recall=0.9121, F1=0.9121\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 480, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8877.11 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7814.11 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 8885.99 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:30<00:00,  1.97it/s, loss=0.414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.1548\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:29<00:00,  2.00it/s, loss=0.331] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.1011\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:29<00:00,  2.04it/s, loss=0.225] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.5986\n",
      "Test Metrics: Precision=0.9168, Recall=0.9168, F1=0.9168\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 9274.39 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 8126.53 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9216.08 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:27<00:00,  2.17it/s, loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.0481\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:27<00:00,  2.16it/s, loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.0850\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:28<00:00,  2.14it/s, loss=0.128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0524\n",
      "Test Metrics: Precision=0.9345, Recall=0.9345, F1=0.9345\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 480, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8299.46 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7371.36 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9460.11 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:10<00:00,  5.81it/s, loss=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 35.7596\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:10<00:00,  5.67it/s, loss=0.236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.8653\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:10<00:00,  5.82it/s, loss=0.226] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.4892\n",
      "Test Metrics: Precision=0.9087, Recall=0.9087, F1=0.9087\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 480, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8370.16 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7497.36 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9594.91 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:30<00:00,  1.96it/s, loss=0.379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.2283\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:31<00:00,  1.90it/s, loss=0.325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.6535\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:31<00:00,  1.90it/s, loss=0.192] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.1545\n",
      "Test Metrics: Precision=0.9171, Recall=0.9171, F1=0.9171\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8601.72 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7268.76 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9628.44 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:29<00:00,  2.05it/s, loss=0.459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.2815\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:29<00:00,  2.04it/s, loss=0.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.4671\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:29<00:00,  2.02it/s, loss=0.167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.3248\n",
      "Test Metrics: Precision=0.9282, Recall=0.9282, F1=0.9282\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 480, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8636.70 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7324.16 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9165.34 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:09<00:00,  6.14it/s, loss=0.308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 35.0335\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:09<00:00,  6.13it/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.5001\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:09<00:00,  6.22it/s, loss=0.235] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.1261\n",
      "Test Metrics: Precision=0.9096, Recall=0.9096, F1=0.9096\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 480, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8607.19 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7158.53 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9120.41 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:30<00:00,  1.97it/s, loss=0.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.5683\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:30<00:00,  1.96it/s, loss=0.129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.4054\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:30<00:00,  2.00it/s, loss=0.105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8390\n",
      "Test Metrics: Precision=0.9230, Recall=0.9230, F1=0.9230\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8877.50 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7580.92 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9468.40 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:28<00:00,  2.13it/s, loss=0.266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.9898\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:27<00:00,  2.14it/s, loss=0.291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.7290\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:27<00:00,  2.15it/s, loss=0.153] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.8623\n",
      "Test Metrics: Precision=0.9262, Recall=0.9262, F1=0.9262\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 480, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8754.89 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 8278.06 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9228.42 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:10<00:00,  5.96it/s, loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 35.2276\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:10<00:00,  5.73it/s, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.8412\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:10<00:00,  5.72it/s, loss=0.16]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.1517\n",
      "Test Metrics: Precision=0.9163, Recall=0.9163, F1=0.9163\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 480, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8678.54 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 8152.19 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9209.37 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:31<00:00,  1.88it/s, loss=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.3074\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:30<00:00,  1.97it/s, loss=0.269] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.2279\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:31<00:00,  1.89it/s, loss=0.106] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.3012\n",
      "Test Metrics: Precision=0.9191, Recall=0.9191, F1=0.9191\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8884.47 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 8065.00 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9481.87 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:29<00:00,  2.03it/s, loss=0.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.0863\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:28<00:00,  2.09it/s, loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.8024\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:28<00:00,  2.08it/s, loss=0.113] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.5779\n",
      "Test Metrics: Precision=0.9343, Recall=0.9343, F1=0.9343\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 480, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8945.86 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 6968.97 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9130.40 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:09<00:00,  6.03it/s, loss=0.309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.8241\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:09<00:00,  6.11it/s, loss=0.513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.8937\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:09<00:00,  6.05it/s, loss=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 14.3750\n",
      "Test Metrics: Precision=0.9107, Recall=0.9107, F1=0.9107\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 480, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8895.03 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7157.39 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9095.63 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:30<00:00,  1.95it/s, loss=0.396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.8843\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:31<00:00,  1.92it/s, loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.1074\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:30<00:00,  1.95it/s, loss=0.143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9043\n",
      "Test Metrics: Precision=0.9229, Recall=0.9229, F1=0.9229\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 9035.23 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 6934.41 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9115.28 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:27<00:00,  2.18it/s, loss=0.233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.5390\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:29<00:00,  2.03it/s, loss=0.406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.2522\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:28<00:00,  2.09it/s, loss=0.169] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.7512\n",
      "Test Metrics: Precision=0.9313, Recall=0.9313, F1=0.9313\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 480, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8438.50 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 8127.02 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9098.57 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:10<00:00,  5.85it/s, loss=0.301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.3138\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:10<00:00,  5.87it/s, loss=0.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.9884\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:10<00:00,  5.81it/s, loss=0.142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.6293\n",
      "Test Metrics: Precision=0.9134, Recall=0.9134, F1=0.9134\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 480, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8540.45 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 8491.03 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9137.53 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:31<00:00,  1.89it/s, loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.6180\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:30<00:00,  1.95it/s, loss=0.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.8331\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:31<00:00,  1.93it/s, loss=0.216] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.5567\n",
      "Test Metrics: Precision=0.9189, Recall=0.9189, F1=0.9189\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8843.42 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 8390.53 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9343.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:29<00:00,  2.05it/s, loss=0.372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.5750\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:29<00:00,  2.03it/s, loss=0.189] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.2727\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:29<00:00,  2.04it/s, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4223\n",
      "Test Metrics: Precision=0.9345, Recall=0.9345, F1=0.9345\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 480, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8574.86 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7510.50 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9114.28 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:10<00:00,  5.56it/s, loss=0.455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 37.2613\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:10<00:00,  5.61it/s, loss=0.255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.1692\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:10<00:00,  5.78it/s, loss=0.192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.9939\n",
      "Test Metrics: Precision=0.9107, Recall=0.9107, F1=0.9107\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 480, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8779.17 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7627.16 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9394.05 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:32<00:00,  1.84it/s, loss=0.315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.9775\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:32<00:00,  1.86it/s, loss=0.358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 23.9057\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:32<00:00,  1.86it/s, loss=0.439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 18.6956\n",
      "Test Metrics: Precision=0.8966, Recall=0.8966, F1=0.8966\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8778.98 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 7455.71 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9375.12 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:29<00:00,  2.02it/s, loss=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 27.7118\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:28<00:00,  2.07it/s, loss=0.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 12.8233\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:30<00:00,  1.97it/s, loss=0.12]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0153\n",
      "Test Metrics: Precision=0.9341, Recall=0.9341, F1=0.9341\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 480, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 2687.93 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 8474.59 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9308.49 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:10<00:00,  5.97it/s, loss=0.455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.8993\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:09<00:00,  6.08it/s, loss=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 16.4436\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:10<00:00,  5.96it/s, loss=0.192] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.7537\n",
      "Test Metrics: Precision=0.9062, Recall=0.9062, F1=0.9062\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 480, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8547.01 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 8033.78 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9236.53 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:31<00:00,  1.93it/s, loss=0.252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.1410\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:30<00:00,  1.95it/s, loss=0.223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.7384\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:31<00:00,  1.93it/s, loss=0.237] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8607\n",
      "Test Metrics: Precision=0.9183, Recall=0.9183, F1=0.9183\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8698.60 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 8336.85 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9538.70 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:28<00:00,  2.10it/s, loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.0701\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:28<00:00,  2.10it/s, loss=0.153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1344\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:28<00:00,  2.07it/s, loss=0.154] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7075\n",
      "Test Metrics: Precision=0.9322, Recall=0.9322, F1=0.9322\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 480, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8348.78 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 8491.93 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9121.50 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:10<00:00,  5.94it/s, loss=0.388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.4145\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:10<00:00,  5.90it/s, loss=0.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.0673\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:10<00:00,  5.94it/s, loss=0.185] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.0990\n",
      "Test Metrics: Precision=0.9105, Recall=0.9105, F1=0.9105\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 480, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8352.55 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 8689.67 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9310.39 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:31<00:00,  1.90it/s, loss=0.273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.4629\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:31<00:00,  1.90it/s, loss=0.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.1685\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:31<00:00,  1.88it/s, loss=0.227] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9579\n",
      "Test Metrics: Precision=0.9218, Recall=0.9218, F1=0.9218\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 480, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 8694.77 examples/s]\n",
      "Map: 100%|██████████| 96/96 [00:00<00:00, 8708.46 examples/s]\n",
      "Map: 100%|██████████| 672/672 [00:00<00:00, 9607.34 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 60/60 [00:29<00:00,  2.01it/s, loss=0.402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.3107\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 60/60 [00:29<00:00,  2.05it/s, loss=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.1275\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 60/60 [00:28<00:00,  2.08it/s, loss=0.122] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8958\n",
      "Test Metrics: Precision=0.9351, Recall=0.9351, F1=0.9351\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 500, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8638.18 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7456.14 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9040.26 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:09<00:00,  6.45it/s, loss=0.649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 37.0915\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:09<00:00,  6.42it/s, loss=0.103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.7207\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:09<00:00,  6.34it/s, loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.6855\n",
      "Test Metrics: Precision=0.9105, Recall=0.9105, F1=0.9105\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 500, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8532.64 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7826.65 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9034.19 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:29<00:00,  2.11it/s, loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.8719\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:30<00:00,  2.06it/s, loss=0.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.0021\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:30<00:00,  2.09it/s, loss=0.12]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.8218\n",
      "Test Metrics: Precision=0.9171, Recall=0.9171, F1=0.9171\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9026.41 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7400.62 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9199.12 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:28<00:00,  2.25it/s, loss=0.515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.1275\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:27<00:00,  2.30it/s, loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.7119\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:27<00:00,  2.27it/s, loss=0.141] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.1696\n",
      "Test Metrics: Precision=0.9329, Recall=0.9329, F1=0.9329\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 500, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8917.45 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7930.99 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 8865.98 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:10<00:00,  6.26it/s, loss=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.9040\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:09<00:00,  6.40it/s, loss=0.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.6204\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:09<00:00,  6.50it/s, loss=0.369] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.9738\n",
      "Test Metrics: Precision=0.9086, Recall=0.9086, F1=0.9086\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 500, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9428.96 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8124.56 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9020.61 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:30<00:00,  2.04it/s, loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.7076\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:31<00:00,  1.98it/s, loss=0.208] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.5862\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:30<00:00,  2.04it/s, loss=0.122] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.3902\n",
      "Test Metrics: Precision=0.9167, Recall=0.9167, F1=0.9167\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9177.35 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8037.84 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9035.66 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:28<00:00,  2.22it/s, loss=0.153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.2893\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:29<00:00,  2.15it/s, loss=0.0653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.3484\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:29<00:00,  2.16it/s, loss=0.0847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.8427\n",
      "Test Metrics: Precision=0.9362, Recall=0.9362, F1=0.9362\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 500, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8329.57 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8437.04 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9283.05 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:10<00:00,  5.92it/s, loss=0.246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.1415\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:10<00:00,  5.78it/s, loss=0.177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.6167\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:10<00:00,  6.04it/s, loss=0.197] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.4641\n",
      "Test Metrics: Precision=0.9030, Recall=0.9030, F1=0.9030\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 500, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8752.65 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8402.89 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9388.06 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:32<00:00,  1.91it/s, loss=0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.4229\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:33<00:00,  1.91it/s, loss=0.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.0501\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:32<00:00,  1.97it/s, loss=0.203] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.5773\n",
      "Test Metrics: Precision=0.9130, Recall=0.9130, F1=0.9130\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8500.06 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8491.18 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9472.97 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:30<00:00,  2.07it/s, loss=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.4187\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:30<00:00,  2.09it/s, loss=0.286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.0847\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:30<00:00,  2.06it/s, loss=0.232] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.1816\n",
      "Test Metrics: Precision=0.9309, Recall=0.9309, F1=0.9309\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 500, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8590.20 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7621.30 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9088.45 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:10<00:00,  6.15it/s, loss=0.388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 38.0097\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:10<00:00,  6.16it/s, loss=0.355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.0114\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:10<00:00,  6.09it/s, loss=0.297] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.6879\n",
      "Test Metrics: Precision=0.9093, Recall=0.9093, F1=0.9093\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 500, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8761.97 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7446.61 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9102.72 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:31<00:00,  1.98it/s, loss=0.653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 37.4448\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:31<00:00,  1.99it/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 19.7947\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:31<00:00,  2.01it/s, loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 14.8879\n",
      "Test Metrics: Precision=0.9072, Recall=0.9072, F1=0.9072\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 2630.21 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7063.14 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9276.68 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:29<00:00,  2.17it/s, loss=0.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.9808\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:29<00:00,  2.13it/s, loss=0.145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.1952\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:29<00:00,  2.15it/s, loss=0.227] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.2135\n",
      "Test Metrics: Precision=0.9309, Recall=0.9309, F1=0.9309\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 500, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8965.98 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8861.27 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 8887.92 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:10<00:00,  5.94it/s, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 37.3274\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:10<00:00,  5.99it/s, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 19.1424\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:10<00:00,  5.86it/s, loss=0.143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.5738\n",
      "Test Metrics: Precision=0.9112, Recall=0.9112, F1=0.9112\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 500, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8658.44 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8665.74 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 8984.64 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:32<00:00,  1.92it/s, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.9216\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:32<00:00,  1.95it/s, loss=0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.1992\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:32<00:00,  1.93it/s, loss=0.199] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.0689\n",
      "Test Metrics: Precision=0.9189, Recall=0.9189, F1=0.9189\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9039.76 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8802.50 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9353.14 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:30<00:00,  2.03it/s, loss=0.174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.3913\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:31<00:00,  2.01it/s, loss=0.352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.4753\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:30<00:00,  2.07it/s, loss=0.0805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.8041\n",
      "Test Metrics: Precision=0.9338, Recall=0.9338, F1=0.9338\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 500, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8998.88 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7032.94 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9211.63 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:10<00:00,  6.09it/s, loss=0.587]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 37.7585\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:10<00:00,  6.05it/s, loss=0.196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.7345\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:10<00:00,  6.20it/s, loss=0.167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.6218\n",
      "Test Metrics: Precision=0.9121, Recall=0.9121, F1=0.9121\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 500, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9207.77 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7294.70 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 8965.59 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:31<00:00,  2.02it/s, loss=0.513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 34.0187\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:31<00:00,  1.99it/s, loss=0.0987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.2445\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:30<00:00,  2.04it/s, loss=0.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.1250\n",
      "Test Metrics: Precision=0.9165, Recall=0.9165, F1=0.9165\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9483.54 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7727.45 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9393.38 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:30<00:00,  2.06it/s, loss=0.221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 30.8285\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:29<00:00,  2.12it/s, loss=0.232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.1095\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:29<00:00,  2.12it/s, loss=0.149] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.6663\n",
      "Test Metrics: Precision=0.9317, Recall=0.9317, F1=0.9317\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 500, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8529.20 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 6773.53 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9416.06 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:10<00:00,  6.05it/s, loss=0.861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 37.5869\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:10<00:00,  5.83it/s, loss=0.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 19.2300\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:10<00:00,  5.90it/s, loss=0.167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.7544\n",
      "Test Metrics: Precision=0.9126, Recall=0.9126, F1=0.9126\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 500, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8505.23 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7031.64 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9665.26 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:32<00:00,  1.94it/s, loss=0.139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 32.8486\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:32<00:00,  1.92it/s, loss=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.0985\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:32<00:00,  1.94it/s, loss=0.202] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.6422\n",
      "Test Metrics: Precision=0.9188, Recall=0.9188, F1=0.9188\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8765.45 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7116.71 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9846.39 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:31<00:00,  2.03it/s, loss=0.248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.9964\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:30<00:00,  2.05it/s, loss=0.148] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.5075\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:31<00:00,  2.03it/s, loss=0.191] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.5072\n",
      "Test Metrics: Precision=0.9336, Recall=0.9336, F1=0.9336\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 500, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8583.91 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7111.40 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9482.49 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:10<00:00,  5.83it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 40.1095\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:10<00:00,  5.85it/s, loss=0.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 20.1604\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:11<00:00,  5.69it/s, loss=0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 15.3217\n",
      "Test Metrics: Precision=0.9063, Recall=0.9063, F1=0.9063\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 500, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8528.75 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 6830.78 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9223.38 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:32<00:00,  1.92it/s, loss=0.474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.1685\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:33<00:00,  1.90it/s, loss=0.456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 15.6335\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:33<00:00,  1.90it/s, loss=0.0704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.3699\n",
      "Test Metrics: Precision=0.9233, Recall=0.9233, F1=0.9233\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8542.51 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 6111.92 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9130.61 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:31<00:00,  2.03it/s, loss=0.476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 29.1134\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:31<00:00,  2.00it/s, loss=0.192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.8184\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:29<00:00,  2.10it/s, loss=0.105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 10.3055\n",
      "Test Metrics: Precision=0.9319, Recall=0.9319, F1=0.9319\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 500, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8542.23 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7868.80 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9385.76 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:10<00:00,  6.07it/s, loss=0.221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 36.2427\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:10<00:00,  6.00it/s, loss=0.196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.6444\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:10<00:00,  6.08it/s, loss=0.159] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.6928\n",
      "Test Metrics: Precision=0.9079, Recall=0.9079, F1=0.9079\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 500, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8563.75 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7772.99 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9272.31 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:32<00:00,  1.97it/s, loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 31.4131\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:32<00:00,  1.93it/s, loss=0.121] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.4020\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:32<00:00,  1.94it/s, loss=0.0973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.2112\n",
      "Test Metrics: Precision=0.9154, Recall=0.9154, F1=0.9154\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8757.03 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7721.61 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9663.92 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:29<00:00,  2.10it/s, loss=0.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.1221\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:29<00:00,  2.16it/s, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 14.9957\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:29<00:00,  2.16it/s, loss=0.0628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.9603\n",
      "Test Metrics: Precision=0.9293, Recall=0.9293, F1=0.9293\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 500, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8414.15 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7509.68 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9128.99 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:10<00:00,  5.95it/s, loss=0.719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 35.8831\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:10<00:00,  5.95it/s, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 18.7975\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:10<00:00,  5.86it/s, loss=0.442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 13.4921\n",
      "Test Metrics: Precision=0.9115, Recall=0.9115, F1=0.9115\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 500, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8378.86 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7479.54 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9407.43 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:32<00:00,  1.96it/s, loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 33.5327\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:32<00:00,  1.95it/s, loss=0.158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 17.3608\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:32<00:00,  1.92it/s, loss=0.314] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 12.8980\n",
      "Test Metrics: Precision=0.9140, Recall=0.9140, F1=0.9140\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 500, Split 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8731.91 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7726.59 examples/s]\n",
      "Map: 100%|██████████| 648/648 [00:00<00:00, 9612.85 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [00:30<00:00,  2.10it/s, loss=0.257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 28.7331\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [00:30<00:00,  2.05it/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.5108\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [00:30<00:00,  2.07it/s, loss=0.122] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.9374\n",
      "Test Metrics: Precision=0.9363, Recall=0.9363, F1=0.9363\n",
      "Results saved to Experiments_moreksplits10_lesssteps20_for_smoother_graphh.xlsx\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"small\": \"bert-base-cased\",\n",
    "    \"medium\": \"bert-large-cased\",\n",
    "    \"large\": \"roberta-large\"\n",
    "}\n",
    "\n",
    "iterate_and_finetune_with_torch(dataset=dataset, file_name='Experiments_moreksplits10_lesssteps20_for_smoother_graphh.xlsx', models=models, start_size=360, end_size=500, step_size=20, k_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35a2457",
   "metadata": {},
   "source": [
    "### Lipitor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f040571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 999/999 [00:00<00:00, 24829.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "lipitor_dataset_path = \"output_datasets/LIPITOR_combined_output.txt\"\n",
    "dataset_dict_lip = create_dataset_from_final_file(lipitor_dataset_path)\n",
    "dataset_lip = dataset_dict_lip['full_data']\n",
    "\n",
    "label_names_lip = dataset_lip.features[\"ner_tags\"].feature.names\n",
    "id2label = {i: label for i, label in enumerate(label_names_lip)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60ad662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_and_finetune_with_torch_lip(\n",
    "    dataset,\n",
    "    file_name,\n",
    "    models,\n",
    "    start_size=5,\n",
    "    end_size=500,\n",
    "    step_size=5,\n",
    "    k_splits=5,\n",
    "    batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.0,\n",
    "    num_epochs=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune models with varying dataset sizes and k-fold splits, saving results to Excel.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (DatasetDict): Dataset for training, validation, and testing.\n",
    "    - file_name (str): Excel file to save results.\n",
    "    - models (dict): Dictionary of model names and their sizes.\n",
    "    - start_size (int): Starting size for training datasets.\n",
    "    - end_size (int): Maximum size for training datasets.\n",
    "    - step_size (int): Step size for increasing dataset sizes.\n",
    "    - k_splits (int): Number of k-fold splits.\n",
    "    - batch_size (int): Training batch size.\n",
    "    - learning_rate (float): Learning rate for fine-tuning.\n",
    "    - weight_decay (float): Weight decay for optimizer.\n",
    "    - num_epochs (int): Number of training epochs.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Check or create the results file\n",
    "    if os.path.exists(file_name):\n",
    "        results_df = pd.read_excel(file_name)\n",
    "    else:\n",
    "        results_df = pd.DataFrame(columns=[\"Train Size\", \"K-Fold\", \"Test F1\", \"Model\"])\n",
    "\n",
    "    for train_size in range(start_size, end_size + 1, step_size):\n",
    "        for split in range(k_splits):\n",
    "            for size, model_name in models.items():\n",
    "                print(f\"\\nFine-tuning {model_name} ({size}) with Train Size {train_size}, Split {split + 1}...\")\n",
    "\n",
    "                # Initialize tokenizer\n",
    "                if size == \"large\":\n",
    "                    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-large\", add_prefix_space=True)\n",
    "                else:\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "                data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "                # Generate datasets\n",
    "                train_datasets = generate_train_datasets(\n",
    "                    dataset, number_of_samples=train_size, number_of_splits=k_splits\n",
    "                )\n",
    "                _, train_dataset, train_indices = train_datasets[split]\n",
    "\n",
    "                val_datasets = generate_validation_datasets(\n",
    "                    dataset, train_indices=train_indices, number_of_samples=train_size, number_of_splits=k_splits\n",
    "                )\n",
    "                _, val_dataset, val_indices = val_datasets[split]\n",
    "\n",
    "                test_datasets = generate_test_datasets(\n",
    "                    dataset, train_indices=train_indices, val_indices=val_indices,\n",
    "                    number_of_samples=train_size, number_of_splits=k_splits\n",
    "                )\n",
    "                _, test_dataset = test_datasets[split]\n",
    "\n",
    "                def align_labels_with_tokens(labels_, word_ids):\n",
    "                    new_labels = []\n",
    "                    current_word = None\n",
    "\n",
    "                    for word_id in word_ids:\n",
    "                        if word_id != current_word:\n",
    "                            current_word = word_id\n",
    "                            label = -100 if word_id is None else labels_[word_id]\n",
    "                            new_labels.append(label)\n",
    "\n",
    "                        elif word_id is None:\n",
    "                            # Special token\n",
    "                            new_labels.append(-100)\n",
    "\n",
    "                        else:\n",
    "                            # Same word as previous token\n",
    "                            label = labels_[word_id]\n",
    "\n",
    "                            # If the label is B-XXX we change it to I-XXX\n",
    "                            if label % 2 == 1:\n",
    "                                label += 1\n",
    "                            new_labels.append(label)\n",
    "\n",
    "                    return new_labels\n",
    "\n",
    "\n",
    "                def tokenize_and_align_labels(examples):\n",
    "                    tokenized_inputs = tokenizer(\n",
    "                        examples[\"tokens\"], truncation=True,\n",
    "                        is_split_into_words=True\n",
    "                    )\n",
    "                    all_labels = examples[\"ner_tags\"]\n",
    "                    new_labels = []\n",
    "                    for i, labels_ in enumerate(all_labels):\n",
    "                        word_ids = tokenized_inputs.word_ids(i)\n",
    "                        new_labels.append(align_labels_with_tokens(labels_, word_ids))\n",
    "\n",
    "                    tokenized_inputs[\"labels\"] = new_labels\n",
    "                    return tokenized_inputs\n",
    "\n",
    "                # Tokenize datasets\n",
    "                tokenized_train = train_dataset.map(tokenize_and_align_labels, batched=True,  remove_columns=dataset.column_names)\n",
    "                tokenized_val = val_dataset.map(tokenize_and_align_labels, batched=True,  remove_columns=dataset.column_names)\n",
    "                tokenized_test = test_dataset.map(tokenize_and_align_labels, batched=True,  remove_columns=dataset.column_names)\n",
    "\n",
    "                train_dataloader = DataLoader(tokenized_train, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "                val_dataloader = DataLoader(tokenized_val, batch_size=batch_size, collate_fn=data_collator)\n",
    "                test_dataloader = DataLoader(tokenized_test, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "                # Initialize the model for token classification\n",
    "                model = AutoModelForTokenClassification.from_pretrained(\n",
    "                    model_name, id2label=id2label, label2id=label2id\n",
    "                )\n",
    "                \n",
    "                #Optimize GPU ram at cost of some speed\n",
    "                model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "                num_training_steps = num_epochs * len(train_dataloader)\n",
    "                lr_scheduler = get_scheduler(\n",
    "                    \"linear\",\n",
    "                    optimizer=optimizer,\n",
    "                    num_warmup_steps=0,\n",
    "                    num_training_steps=num_training_steps\n",
    "                )\n",
    "\n",
    "                accelerator = Accelerator()\n",
    "                model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "                    model, optimizer, train_dataloader, val_dataloader\n",
    "                )\n",
    "\n",
    "\n",
    "                # Training loop\n",
    "                for epoch in range(num_epochs):\n",
    "                    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "                    model.train()\n",
    "                    total_loss = 0\n",
    "                    progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\")\n",
    "                    for batch in progress_bar:\n",
    "                        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                        outputs = model(**batch)\n",
    "                        loss = outputs.loss\n",
    "                        total_loss += loss.item()\n",
    "\n",
    "                        accelerator.backward(loss)\n",
    "                        optimizer.step()\n",
    "                        lr_scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "                    print(f\"Epoch {epoch + 1} Loss: {total_loss:.4f}\")\n",
    "\n",
    "                # Validation loop\n",
    "                model.eval()\n",
    "                val_predictions, val_labels = [], []\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_dataloader:\n",
    "                        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                        outputs = model(**batch)\n",
    "                        logits = outputs.logits\n",
    "                        predictions = logits.argmax(dim=-1)\n",
    "                        labels = batch[\"labels\"]\n",
    "\n",
    "                        predictions = accelerator.gather(predictions)\n",
    "                        labels = accelerator.gather(labels)\n",
    "\n",
    "                        flat_labels, flat_predictions = postprocess(predictions, labels)\n",
    "                        val_labels.extend(flat_labels)\n",
    "                        val_predictions.extend(flat_predictions)\n",
    "\n",
    "                # Test loop\n",
    "                test_predictions, test_labels = [], []\n",
    "                with torch.no_grad():\n",
    "                    for batch in test_dataloader:\n",
    "                        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                        outputs = model(**batch)\n",
    "                        logits = outputs.logits\n",
    "                        predictions = logits.argmax(dim=-1)\n",
    "                        labels = batch[\"labels\"]\n",
    "\n",
    "                        predictions = accelerator.gather(predictions)\n",
    "                        labels = accelerator.gather(labels)\n",
    "\n",
    "                        flat_labels, flat_predictions = postprocess(predictions, labels)\n",
    "                        test_labels.extend(flat_labels)\n",
    "                        test_predictions.extend(flat_predictions)\n",
    "\n",
    "                # Calculate test metrics\n",
    "                precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                    test_labels, test_predictions, average=\"micro\"\n",
    "                )\n",
    "                print(f\"Test Metrics: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "                # Append results\n",
    "                new_row = pd.DataFrame(\n",
    "                    [{\"Train Size\": train_size, \"K-Fold\": split + 1, \"Test F1\": f1, \"Model\": model_name}]\n",
    "                )\n",
    "                results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "                results_df.to_excel(file_name, index=False)\n",
    "\n",
    "                # Cleanup\n",
    "                del model, train_dataloader, val_dataloader, test_dataloader, predictions, labels, test_labels\n",
    "                del test_predictions, logits, loss, outputs, precision, recall, f1, new_row \n",
    "                del val_dataset, train_dataset, test_dataset, val_datasets, train_datasets, test_datasets\n",
    "                del optimizer, tokenized_train, tokenized_test, tokenized_val, tokenizer\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Results saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6b477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 5, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1133.66 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 675.09 examples/s]\n",
      "Map: 100%|██████████| 993/993 [00:00<00:00, 8831.33 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  3.91it/s, loss=2.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.7308\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  9.52it/s, loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.0692\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  9.39it/s, loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.6527\n",
      "Test Metrics: Precision=0.7757, Recall=0.7757, F1=0.7757\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 5, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_150307/664759756.py:206: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1585.99 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 683.89 examples/s]\n",
      "Map: 100%|██████████| 993/993 [00:00<00:00, 8937.36 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s, loss=2.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.6225\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s, loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.2542\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  3.23it/s, loss=0.857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.8575\n",
      "Test Metrics: Precision=0.7756, Recall=0.7756, F1=0.7756\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 5, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1449.61 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 479.18 examples/s]\n",
      "Map: 100%|██████████| 993/993 [00:00<00:00, 8967.01 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s, loss=2.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.7657\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s, loss=1.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.6449\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s, loss=0.929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.9289\n",
      "Test Metrics: Precision=0.7877, Recall=0.7877, F1=0.7877\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 5, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1407.11 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 759.29 examples/s]\n",
      "Map: 100%|██████████| 993/993 [00:00<00:00, 8735.77 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  5.06it/s, loss=2.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.5098\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  5.09it/s, loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.8105\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  5.13it/s, loss=1.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.3619\n",
      "Test Metrics: Precision=0.7756, Recall=0.7756, F1=0.7756\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 5, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1457.87 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 727.42 examples/s]\n",
      "Map: 100%|██████████| 993/993 [00:00<00:00, 8644.91 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s, loss=2.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.5612\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.1738\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s, loss=0.891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.8914\n",
      "Test Metrics: Precision=0.7735, Recall=0.7735, F1=0.7735\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 5, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1377.98 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 686.13 examples/s]\n",
      "Map: 100%|██████████| 993/993 [00:00<00:00, 9190.10 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s, loss=3.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.0321\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s, loss=2.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.0979\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s, loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.5265\n",
      "Test Metrics: Precision=0.7847, Recall=0.7847, F1=0.7847\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 5, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 2196.43 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 797.55 examples/s]\n",
      "Map: 100%|██████████| 993/993 [00:00<00:00, 9053.66 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s, loss=2.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.4422\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00, 13.75it/s, loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.6627\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00, 13.72it/s, loss=1.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.2151\n",
      "Test Metrics: Precision=0.7766, Recall=0.7766, F1=0.7766\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 5, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 2047.20 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 716.98 examples/s]\n",
      "Map: 100%|██████████| 993/993 [00:00<00:00, 8905.18 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  4.31it/s, loss=2.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.4925\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  4.43it/s, loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.1581\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  4.49it/s, loss=0.795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.7955\n",
      "Test Metrics: Precision=0.7732, Recall=0.7732, F1=0.7732\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 5, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1896.50 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 585.47 examples/s]\n",
      "Map: 100%|██████████| 993/993 [00:00<00:00, 8982.12 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  4.44it/s, loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.5533\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  4.52it/s, loss=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.0187\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  4.33it/s, loss=0.833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.8329\n",
      "Test Metrics: Precision=0.7880, Recall=0.7880, F1=0.7880\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 5, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1991.98 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 730.33 examples/s]\n",
      "Map: 100%|██████████| 993/993 [00:00<00:00, 8654.30 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00, 12.85it/s, loss=2.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.5768\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00, 14.16it/s, loss=1.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.9870\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00, 14.20it/s, loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.6660\n",
      "Test Metrics: Precision=0.7679, Recall=0.7679, F1=0.7679\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 5, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 2044.41 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 788.55 examples/s]\n",
      "Map: 100%|██████████| 993/993 [00:00<00:00, 4244.33 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  4.42it/s, loss=2.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.4641\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s, loss=1.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.2998\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  4.57it/s, loss=0.989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.9889\n",
      "Test Metrics: Precision=0.7791, Recall=0.7791, F1=0.7791\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 5, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1737.35 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 582.95 examples/s]\n",
      "Map: 100%|██████████| 993/993 [00:00<00:00, 9075.53 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  4.44it/s, loss=2.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.2835\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  4.63it/s, loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.2862\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s, loss=0.982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.9817\n",
      "Test Metrics: Precision=0.7874, Recall=0.7874, F1=0.7874\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 5, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1514.95 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 638.40 examples/s]\n",
      "Map: 100%|██████████| 993/993 [00:00<00:00, 8894.45 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  6.61it/s, loss=2.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.4843\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  6.95it/s, loss=1.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.8688\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  6.91it/s, loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.5118\n",
      "Test Metrics: Precision=0.7761, Recall=0.7761, F1=0.7761\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 5, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1678.39 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 650.58 examples/s]\n",
      "Map: 100%|██████████| 993/993 [00:00<00:00, 8797.81 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  2.04it/s, loss=2.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.5908\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s, loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.2927\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s, loss=0.927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.9268\n",
      "Test Metrics: Precision=0.7752, Recall=0.7752, F1=0.7752\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 5, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1452.52 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 619.54 examples/s]\n",
      "Map: 100%|██████████| 993/993 [00:00<00:00, 9125.30 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s, loss=2.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.3975\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s, loss=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.5893\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.0277\n",
      "Test Metrics: Precision=0.7883, Recall=0.7883, F1=0.7883\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 10, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2714.06 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 1047.01 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:00<00:00, 8811.34 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  7.58it/s, loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.8909\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  7.53it/s, loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.7427\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  8.33it/s, loss=1.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.0650\n",
      "Test Metrics: Precision=0.7761, Recall=0.7761, F1=0.7761\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 10, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2777.13 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 1063.60 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:00<00:00, 9072.63 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  2.59it/s, loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.6080\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  2.44it/s, loss=0.595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.5898\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  2.43it/s, loss=0.942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.7217\n",
      "Test Metrics: Precision=0.7763, Recall=0.7763, F1=0.7763\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 10, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2634.61 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 1080.45 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:00<00:00, 9066.35 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  2.54it/s, loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.3965\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  2.56it/s, loss=0.665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.7288\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  2.71it/s, loss=0.726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.5352\n",
      "Test Metrics: Precision=0.7878, Recall=0.7878, F1=0.7878\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 10, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2642.91 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 1013.61 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:00<00:00, 9058.34 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  5.75it/s, loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.8156\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  6.26it/s, loss=0.971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.1389\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  6.20it/s, loss=0.695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.5224\n",
      "Test Metrics: Precision=0.7763, Recall=0.7763, F1=0.7763\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 10, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2544.01 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 923.14 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:00<00:00, 8677.65 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  2.04it/s, loss=1.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.1420\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  2.63it/s, loss=0.618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.5515\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  2.00it/s, loss=1.08] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.7196\n",
      "Test Metrics: Precision=0.7763, Recall=0.7763, F1=0.7763\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 10, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2516.53 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 860.02 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:00<00:00, 9068.00 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  2.21it/s, loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.7566\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  2.27it/s, loss=0.749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.6168\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  2.25it/s, loss=0.48] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.3327\n",
      "Test Metrics: Precision=0.7870, Recall=0.7870, F1=0.7870\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 10, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 3636.47 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 1145.52 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:00<00:00, 8836.41 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 11.64it/s, loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.7335\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 12.01it/s, loss=1.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.0225\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 12.11it/s, loss=1.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.2541\n",
      "Test Metrics: Precision=0.7765, Recall=0.7765, F1=0.7765\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 10, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 3753.63 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 1168.98 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:00<00:00, 8807.83 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  3.82it/s, loss=1.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.8392\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  3.92it/s, loss=0.845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.6298\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  4.58it/s, loss=0.536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.2509\n",
      "Test Metrics: Precision=0.7765, Recall=0.7765, F1=0.7765\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 10, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 3322.48 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 1109.16 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:00<00:00, 9143.31 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  4.62it/s, loss=1.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.9051\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  4.18it/s, loss=0.738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.6248\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  4.20it/s, loss=0.696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.3130\n",
      "Test Metrics: Precision=0.7878, Recall=0.7878, F1=0.7878\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 10, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2884.67 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 831.87 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:00<00:00, 8672.02 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  7.71it/s, loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.8876\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  8.42it/s, loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.3225\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  8.64it/s, loss=1.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.0183\n",
      "Test Metrics: Precision=0.7770, Recall=0.7770, F1=0.7770\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 10, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2914.94 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 881.43 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:00<00:00, 8980.62 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  2.57it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.5275\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  2.60it/s, loss=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.5819\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  2.79it/s, loss=0.557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.3860\n",
      "Test Metrics: Precision=0.7821, Recall=0.7821, F1=0.7821\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 10, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2597.57 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 768.12 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:00<00:00, 9136.73 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  2.96it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.5305\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  3.10it/s, loss=1.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.9921\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  2.99it/s, loss=0.498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.1242\n",
      "Test Metrics: Precision=0.7879, Recall=0.7879, F1=0.7879\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 10, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2639.42 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 1297.34 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:00<00:00, 8817.42 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  8.16it/s, loss=1.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.0582\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  7.96it/s, loss=1.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.3897\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  7.57it/s, loss=1.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.9335\n",
      "Test Metrics: Precision=0.7761, Recall=0.7761, F1=0.7761\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 10, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2644.25 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 1287.19 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:00<00:00, 9031.95 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  2.56it/s, loss=1.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.6051\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s, loss=0.838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.7602\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  2.62it/s, loss=0.836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.6321\n",
      "Test Metrics: Precision=0.7771, Recall=0.7771, F1=0.7771\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 10, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2463.90 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 1162.98 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:00<00:00, 9067.86 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  2.73it/s, loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.1358\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  2.82it/s, loss=0.794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.9707\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  2.74it/s, loss=0.791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.4852\n",
      "Test Metrics: Precision=0.7880, Recall=0.7880, F1=0.7880\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 15, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3677.06 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1383.35 examples/s]\n",
      "Map: 100%|██████████| 981/981 [00:00<00:00, 8923.47 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  5.84it/s, loss=2.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.6338\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  5.50it/s, loss=1.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.8448\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  5.70it/s, loss=0.902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.0607\n",
      "Test Metrics: Precision=0.7767, Recall=0.7767, F1=0.7767\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 15, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3496.03 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1387.01 examples/s]\n",
      "Map: 100%|██████████| 981/981 [00:00<00:00, 4416.43 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:01<00:00,  1.92it/s, loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.0541\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:01<00:00,  1.81it/s, loss=0.98] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.8882\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:01<00:00,  1.75it/s, loss=0.807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.6805\n",
      "Test Metrics: Precision=0.7768, Recall=0.7768, F1=0.7768\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 15, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3217.97 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1247.93 examples/s]\n",
      "Map: 100%|██████████| 981/981 [00:00<00:00, 9111.41 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:01<00:00,  1.82it/s, loss=1.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.4904\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:01<00:00,  1.89it/s, loss=1.15] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.0800\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:01<00:00,  1.92it/s, loss=0.777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.5682\n",
      "Test Metrics: Precision=0.7878, Recall=0.7878, F1=0.7878\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 15, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3101.23 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1111.07 examples/s]\n",
      "Map: 100%|██████████| 981/981 [00:00<00:00, 8659.81 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  4.83it/s, loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.1962\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  4.97it/s, loss=1.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.3714\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  5.12it/s, loss=0.828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.7995\n",
      "Test Metrics: Precision=0.7765, Recall=0.7765, F1=0.7765\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 15, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3561.94 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1222.83 examples/s]\n",
      "Map: 100%|██████████| 981/981 [00:00<00:00, 8785.84 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:01<00:00,  1.56it/s, loss=1.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.7709\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:01<00:00,  1.61it/s, loss=0.792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.7560\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:01<00:00,  1.61it/s, loss=0.78] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.6140\n",
      "Test Metrics: Precision=0.7765, Recall=0.7765, F1=0.7765\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 15, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3421.31 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 965.24 examples/s]\n",
      "Map: 100%|██████████| 981/981 [00:00<00:00, 9203.30 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:01<00:00,  1.75it/s, loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.9911\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:01<00:00,  1.78it/s, loss=0.979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.8766\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:01<00:00,  1.75it/s, loss=0.641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.3183\n",
      "Test Metrics: Precision=0.7894, Recall=0.7894, F1=0.7894\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 15, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15/15 [00:00<00:00, 4252.71 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1339.46 examples/s]\n",
      "Map: 100%|██████████| 981/981 [00:00<00:00, 8498.23 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  7.70it/s, loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.1231\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  9.09it/s, loss=1.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.3400\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  7.69it/s, loss=1.05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.8511\n",
      "Test Metrics: Precision=0.7769, Recall=0.7769, F1=0.7769\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 15, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15/15 [00:00<00:00, 4572.61 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1404.50 examples/s]\n",
      "Map: 100%|██████████| 981/981 [00:00<00:00, 8583.82 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  2.54it/s, loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.9819\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  3.00it/s, loss=0.873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.8012\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  3.01it/s, loss=0.834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.6668\n",
      "Test Metrics: Precision=0.7775, Recall=0.7775, F1=0.7775\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 15, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15/15 [00:00<00:00, 4182.59 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1157.26 examples/s]\n",
      "Map: 100%|██████████| 981/981 [00:00<00:00, 9256.76 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  3.17it/s, loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.2177\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  3.20it/s, loss=1.08] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.0182\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  2.79it/s, loss=0.69] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.6116\n",
      "Test Metrics: Precision=0.7882, Recall=0.7882, F1=0.7882\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 15, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3852.93 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1265.63 examples/s]\n",
      "Map: 100%|██████████| 981/981 [00:00<00:00, 8796.12 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  6.70it/s, loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.2331\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  5.89it/s, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.1839\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  7.15it/s, loss=0.891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.7457\n",
      "Test Metrics: Precision=0.7767, Recall=0.7767, F1=0.7767\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 15, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3756.77 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1313.46 examples/s]\n",
      "Map: 100%|██████████| 981/981 [00:00<00:00, 8976.95 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:01<00:00,  1.91it/s, loss=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.7072\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:01<00:00,  1.89it/s, loss=0.917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.7858\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:01<00:00,  1.94it/s, loss=0.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.5800\n",
      "Test Metrics: Precision=0.7766, Recall=0.7766, F1=0.7766\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 15, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3678.14 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1169.52 examples/s]\n",
      "Map: 100%|██████████| 981/981 [00:00<00:00, 9167.64 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:01<00:00,  2.00it/s, loss=1.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.5088\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  2.49it/s, loss=1.35] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.1748\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  2.01it/s, loss=0.586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.4699\n",
      "Test Metrics: Precision=0.7876, Recall=0.7876, F1=0.7876\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 15, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3824.59 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1316.89 examples/s]\n",
      "Map: 100%|██████████| 981/981 [00:00<00:00, 8775.56 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  5.67it/s, loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.3328\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  5.40it/s, loss=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.5071\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  5.78it/s, loss=0.921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.9179\n",
      "Test Metrics: Precision=0.7772, Recall=0.7772, F1=0.7772\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 15, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3672.77 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1349.95 examples/s]\n",
      "Map: 100%|██████████| 981/981 [00:00<00:00, 9035.06 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:01<00:00,  1.79it/s, loss=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.6150\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:01<00:00,  1.88it/s, loss=0.901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.8851\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:01<00:00,  1.74it/s, loss=0.886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.7147\n",
      "Test Metrics: Precision=0.7772, Recall=0.7772, F1=0.7772\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 15, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3412.41 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1159.72 examples/s]\n",
      "Map: 100%|██████████| 981/981 [00:00<00:00, 9086.82 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s, loss=1.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.4532\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  2.27it/s, loss=1.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.1162\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:01<00:00,  1.89it/s, loss=0.95] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.7980\n",
      "Test Metrics: Precision=0.7812, Recall=0.7812, F1=0.7812\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 20, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 3946.84 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 1609.48 examples/s]\n",
      "Map: 100%|██████████| 975/975 [00:00<00:00, 8926.68 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s, loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.8139\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3/3 [00:00<00:00,  6.48it/s, loss=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.2788\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3/3 [00:00<00:00,  6.53it/s, loss=0.656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.5293\n",
      "Test Metrics: Precision=0.7767, Recall=0.7767, F1=0.7767\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 20, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 3973.76 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 1322.19 examples/s]\n",
      "Map: 100%|██████████| 975/975 [00:00<00:00, 8645.71 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3/3 [00:01<00:00,  2.02it/s, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.6949\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  2.00it/s, loss=0.719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.3505\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3/3 [00:01<00:00,  2.07it/s, loss=0.652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.9175\n",
      "Test Metrics: Precision=0.7821, Recall=0.7821, F1=0.7821\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 20, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 3989.07 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 1274.96 examples/s]\n",
      "Map: 100%|██████████| 975/975 [00:00<00:00, 8992.52 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3/3 [00:01<00:00,  2.23it/s, loss=0.875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.3140\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  2.79it/s, loss=0.826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.5813\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3/3 [00:01<00:00,  2.24it/s, loss=0.608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.9441\n",
      "Test Metrics: Precision=0.7879, Recall=0.7879, F1=0.7879\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 20, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4324.69 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 1450.81 examples/s]\n",
      "Map: 100%|██████████| 975/975 [00:00<00:00, 8953.52 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3/3 [00:00<00:00,  6.49it/s, loss=1.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.6741\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3/3 [00:00<00:00,  5.71it/s, loss=1.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.0706\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3/3 [00:00<00:00,  5.65it/s, loss=1.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.9134\n",
      "Test Metrics: Precision=0.7771, Recall=0.7771, F1=0.7771\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 20, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4329.38 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 1403.83 examples/s]\n",
      "Map: 100%|██████████| 975/975 [00:00<00:00, 8811.81 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3/3 [00:01<00:00,  1.98it/s, loss=1.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.2517\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s, loss=0.719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.4145\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3/3 [00:01<00:00,  2.00it/s, loss=0.761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.0900\n",
      "Test Metrics: Precision=0.7787, Recall=0.7787, F1=0.7787\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 20, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 3952.97 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 1208.12 examples/s]\n",
      "Map: 100%|██████████| 975/975 [00:00<00:00, 8957.40 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3/3 [00:01<00:00,  2.02it/s, loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.6569\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  2.19it/s, loss=0.655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.8044\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3/3 [00:01<00:00,  2.36it/s, loss=0.605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.9328\n",
      "Test Metrics: Precision=0.8170, Recall=0.8170, F1=0.8170\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 20, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4735.31 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 1734.98 examples/s]\n",
      "Map: 100%|██████████| 975/975 [00:00<00:00, 8754.73 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3/3 [00:00<00:00,  8.40it/s, loss=1.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.8302\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3/3 [00:00<00:00,  7.50it/s, loss=0.693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.8648\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3/3 [00:00<00:00,  8.51it/s, loss=0.851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.6139\n",
      "Test Metrics: Precision=0.7769, Recall=0.7769, F1=0.7769\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 20, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4356.81 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 1803.23 examples/s]\n",
      "Map: 100%|██████████| 975/975 [00:00<00:00, 8696.07 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3/3 [00:00<00:00,  3.02it/s, loss=0.842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.6296\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  2.78it/s, loss=0.92] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.6265\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3/3 [00:01<00:00,  2.60it/s, loss=0.711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.1388\n",
      "Test Metrics: Precision=0.7770, Recall=0.7770, F1=0.7770\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 20, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4498.15 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 1530.49 examples/s]\n",
      "Map: 100%|██████████| 975/975 [00:00<00:00, 8813.86 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3/3 [00:01<00:00,  2.59it/s, loss=0.968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.8231\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  2.56it/s, loss=0.851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.7695\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3/3 [00:01<00:00,  2.51it/s, loss=0.724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.2122\n",
      "Test Metrics: Precision=0.8017, Recall=0.8017, F1=0.8017\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 20, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4082.25 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 2006.84 examples/s]\n",
      "Map: 100%|██████████| 975/975 [00:00<00:00, 8526.13 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3/3 [00:00<00:00,  8.31it/s, loss=1.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.5592\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3/3 [00:00<00:00,  6.57it/s, loss=0.711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.7976\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3/3 [00:00<00:00,  8.24it/s, loss=0.854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.5721\n",
      "Test Metrics: Precision=0.7765, Recall=0.7765, F1=0.7765\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 20, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4162.46 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 2196.55 examples/s]\n",
      "Map: 100%|██████████| 975/975 [00:00<00:00, 8718.57 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3/3 [00:01<00:00,  2.26it/s, loss=1.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.3474\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  2.37it/s, loss=0.997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.5941\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3/3 [00:01<00:00,  2.52it/s, loss=0.581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.0556\n",
      "Test Metrics: Precision=0.7765, Recall=0.7765, F1=0.7765\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 20, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 3965.31 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 1820.05 examples/s]\n",
      "Map: 100%|██████████| 975/975 [00:00<00:00, 9067.41 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3/3 [00:01<00:00,  2.26it/s, loss=0.566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.6749\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  2.80it/s, loss=0.61] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.6911\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3/3 [00:01<00:00,  2.69it/s, loss=0.541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.8118\n",
      "Test Metrics: Precision=0.7950, Recall=0.7950, F1=0.7950\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 20, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4318.46 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 1926.87 examples/s]\n",
      "Map: 100%|██████████| 975/975 [00:00<00:00, 8610.39 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3/3 [00:00<00:00,  6.59it/s, loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.4239\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3/3 [00:00<00:00,  7.06it/s, loss=0.897]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.9414\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3/3 [00:00<00:00,  7.00it/s, loss=0.858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.6266\n",
      "Test Metrics: Precision=0.7773, Recall=0.7773, F1=0.7773\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 20, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4214.96 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 1993.02 examples/s]\n",
      "Map: 100%|██████████| 975/975 [00:00<00:00, 8655.06 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3/3 [00:01<00:00,  2.06it/s, loss=0.887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.8177\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  2.08it/s, loss=0.885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.7312\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3/3 [00:01<00:00,  2.18it/s, loss=0.544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.9940\n",
      "Test Metrics: Precision=0.7806, Recall=0.7806, F1=0.7806\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 20, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4196.40 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 1841.02 examples/s]\n",
      "Map: 100%|██████████| 975/975 [00:00<00:00, 4493.20 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3/3 [00:01<00:00,  2.35it/s, loss=0.697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.1592\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  2.29it/s, loss=0.751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.4642\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3/3 [00:01<00:00,  2.36it/s, loss=0.685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.9576\n",
      "Test Metrics: Precision=0.8092, Recall=0.8092, F1=0.8092\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 25, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4775.37 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 2169.62 examples/s]\n",
      "Map: 100%|██████████| 969/969 [00:00<00:00, 8549.83 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:00<00:00,  7.14it/s, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.6196\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:00<00:00,  7.46it/s, loss=0.895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.6071\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:00<00:00,  6.95it/s, loss=0.482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.9084\n",
      "Test Metrics: Precision=0.7762, Recall=0.7762, F1=0.7762\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 25, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4997.26 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 2228.17 examples/s]\n",
      "Map: 100%|██████████| 969/969 [00:00<00:00, 8821.72 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:01<00:00,  2.36it/s, loss=1.62] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.3205\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:01<00:00,  2.37it/s, loss=1.39] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.6840\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.34it/s, loss=0.491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.3949\n",
      "Test Metrics: Precision=0.7910, Recall=0.7910, F1=0.7910\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 25, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4618.87 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1775.14 examples/s]\n",
      "Map: 100%|██████████| 969/969 [00:00<00:00, 9088.30 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:01<00:00,  2.73it/s, loss=0.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.6200\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:01<00:00,  2.41it/s, loss=1.09] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.1512\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.58it/s, loss=0.853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.5312\n",
      "Test Metrics: Precision=0.8305, Recall=0.8305, F1=0.8305\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 25, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4890.97 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1771.09 examples/s]\n",
      "Map: 100%|██████████| 969/969 [00:00<00:00, 8923.44 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:00<00:00,  6.73it/s, loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.9985\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:00<00:00,  6.49it/s, loss=1.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.1314\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:00<00:00,  6.59it/s, loss=0.859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.4293\n",
      "Test Metrics: Precision=0.7764, Recall=0.7764, F1=0.7764\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 25, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4937.03 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1828.54 examples/s]\n",
      "Map: 100%|██████████| 969/969 [00:00<00:00, 8787.04 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:01<00:00,  2.10it/s, loss=1.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.7416\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:01<00:00,  2.12it/s, loss=0.346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.6591\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.28it/s, loss=1.17] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.9877\n",
      "Test Metrics: Precision=0.7867, Recall=0.7867, F1=0.7867\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 25, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4682.40 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1667.58 examples/s]\n",
      "Map: 100%|██████████| 969/969 [00:00<00:00, 9142.77 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:01<00:00,  2.31it/s, loss=1.15] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.9249\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:01<00:00,  2.26it/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.3069\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.24it/s, loss=0.17] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.8255\n",
      "Test Metrics: Precision=0.8091, Recall=0.8091, F1=0.8091\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 25, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25/25 [00:00<00:00, 5427.13 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1680.68 examples/s]\n",
      "Map: 100%|██████████| 969/969 [00:00<00:00, 8924.27 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:00<00:00,  8.28it/s, loss=0.991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.5549\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:00<00:00, 10.34it/s, loss=0.642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.5658\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:00<00:00,  8.80it/s, loss=0.51] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.0180\n",
      "Test Metrics: Precision=0.7767, Recall=0.7767, F1=0.7767\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 25, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25/25 [00:00<00:00, 5294.77 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1712.80 examples/s]\n",
      "Map: 100%|██████████| 969/969 [00:00<00:00, 8837.76 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:01<00:00,  2.96it/s, loss=0.856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.5390\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:01<00:00,  3.08it/s, loss=0.459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.9382\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.66it/s, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.3221\n",
      "Test Metrics: Precision=0.8078, Recall=0.8078, F1=0.8078\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 25, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4891.43 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1338.75 examples/s]\n",
      "Map: 100%|██████████| 969/969 [00:00<00:00, 8932.70 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:01<00:00,  2.78it/s, loss=0.706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.9486\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:01<00:00,  3.03it/s, loss=0.568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.8038\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.88it/s, loss=0.522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.3675\n",
      "Test Metrics: Precision=0.8145, Recall=0.8145, F1=0.8145\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 25, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4541.45 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 2011.66 examples/s]\n",
      "Map: 100%|██████████| 969/969 [00:00<00:00, 8673.81 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:00<00:00,  6.53it/s, loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.9037\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:00<00:00,  6.53it/s, loss=0.929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.7730\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:00<00:00,  6.92it/s, loss=1.33] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.5611\n",
      "Test Metrics: Precision=0.7794, Recall=0.7794, F1=0.7794\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 25, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4426.24 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 2017.85 examples/s]\n",
      "Map: 100%|██████████| 969/969 [00:00<00:00, 8796.50 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:01<00:00,  2.48it/s, loss=1.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.5900\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:01<00:00,  2.31it/s, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.6457\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.35it/s, loss=0.655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.6004\n",
      "Test Metrics: Precision=0.7978, Recall=0.7978, F1=0.7978\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 25, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4111.58 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1637.89 examples/s]\n",
      "Map: 100%|██████████| 969/969 [00:00<00:00, 8917.61 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:01<00:00,  2.53it/s, loss=0.912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.3239\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:01<00:00,  2.24it/s, loss=1.12] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.5670\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.60it/s, loss=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.8039\n",
      "Test Metrics: Precision=0.8030, Recall=0.8030, F1=0.8030\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 25, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4609.13 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1637.76 examples/s]\n",
      "Map: 100%|██████████| 969/969 [00:00<00:00, 8844.74 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:00<00:00,  6.72it/s, loss=0.694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.0722\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:00<00:00,  7.29it/s, loss=0.671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.6864\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:00<00:00,  6.29it/s, loss=0.871]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.5110\n",
      "Test Metrics: Precision=0.7766, Recall=0.7766, F1=0.7766\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 25, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4438.04 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1601.12 examples/s]\n",
      "Map: 100%|██████████| 969/969 [00:00<00:00, 4132.46 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.0781\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:02<00:00,  1.93it/s, loss=0.758]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.9750\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.11it/s, loss=0.415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.2245\n",
      "Test Metrics: Precision=0.8032, Recall=0.8032, F1=0.8032\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 25, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4230.18 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1391.61 examples/s]\n",
      "Map: 100%|██████████| 969/969 [00:00<00:00, 9059.27 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:02<00:00,  1.92it/s, loss=1.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.8034\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:01<00:00,  2.09it/s, loss=0.57] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.7468\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:02<00:00,  1.94it/s, loss=0.557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.1148\n",
      "Test Metrics: Precision=0.8166, Recall=0.8166, F1=0.8166\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 30, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 4941.06 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 2363.21 examples/s]\n",
      "Map: 100%|██████████| 963/963 [00:00<00:00, 8952.12 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:00<00:00,  6.32it/s, loss=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.4713\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:00<00:00,  5.82it/s, loss=0.631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.3758\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:00<00:00,  5.60it/s, loss=0.61] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.9467\n",
      "Test Metrics: Precision=0.7829, Recall=0.7829, F1=0.7829\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 30, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5112.10 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 2079.82 examples/s]\n",
      "Map: 100%|██████████| 963/963 [00:00<00:00, 8936.59 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:02<00:00,  1.88it/s, loss=0.781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.7421\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:01<00:00,  2.00it/s, loss=0.798]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.1197\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:02<00:00,  1.85it/s, loss=0.496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.4392\n",
      "Test Metrics: Precision=0.8123, Recall=0.8123, F1=0.8123\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 30, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5169.01 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 2333.63 examples/s]\n",
      "Map: 100%|██████████| 963/963 [00:00<00:00, 9063.20 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:01<00:00,  2.21it/s, loss=0.802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.8340\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:01<00:00,  2.02it/s, loss=0.577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.5897\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.13it/s, loss=0.59] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.1185\n",
      "Test Metrics: Precision=0.8188, Recall=0.8188, F1=0.8188\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 30, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5086.27 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 2506.06 examples/s]\n",
      "Map: 100%|██████████| 963/963 [00:00<00:00, 8856.58 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:00<00:00,  5.63it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.6661\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:00<00:00,  5.60it/s, loss=0.802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.4187\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:00<00:00,  6.04it/s, loss=0.669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.8808\n",
      "Test Metrics: Precision=0.7767, Recall=0.7767, F1=0.7767\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 30, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5075.39 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 2432.42 examples/s]\n",
      "Map: 100%|██████████| 963/963 [00:00<00:00, 8634.24 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:02<00:00,  1.88it/s, loss=0.768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.9904\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:02<00:00,  1.95it/s, loss=0.642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.0762\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:02<00:00,  1.95it/s, loss=0.528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.3743\n",
      "Test Metrics: Precision=0.7965, Recall=0.7965, F1=0.7965\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 30, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 4761.20 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 2165.55 examples/s]\n",
      "Map: 100%|██████████| 963/963 [00:00<00:00, 9016.87 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:02<00:00,  1.99it/s, loss=1.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.6682\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:01<00:00,  2.07it/s, loss=0.652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.2109\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.11it/s, loss=0.636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.3223\n",
      "Test Metrics: Precision=0.7880, Recall=0.7880, F1=0.7880\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 30, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5198.91 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 2375.48 examples/s]\n",
      "Map: 100%|██████████| 963/963 [00:00<00:00, 8652.96 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:00<00:00,  6.58it/s, loss=1.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.4061\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:00<00:00,  6.61it/s, loss=0.724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.8266\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:00<00:00,  6.87it/s, loss=0.825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.1340\n",
      "Test Metrics: Precision=0.7774, Recall=0.7774, F1=0.7774\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 30, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5346.47 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 2381.10 examples/s]\n",
      "Map: 100%|██████████| 963/963 [00:00<00:00, 8853.55 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:01<00:00,  2.23it/s, loss=0.979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.6058\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:01<00:00,  2.01it/s, loss=0.766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.9546\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.09it/s, loss=0.527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.3404\n",
      "Test Metrics: Precision=0.8202, Recall=0.8202, F1=0.8202\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 30, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5128.97 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 2149.46 examples/s]\n",
      "Map: 100%|██████████| 963/963 [00:00<00:00, 8916.07 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:01<00:00,  2.26it/s, loss=0.952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.3455\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:01<00:00,  2.34it/s, loss=0.761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.2741\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.17it/s, loss=0.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.6160\n",
      "Test Metrics: Precision=0.7909, Recall=0.7909, F1=0.7909\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 30, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5024.92 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 2077.25 examples/s]\n",
      "Map: 100%|██████████| 963/963 [00:00<00:00, 8690.92 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:00<00:00,  6.80it/s, loss=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.6057\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:00<00:00,  5.86it/s, loss=0.928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.9022\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:00<00:00,  6.09it/s, loss=1.12] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.5292\n",
      "Test Metrics: Precision=0.7833, Recall=0.7833, F1=0.7833\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 30, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5014.91 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 1742.91 examples/s]\n",
      "Map: 100%|██████████| 963/963 [00:00<00:00, 8519.58 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:02<00:00,  1.91it/s, loss=0.859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.1741\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:02<00:00,  1.99it/s, loss=0.682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.9175\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.11it/s, loss=0.508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.2988\n",
      "Test Metrics: Precision=0.8375, Recall=0.8375, F1=0.8375\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 30, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 4966.02 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 1867.18 examples/s]\n",
      "Map: 100%|██████████| 963/963 [00:00<00:00, 9127.74 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:01<00:00,  2.01it/s, loss=1.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.1383\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:02<00:00,  1.94it/s, loss=0.603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.0719\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.16it/s, loss=0.492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.4828\n",
      "Test Metrics: Precision=0.7937, Recall=0.7937, F1=0.7937\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 30, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5168.58 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 2632.41 examples/s]\n",
      "Map: 100%|██████████| 963/963 [00:00<00:00, 8668.78 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:00<00:00,  5.54it/s, loss=0.903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.9780\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:00<00:00,  5.31it/s, loss=1.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.8134\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:00<00:00,  5.39it/s, loss=0.846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.2430\n",
      "Test Metrics: Precision=0.7771, Recall=0.7771, F1=0.7771\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 30, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 4867.48 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 2628.83 examples/s]\n",
      "Map: 100%|██████████| 963/963 [00:00<00:00, 8899.75 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s, loss=0.824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.9034\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:02<00:00,  1.72it/s, loss=0.615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.0917\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:02<00:00,  1.75it/s, loss=0.704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.5366\n",
      "Test Metrics: Precision=0.7994, Recall=0.7994, F1=0.7994\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 30, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 4901.22 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 2320.29 examples/s]\n",
      "Map: 100%|██████████| 963/963 [00:00<00:00, 9182.44 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s, loss=0.841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.0885\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s, loss=0.608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.5765\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:02<00:00,  1.77it/s, loss=0.418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.9136\n",
      "Test Metrics: Precision=0.8601, Recall=0.8601, F1=0.8601\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 35, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35/35 [00:00<00:00, 5501.65 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 2567.34 examples/s]\n",
      "Map: 100%|██████████| 957/957 [00:00<00:00, 8901.81 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:00<00:00,  5.68it/s, loss=0.873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.9146\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:00<00:00,  5.56it/s, loss=0.655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.2713\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:00<00:00,  5.57it/s, loss=0.646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.6066\n",
      "Test Metrics: Precision=0.7827, Recall=0.7827, F1=0.7827\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 35, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35/35 [00:00<00:00, 5473.14 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 2472.85 examples/s]\n",
      "Map: 100%|██████████| 957/957 [00:00<00:00, 9067.77 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  1.94it/s, loss=1.18] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.3720\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  1.79it/s, loss=0.728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.7800\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  1.78it/s, loss=0.561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.8647\n",
      "Test Metrics: Precision=0.8156, Recall=0.8156, F1=0.8156\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 35, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35/35 [00:00<00:00, 5175.96 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 2310.91 examples/s]\n",
      "Map: 100%|██████████| 957/957 [00:00<00:00, 8916.72 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  1.94it/s, loss=0.749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.1238\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  1.96it/s, loss=0.63] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.2717\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  2.01it/s, loss=0.396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.4184\n",
      "Test Metrics: Precision=0.8517, Recall=0.8517, F1=0.8517\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 35, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35/35 [00:00<00:00, 5689.73 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 2183.40 examples/s]\n",
      "Map: 100%|██████████| 957/957 [00:00<00:00, 4413.50 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:00<00:00,  6.98it/s, loss=0.726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.1777\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:00<00:00,  6.35it/s, loss=0.638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.1804\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:00<00:00,  6.52it/s, loss=0.699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.5628\n",
      "Test Metrics: Precision=0.7760, Recall=0.7760, F1=0.7760\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 35, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35/35 [00:00<00:00, 5783.42 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 2120.17 examples/s]\n",
      "Map: 100%|██████████| 957/957 [00:00<00:00, 8902.44 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  2.13it/s, loss=1.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.0891\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  2.27it/s, loss=0.525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.4619\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  2.06it/s, loss=0.418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.7343\n",
      "Test Metrics: Precision=0.8102, Recall=0.8102, F1=0.8102\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 35, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35/35 [00:00<00:00, 5622.18 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 2070.53 examples/s]\n",
      "Map: 100%|██████████| 957/957 [00:00<00:00, 9059.09 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  2.40it/s, loss=0.838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.9464\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  2.24it/s, loss=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.4473\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  2.18it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.8554\n",
      "Test Metrics: Precision=0.8661, Recall=0.8661, F1=0.8661\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 35, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35/35 [00:00<00:00, 5574.57 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 3141.13 examples/s]\n",
      "Map: 100%|██████████| 957/957 [00:00<00:00, 8790.12 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:00<00:00,  6.58it/s, loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.6379\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:00<00:00,  6.66it/s, loss=0.717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.5278\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:00<00:00,  6.96it/s, loss=0.592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.7565\n",
      "Test Metrics: Precision=0.7839, Recall=0.7839, F1=0.7839\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 35, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35/35 [00:00<00:00, 5830.28 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 3103.28 examples/s]\n",
      "Map: 100%|██████████| 957/957 [00:00<00:00, 8567.40 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s, loss=1.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.4848\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  2.42it/s, loss=0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.2988\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  2.48it/s, loss=0.447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.8264\n",
      "Test Metrics: Precision=0.8315, Recall=0.8315, F1=0.8315\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 35, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35/35 [00:00<00:00, 5649.44 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 3013.15 examples/s]\n",
      "Map: 100%|██████████| 957/957 [00:00<00:00, 8950.38 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  2.34it/s, loss=0.975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.7518\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  2.45it/s, loss=0.697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.3079\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:01<00:00,  2.61it/s, loss=0.281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.1115\n",
      "Test Metrics: Precision=0.8658, Recall=0.8658, F1=0.8658\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 35, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35/35 [00:00<00:00, 5596.67 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 3035.89 examples/s]\n",
      "Map: 100%|██████████| 957/957 [00:00<00:00, 8655.53 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:00<00:00,  6.87it/s, loss=0.913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.8771\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:00<00:00,  6.62it/s, loss=0.942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.4597\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:00<00:00,  6.09it/s, loss=0.721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.5777\n",
      "Test Metrics: Precision=0.8020, Recall=0.8020, F1=0.8020\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 35, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35/35 [00:00<00:00, 5383.03 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 3159.38 examples/s]\n",
      "Map: 100%|██████████| 957/957 [00:00<00:00, 8560.99 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  2.20it/s, loss=0.847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.9124\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s, loss=0.689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.6001\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  2.06it/s, loss=0.535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.0728\n",
      "Test Metrics: Precision=0.8280, Recall=0.8280, F1=0.8280\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 35, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35/35 [00:00<00:00, 5289.54 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 2720.55 examples/s]\n",
      "Map: 100%|██████████| 957/957 [00:00<00:00, 8769.37 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  2.33it/s, loss=0.908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.6227\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  2.28it/s, loss=0.639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.7979\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s, loss=0.525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.6296\n",
      "Test Metrics: Precision=0.8467, Recall=0.8467, F1=0.8467\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 35, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35/35 [00:00<00:00, 5591.13 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 2065.29 examples/s]\n",
      "Map: 100%|██████████| 957/957 [00:00<00:00, 8864.45 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:00<00:00,  5.67it/s, loss=0.996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.3080\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:00<00:00,  6.03it/s, loss=0.987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.7920\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:00<00:00,  5.62it/s, loss=0.604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9179\n",
      "Test Metrics: Precision=0.7835, Recall=0.7835, F1=0.7835\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 35, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35/35 [00:00<00:00, 5311.74 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 2010.83 examples/s]\n",
      "Map: 100%|██████████| 957/957 [00:00<00:00, 8621.75 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  1.78it/s, loss=0.903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.9613\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  1.89it/s, loss=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.6307\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  1.92it/s, loss=0.574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.1035\n",
      "Test Metrics: Precision=0.8239, Recall=0.8239, F1=0.8239\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 35, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35/35 [00:00<00:00, 5126.79 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 1883.63 examples/s]\n",
      "Map: 100%|██████████| 957/957 [00:00<00:00, 9206.03 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  1.93it/s, loss=0.949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.4498\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  1.89it/s, loss=0.57] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.2463\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  2.01it/s, loss=0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.3034\n",
      "Test Metrics: Precision=0.8612, Recall=0.8612, F1=0.8612\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 40, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 5515.37 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 2340.24 examples/s]\n",
      "Map: 100%|██████████| 951/951 [00:00<00:00, 8794.56 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:01<00:00,  4.87it/s, loss=1.05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.3036\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:00<00:00,  5.04it/s, loss=0.872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.2963\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:01<00:00,  4.95it/s, loss=0.751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.7328\n",
      "Test Metrics: Precision=0.7820, Recall=0.7820, F1=0.7820\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 40, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 5286.83 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 2135.05 examples/s]\n",
      "Map: 100%|██████████| 951/951 [00:00<00:00, 8755.78 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:03<00:00,  1.65it/s, loss=0.95] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.3487\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  1.73it/s, loss=0.605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.1558\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  1.70it/s, loss=0.475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.7019\n",
      "Test Metrics: Precision=0.8192, Recall=0.8192, F1=0.8192\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 40, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 5405.73 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 2150.93 examples/s]\n",
      "Map: 100%|██████████| 951/951 [00:00<00:00, 8982.21 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:03<00:00,  1.65it/s, loss=0.857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.9731\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  1.68it/s, loss=0.646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.3366\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, loss=0.449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.7253\n",
      "Test Metrics: Precision=0.8152, Recall=0.8152, F1=0.8152\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 40, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 5908.30 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 2801.10 examples/s]\n",
      "Map: 100%|██████████| 951/951 [00:00<00:00, 8650.28 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:00<00:00,  5.60it/s, loss=1.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.0094\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:00<00:00,  6.09it/s, loss=0.692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.1904\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:00<00:00,  5.74it/s, loss=0.606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.2466\n",
      "Test Metrics: Precision=0.7889, Recall=0.7889, F1=0.7889\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 40, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 6166.96 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 2951.66 examples/s]\n",
      "Map: 100%|██████████| 951/951 [00:00<00:00, 8809.89 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  1.90it/s, loss=1.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.2225\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  1.88it/s, loss=0.537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.1660\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  1.87it/s, loss=0.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.5644\n",
      "Test Metrics: Precision=0.8284, Recall=0.8284, F1=0.8284\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 40, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 5823.00 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 2836.62 examples/s]\n",
      "Map: 100%|██████████| 951/951 [00:00<00:00, 9017.30 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  1.93it/s, loss=0.962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.1304\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  1.97it/s, loss=0.514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.8643\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  1.98it/s, loss=0.406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.1473\n",
      "Test Metrics: Precision=0.8569, Recall=0.8569, F1=0.8569\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 40, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 5583.66 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 2808.84 examples/s]\n",
      "Map: 100%|██████████| 951/951 [00:00<00:00, 8567.23 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:00<00:00,  5.74it/s, loss=0.939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.1932\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:00<00:00,  6.31it/s, loss=0.828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.3965\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:00<00:00,  6.28it/s, loss=0.774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.6232\n",
      "Test Metrics: Precision=0.7904, Recall=0.7904, F1=0.7904\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 40, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 6048.68 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 3154.50 examples/s]\n",
      "Map: 100%|██████████| 951/951 [00:00<00:00, 8838.27 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  1.88it/s, loss=0.698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.9061\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  1.94it/s, loss=0.606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.5222\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  2.00it/s, loss=0.605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.7094\n",
      "Test Metrics: Precision=0.8335, Recall=0.8335, F1=0.8335\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 40, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 5626.73 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 3006.40 examples/s]\n",
      "Map: 100%|██████████| 951/951 [00:00<00:00, 9083.31 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  2.24it/s, loss=0.847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.6728\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  2.23it/s, loss=0.653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.4364\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  2.16it/s, loss=0.511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.7277\n",
      "Test Metrics: Precision=0.8338, Recall=0.8338, F1=0.8338\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 40, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 5609.98 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 2924.13 examples/s]\n",
      "Map: 100%|██████████| 951/951 [00:00<00:00, 8656.48 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:00<00:00,  5.51it/s, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.7905\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:00<00:00,  5.09it/s, loss=0.769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.3719\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:00<00:00,  5.30it/s, loss=0.661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.7102\n",
      "Test Metrics: Precision=0.7886, Recall=0.7886, F1=0.7886\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 40, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 5883.23 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 2971.79 examples/s]\n",
      "Map: 100%|██████████| 951/951 [00:00<00:00, 8883.49 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  1.98it/s, loss=0.998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.6697\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  1.85it/s, loss=0.582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.5922\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  1.90it/s, loss=0.598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.8626\n",
      "Test Metrics: Precision=0.8320, Recall=0.8320, F1=0.8320\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 40, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 5573.27 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 2509.87 examples/s]\n",
      "Map: 100%|██████████| 951/951 [00:00<00:00, 9014.91 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  2.00it/s, loss=0.841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.8484\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  1.91it/s, loss=0.616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.1690\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  1.91it/s, loss=0.482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.5562\n",
      "Test Metrics: Precision=0.8399, Recall=0.8399, F1=0.8399\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 40, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 6115.48 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 2935.90 examples/s]\n",
      "Map: 100%|██████████| 951/951 [00:00<00:00, 8782.07 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:00<00:00,  5.78it/s, loss=1.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.3137\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:00<00:00,  5.49it/s, loss=0.666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.5230\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:00<00:00,  5.23it/s, loss=0.673]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.6899\n",
      "Test Metrics: Precision=0.7890, Recall=0.7890, F1=0.7890\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 40, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 6223.93 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 3123.96 examples/s]\n",
      "Map: 100%|██████████| 951/951 [00:00<00:00, 8876.01 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s, loss=0.945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.8399\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:03<00:00,  1.56it/s, loss=0.583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.8018\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, loss=0.598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.0049\n",
      "Test Metrics: Precision=0.8238, Recall=0.8238, F1=0.8238\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 40, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 5963.75 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 2701.21 examples/s]\n",
      "Map: 100%|██████████| 951/951 [00:00<00:00, 8947.17 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [00:02<00:00,  1.92it/s, loss=1.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.4527\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  1.72it/s, loss=0.644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.8425\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  1.97it/s, loss=0.723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.0626\n",
      "Test Metrics: Precision=0.8219, Recall=0.8219, F1=0.8219\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 45, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 5987.49 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 3137.10 examples/s]\n",
      "Map: 100%|██████████| 945/945 [00:00<00:00, 8845.90 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 6/6 [00:01<00:00,  5.38it/s, loss=0.676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.1210\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 6/6 [00:01<00:00,  5.24it/s, loss=0.665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.6552\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 6/6 [00:00<00:00,  6.27it/s, loss=0.633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.7684\n",
      "Test Metrics: Precision=0.8014, Recall=0.8014, F1=0.8014\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 45, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 5738.63 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2853.48 examples/s]\n",
      "Map: 100%|██████████| 945/945 [00:00<00:00, 8754.13 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 6/6 [00:03<00:00,  1.75it/s, loss=0.882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.7456\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 6/6 [00:03<00:00,  1.76it/s, loss=0.715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.6553\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 6/6 [00:03<00:00,  1.74it/s, loss=0.386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.8238\n",
      "Test Metrics: Precision=0.8288, Recall=0.8288, F1=0.8288\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 45, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 5541.02 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2669.45 examples/s]\n",
      "Map: 100%|██████████| 945/945 [00:00<00:00, 9126.74 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, loss=1.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.5078\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 6/6 [00:03<00:00,  1.80it/s, loss=0.817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.1565\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 6/6 [00:03<00:00,  1.94it/s, loss=0.568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.2601\n",
      "Test Metrics: Precision=0.8134, Recall=0.8134, F1=0.8134\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 45, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 5803.39 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2547.66 examples/s]\n",
      "Map: 100%|██████████| 945/945 [00:00<00:00, 8762.92 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 6/6 [00:00<00:00,  6.54it/s, loss=0.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.0345\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 6/6 [00:01<00:00,  5.92it/s, loss=0.682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.3355\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 6/6 [00:00<00:00,  6.27it/s, loss=0.571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.4108\n",
      "Test Metrics: Precision=0.8133, Recall=0.8133, F1=0.8133\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 45, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 6359.72 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2920.82 examples/s]\n",
      "Map: 100%|██████████| 945/945 [00:00<00:00, 8548.84 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 6/6 [00:02<00:00,  2.04it/s, loss=0.763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.3979\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 6/6 [00:02<00:00,  2.02it/s, loss=0.534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.9210\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 6/6 [00:02<00:00,  2.13it/s, loss=0.409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.1832\n",
      "Test Metrics: Precision=0.8239, Recall=0.8239, F1=0.8239\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 45, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 6024.38 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2378.62 examples/s]\n",
      "Map: 100%|██████████| 945/945 [00:00<00:00, 8839.25 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 6/6 [00:02<00:00,  2.02it/s, loss=0.857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.1348\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 6/6 [00:02<00:00,  2.01it/s, loss=0.488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.5825\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 6/6 [00:03<00:00,  1.92it/s, loss=0.394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.9165\n",
      "Test Metrics: Precision=0.8464, Recall=0.8464, F1=0.8464\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 45, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 6313.76 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 3012.91 examples/s]\n",
      "Map: 100%|██████████| 945/945 [00:00<00:00, 8596.56 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 6/6 [00:00<00:00,  6.34it/s, loss=0.879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.8291\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 6/6 [00:00<00:00,  6.30it/s, loss=0.803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.0342\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 6/6 [00:00<00:00,  6.58it/s, loss=0.603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.0864\n",
      "Test Metrics: Precision=0.7903, Recall=0.7903, F1=0.7903\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 45, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 6231.43 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2998.31 examples/s]\n",
      "Map: 100%|██████████| 945/945 [00:00<00:00, 8667.04 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 6/6 [00:02<00:00,  2.04it/s, loss=0.837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.6329\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 6/6 [00:03<00:00,  1.98it/s, loss=0.669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.9131\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 6/6 [00:02<00:00,  2.22it/s, loss=0.399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.9922\n",
      "Test Metrics: Precision=0.8412, Recall=0.8412, F1=0.8412\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 45, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 6120.49 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2728.89 examples/s]\n",
      "Map: 100%|██████████| 945/945 [00:00<00:00, 8951.16 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 6/6 [00:02<00:00,  2.26it/s, loss=0.775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.7807\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 6/6 [00:02<00:00,  2.29it/s, loss=0.664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.6814\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 6/6 [00:02<00:00,  2.27it/s, loss=0.429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.7372\n",
      "Test Metrics: Precision=0.8765, Recall=0.8765, F1=0.8765\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 45, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 5689.68 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 3148.61 examples/s]\n",
      "Map: 100%|██████████| 945/945 [00:00<00:00, 8616.26 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 6/6 [00:01<00:00,  5.63it/s, loss=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.6327\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 6/6 [00:01<00:00,  5.58it/s, loss=0.57] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.6490\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 6/6 [00:01<00:00,  5.89it/s, loss=0.692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.6378\n",
      "Test Metrics: Precision=0.8027, Recall=0.8027, F1=0.8027\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 45, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 5796.08 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2956.74 examples/s]\n",
      "Map: 100%|██████████| 945/945 [00:00<00:00, 8163.74 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 6/6 [00:03<00:00,  1.95it/s, loss=0.782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.5982\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 6/6 [00:03<00:00,  1.92it/s, loss=0.675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.8814\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 6/6 [00:03<00:00,  1.96it/s, loss=0.508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.1887\n",
      "Test Metrics: Precision=0.8275, Recall=0.8275, F1=0.8275\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 45, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 5349.27 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2793.93 examples/s]\n",
      "Map: 100%|██████████| 945/945 [00:00<00:00, 8752.49 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 6/6 [00:03<00:00,  1.92it/s, loss=0.652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.0836\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 6/6 [00:02<00:00,  2.31it/s, loss=0.523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.3121\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 6/6 [00:02<00:00,  2.09it/s, loss=0.368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.6422\n",
      "Test Metrics: Precision=0.8651, Recall=0.8651, F1=0.8651\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 45, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 6057.44 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2257.29 examples/s]\n",
      "Map: 100%|██████████| 945/945 [00:00<00:00, 8542.92 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 6/6 [00:01<00:00,  5.37it/s, loss=0.994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.7606\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 6/6 [00:01<00:00,  4.72it/s, loss=0.963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.2055\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 6/6 [00:01<00:00,  4.64it/s, loss=0.679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4642\n",
      "Test Metrics: Precision=0.7921, Recall=0.7921, F1=0.7921\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 45, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 6125.06 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2471.28 examples/s]\n",
      "Map: 100%|██████████| 945/945 [00:00<00:00, 4130.38 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 6/6 [00:03<00:00,  1.51it/s, loss=1.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.9212\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 6/6 [00:03<00:00,  1.66it/s, loss=0.669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.1595\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 6/6 [00:04<00:00,  1.48it/s, loss=0.712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.3568\n",
      "Test Metrics: Precision=0.8347, Recall=0.8347, F1=0.8347\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 45, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 6124.46 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2046.78 examples/s]\n",
      "Map: 100%|██████████| 945/945 [00:00<00:00, 9014.26 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, loss=0.928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.4606\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 6/6 [00:03<00:00,  1.66it/s, loss=0.524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.4316\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 6/6 [00:03<00:00,  1.66it/s, loss=0.49] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.6384\n",
      "Test Metrics: Precision=0.8474, Recall=0.8474, F1=0.8474\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 50, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6163.92 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 3413.06 examples/s]\n",
      "Map: 100%|██████████| 939/939 [00:00<00:00, 8596.99 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:01<00:00,  6.24it/s, loss=1.17] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.0141\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:01<00:00,  5.56it/s, loss=0.667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.4401\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:01<00:00,  5.70it/s, loss=0.49] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.2618\n",
      "Test Metrics: Precision=0.8007, Recall=0.8007, F1=0.8007\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 50, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6333.32 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 3443.88 examples/s]\n",
      "Map: 100%|██████████| 939/939 [00:00<00:00, 8532.38 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:03<00:00,  2.06it/s, loss=0.479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.8736\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:03<00:00,  1.82it/s, loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.6222\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:03<00:00,  1.77it/s, loss=0.208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.9005\n",
      "Test Metrics: Precision=0.8504, Recall=0.8504, F1=0.8504\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 50, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 5667.83 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2894.42 examples/s]\n",
      "Map: 100%|██████████| 939/939 [00:00<00:00, 8527.08 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:03<00:00,  2.05it/s, loss=0.542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.5024\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:03<00:00,  1.92it/s, loss=0.522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.0930\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:03<00:00,  1.78it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.9766\n",
      "Test Metrics: Precision=0.8612, Recall=0.8612, F1=0.8612\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 50, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6264.64 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 3245.36 examples/s]\n",
      "Map: 100%|██████████| 939/939 [00:00<00:00, 8484.38 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:01<00:00,  5.66it/s, loss=0.868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.7232\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:01<00:00,  6.24it/s, loss=1.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.3594\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:01<00:00,  6.48it/s, loss=0.381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9207\n",
      "Test Metrics: Precision=0.8071, Recall=0.8071, F1=0.8071\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 50, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6251.76 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 3068.93 examples/s]\n",
      "Map: 100%|██████████| 939/939 [00:00<00:00, 8625.89 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:03<00:00,  2.04it/s, loss=0.876]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.2461\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:03<00:00,  2.01it/s, loss=0.51] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.9188\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:03<00:00,  2.12it/s, loss=0.404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.1104\n",
      "Test Metrics: Precision=0.8418, Recall=0.8418, F1=0.8418\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 50, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6379.95 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 3172.94 examples/s]\n",
      "Map: 100%|██████████| 939/939 [00:00<00:00, 9074.84 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:03<00:00,  2.12it/s, loss=0.456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.7191\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:02<00:00,  2.34it/s, loss=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.1759\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:03<00:00,  2.23it/s, loss=0.308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.3746\n",
      "Test Metrics: Precision=0.8766, Recall=0.8766, F1=0.8766\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 50, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6406.84 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 3434.29 examples/s]\n",
      "Map: 100%|██████████| 939/939 [00:00<00:00, 8619.56 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:01<00:00,  6.39it/s, loss=1.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.1677\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:01<00:00,  5.85it/s, loss=0.545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.4206\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:01<00:00,  6.63it/s, loss=0.71] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.3941\n",
      "Test Metrics: Precision=0.8403, Recall=0.8403, F1=0.8403\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 50, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6193.60 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 3294.04 examples/s]\n",
      "Map: 100%|██████████| 939/939 [00:00<00:00, 8663.44 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:03<00:00,  2.08it/s, loss=1.75] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.5522\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:03<00:00,  2.09it/s, loss=0.504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.6171\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:03<00:00,  2.07it/s, loss=0.439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.8770\n",
      "Test Metrics: Precision=0.8347, Recall=0.8347, F1=0.8347\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 50, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6123.43 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2896.82 examples/s]\n",
      "Map: 100%|██████████| 939/939 [00:00<00:00, 8888.40 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:03<00:00,  2.29it/s, loss=0.844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.0318\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:03<00:00,  2.25it/s, loss=0.662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.2178\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:03<00:00,  2.15it/s, loss=0.511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.2691\n",
      "Test Metrics: Precision=0.8784, Recall=0.8784, F1=0.8784\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 50, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6336.38 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2796.20 examples/s]\n",
      "Map: 100%|██████████| 939/939 [00:00<00:00, 8657.48 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:00<00:00,  7.16it/s, loss=1.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.8083\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:01<00:00,  6.31it/s, loss=0.609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.7241\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:01<00:00,  6.79it/s, loss=0.633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.5580\n",
      "Test Metrics: Precision=0.7913, Recall=0.7913, F1=0.7913\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 50, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 5983.14 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2523.65 examples/s]\n",
      "Map: 100%|██████████| 939/939 [00:00<00:00, 8774.68 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:03<00:00,  2.14it/s, loss=0.491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.7704\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:03<00:00,  2.12it/s, loss=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.8596\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:03<00:00,  2.25it/s, loss=0.445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.1420\n",
      "Test Metrics: Precision=0.8543, Recall=0.8543, F1=0.8543\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 50, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6280.03 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2441.39 examples/s]\n",
      "Map: 100%|██████████| 939/939 [00:00<00:00, 8923.63 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:03<00:00,  2.31it/s, loss=0.7]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.8894\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:03<00:00,  2.23it/s, loss=0.587]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.1848\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:03<00:00,  2.15it/s, loss=0.53] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.2257\n",
      "Test Metrics: Precision=0.8559, Recall=0.8559, F1=0.8559\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 50, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6307.60 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 3791.63 examples/s]\n",
      "Map: 100%|██████████| 939/939 [00:00<00:00, 8434.58 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:01<00:00,  5.15it/s, loss=1.41] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.3673\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:01<00:00,  4.67it/s, loss=1.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.1502\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:01<00:00,  4.90it/s, loss=1.12] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.2237\n",
      "Test Metrics: Precision=0.8086, Recall=0.8086, F1=0.8086\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 50, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6533.39 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 3873.93 examples/s]\n",
      "Map: 100%|██████████| 939/939 [00:00<00:00, 8585.69 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:03<00:00,  1.84it/s, loss=0.578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.0985\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:04<00:00,  1.60it/s, loss=0.957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.3400\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:04<00:00,  1.71it/s, loss=0.414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.8910\n",
      "Test Metrics: Precision=0.8326, Recall=0.8326, F1=0.8326\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 50, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6299.27 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 3297.67 examples/s]\n",
      "Map: 100%|██████████| 939/939 [00:00<00:00, 8743.27 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:03<00:00,  1.82it/s, loss=0.848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.7800\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:03<00:00,  1.78it/s, loss=1.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.7101\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:04<00:00,  1.71it/s, loss=0.42] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.3261\n",
      "Test Metrics: Precision=0.8577, Recall=0.8577, F1=0.8577\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 55, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 5364.31 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 3227.29 examples/s]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 8416.16 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:01<00:00,  4.53it/s, loss=1.02] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.7824\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:01<00:00,  4.72it/s, loss=0.659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.0722\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:01<00:00,  4.87it/s, loss=0.419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.1347\n",
      "Test Metrics: Precision=0.8174, Recall=0.8174, F1=0.8174\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 55, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6181.98 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 3702.24 examples/s]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 8940.73 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:04<00:00,  1.66it/s, loss=0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.8052\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:04<00:00,  1.54it/s, loss=0.602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.0600\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:04<00:00,  1.71it/s, loss=0.514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.3275\n",
      "Test Metrics: Precision=0.8463, Recall=0.8463, F1=0.8463\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 55, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 5851.28 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 3260.82 examples/s]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 8563.74 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:04<00:00,  1.60it/s, loss=0.597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.4101\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:04<00:00,  1.60it/s, loss=0.41] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.9496\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:04<00:00,  1.61it/s, loss=0.543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.1637\n",
      "Test Metrics: Precision=0.8623, Recall=0.8623, F1=0.8623\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 55, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6803.31 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 3392.20 examples/s]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 8857.09 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:01<00:00,  6.35it/s, loss=0.829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.6209\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:01<00:00,  5.70it/s, loss=0.709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.7029\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:01<00:00,  6.21it/s, loss=0.49] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.6849\n",
      "Test Metrics: Precision=0.8238, Recall=0.8238, F1=0.8238\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 55, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6517.68 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 3159.88 examples/s]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 8401.77 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:03<00:00,  1.91it/s, loss=0.64] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.4337\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:03<00:00,  1.82it/s, loss=0.461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.6878\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:03<00:00,  1.90it/s, loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.9095\n",
      "Test Metrics: Precision=0.8517, Recall=0.8517, F1=0.8517\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 55, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6651.48 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 3165.95 examples/s]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 8573.55 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:03<00:00,  2.02it/s, loss=0.753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.0862\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:03<00:00,  2.07it/s, loss=0.479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.5275\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:03<00:00,  1.92it/s, loss=0.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.4797\n",
      "Test Metrics: Precision=0.8773, Recall=0.8773, F1=0.8773\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 55, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6060.18 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 2432.38 examples/s]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 8485.28 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:01<00:00,  4.59it/s, loss=1.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.9197\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:01<00:00,  4.87it/s, loss=0.857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.6250\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:01<00:00,  4.63it/s, loss=0.563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.7271\n",
      "Test Metrics: Precision=0.8284, Recall=0.8284, F1=0.8284\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 55, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 5935.28 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 2441.52 examples/s]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 8718.47 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:04<00:00,  1.47it/s, loss=0.808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.5235\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:04<00:00,  1.51it/s, loss=0.619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.4792\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:04<00:00,  1.59it/s, loss=0.586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.5643\n",
      "Test Metrics: Precision=0.8464, Recall=0.8464, F1=0.8464\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 55, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 5947.53 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 2180.20 examples/s]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 8468.01 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:04<00:00,  1.67it/s, loss=0.702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.0800\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:04<00:00,  1.73it/s, loss=0.607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.9244\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:03<00:00,  1.81it/s, loss=0.442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.9400\n",
      "Test Metrics: Precision=0.8820, Recall=0.8820, F1=0.8820\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 55, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6346.61 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 3016.89 examples/s]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 8375.54 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:01<00:00,  5.52it/s, loss=0.698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.0258\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:01<00:00,  5.64it/s, loss=0.571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.4032\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:01<00:00,  5.16it/s, loss=0.591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.5713\n",
      "Test Metrics: Precision=0.8466, Recall=0.8466, F1=0.8466\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 55, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6720.66 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 3841.90 examples/s]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 8943.59 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:03<00:00,  1.75it/s, loss=0.529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.1063\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:03<00:00,  1.79it/s, loss=0.52] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.6267\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:03<00:00,  1.82it/s, loss=0.488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.0762\n",
      "Test Metrics: Precision=0.8537, Recall=0.8537, F1=0.8537\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 55, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6554.72 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 3472.89 examples/s]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 9092.84 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:03<00:00,  1.92it/s, loss=0.648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.6046\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:03<00:00,  1.91it/s, loss=0.394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.4175\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:03<00:00,  2.01it/s, loss=0.353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.6225\n",
      "Test Metrics: Precision=0.8733, Recall=0.8733, F1=0.8733\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 55, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6230.40 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 3023.42 examples/s]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 8509.20 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:01<00:00,  4.51it/s, loss=0.84] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.0044\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:01<00:00,  4.95it/s, loss=0.602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.8716\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:01<00:00,  4.48it/s, loss=0.843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.8214\n",
      "Test Metrics: Precision=0.8056, Recall=0.8056, F1=0.8056\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 55, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6303.95 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 3144.80 examples/s]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 8667.28 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:04<00:00,  1.52it/s, loss=0.787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.4426\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:04<00:00,  1.47it/s, loss=0.648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.7861\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:04<00:00,  1.44it/s, loss=0.477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.8242\n",
      "Test Metrics: Precision=0.8433, Recall=0.8433, F1=0.8433\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 55, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 6310.67 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 2904.09 examples/s]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 8699.96 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 7/7 [00:04<00:00,  1.59it/s, loss=0.751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.2571\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 7/7 [00:04<00:00,  1.71it/s, loss=0.421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.0496\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 7/7 [00:04<00:00,  1.63it/s, loss=0.569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.1313\n",
      "Test Metrics: Precision=0.8695, Recall=0.8695, F1=0.8695\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 60, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 5857.28 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 3261.09 examples/s]\n",
      "Map: 100%|██████████| 927/927 [00:00<00:00, 8281.99 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:01<00:00,  4.95it/s, loss=0.823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.6190\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 8/8 [00:01<00:00,  5.20it/s, loss=0.508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.3374\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 8/8 [00:01<00:00,  4.71it/s, loss=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4474\n",
      "Test Metrics: Precision=0.8388, Recall=0.8388, F1=0.8388\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 60, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 5964.60 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 3606.19 examples/s]\n",
      "Map: 100%|██████████| 927/927 [00:00<00:00, 3959.46 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:04<00:00,  1.74it/s, loss=0.835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.7965\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 8/8 [00:04<00:00,  1.78it/s, loss=0.329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.2918\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 8/8 [00:04<00:00,  1.77it/s, loss=0.575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.8375\n",
      "Test Metrics: Precision=0.8526, Recall=0.8526, F1=0.8526\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 60, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 5398.65 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 2974.86 examples/s]\n",
      "Map: 100%|██████████| 927/927 [00:00<00:00, 8700.12 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:04<00:00,  1.77it/s, loss=0.785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.7859\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 8/8 [00:04<00:00,  1.69it/s, loss=0.519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.9472\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 8/8 [00:04<00:00,  1.94it/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.0587\n",
      "Test Metrics: Precision=0.8759, Recall=0.8759, F1=0.8759\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 60, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 7348.97 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 3465.89 examples/s]\n",
      "Map: 100%|██████████| 927/927 [00:00<00:00, 8526.50 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:01<00:00,  6.25it/s, loss=0.672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.4388\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 8/8 [00:01<00:00,  6.06it/s, loss=0.579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.7763\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 8/8 [00:01<00:00,  6.94it/s, loss=0.308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.6891\n",
      "Test Metrics: Precision=0.8546, Recall=0.8546, F1=0.8546\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 60, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 6847.84 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 3338.97 examples/s]\n",
      "Map: 100%|██████████| 927/927 [00:00<00:00, 8488.90 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:03<00:00,  2.05it/s, loss=0.8]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.1367\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 8/8 [00:03<00:00,  2.02it/s, loss=0.525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.0799\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 8/8 [00:04<00:00,  1.87it/s, loss=0.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.8686\n",
      "Test Metrics: Precision=0.8376, Recall=0.8376, F1=0.8376\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 60, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 7098.96 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 3365.99 examples/s]\n",
      "Map: 100%|██████████| 927/927 [00:00<00:00, 8964.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:03<00:00,  2.06it/s, loss=0.457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.8834\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 8/8 [00:03<00:00,  2.08it/s, loss=0.423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.8624\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 8/8 [00:04<00:00,  1.92it/s, loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.8733\n",
      "Test Metrics: Precision=0.8740, Recall=0.8740, F1=0.8740\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 60, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 6028.32 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 3589.48 examples/s]\n",
      "Map: 100%|██████████| 927/927 [00:00<00:00, 8526.90 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:01<00:00,  4.85it/s, loss=1.03] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.0814\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 8/8 [00:01<00:00,  5.03it/s, loss=0.982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.1175\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 8/8 [00:01<00:00,  4.85it/s, loss=0.661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.9809\n",
      "Test Metrics: Precision=0.8260, Recall=0.8260, F1=0.8260\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 60, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 5860.29 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 3942.63 examples/s]\n",
      "Map: 100%|██████████| 927/927 [00:00<00:00, 8463.58 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:04<00:00,  1.71it/s, loss=0.638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.6262\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 8/8 [00:04<00:00,  1.70it/s, loss=0.441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.1709\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 8/8 [00:04<00:00,  1.61it/s, loss=0.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.1276\n",
      "Test Metrics: Precision=0.8526, Recall=0.8526, F1=0.8526\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 60, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 6325.14 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 3710.68 examples/s]\n",
      "Map: 100%|██████████| 927/927 [00:00<00:00, 9006.24 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:04<00:00,  1.87it/s, loss=0.776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.2302\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 8/8 [00:04<00:00,  1.99it/s, loss=0.645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.8200\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 8/8 [00:03<00:00,  2.01it/s, loss=0.491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.1446\n",
      "Test Metrics: Precision=0.8596, Recall=0.8596, F1=0.8596\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 60, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 6575.35 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 4032.98 examples/s]\n",
      "Map: 100%|██████████| 927/927 [00:00<00:00, 8598.40 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:01<00:00,  5.84it/s, loss=0.712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.0791\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 8/8 [00:01<00:00,  6.14it/s, loss=0.535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.2180\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 8/8 [00:01<00:00,  6.45it/s, loss=0.416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.0334\n",
      "Test Metrics: Precision=0.8385, Recall=0.8385, F1=0.8385\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 60, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 6747.05 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 4059.66 examples/s]\n",
      "Map: 100%|██████████| 927/927 [00:00<00:00, 8669.98 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:04<00:00,  1.97it/s, loss=0.562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.1848\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 8/8 [00:04<00:00,  1.91it/s, loss=0.431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.5627\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 8/8 [00:04<00:00,  1.87it/s, loss=0.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.7326\n",
      "Test Metrics: Precision=0.8464, Recall=0.8464, F1=0.8464\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 60, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 6459.57 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 3715.61 examples/s]\n",
      "Map: 100%|██████████| 927/927 [00:00<00:00, 8985.45 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:03<00:00,  2.00it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.9497\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 8/8 [00:03<00:00,  2.18it/s, loss=0.253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.9864\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 8/8 [00:03<00:00,  2.27it/s, loss=0.482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.1790\n",
      "Test Metrics: Precision=0.8747, Recall=0.8747, F1=0.8747\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 60, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 7031.13 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 3616.04 examples/s]\n",
      "Map: 100%|██████████| 927/927 [00:00<00:00, 8689.88 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:01<00:00,  4.73it/s, loss=1.17] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.2451\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 8/8 [00:01<00:00,  4.48it/s, loss=0.945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.1320\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 8/8 [00:01<00:00,  4.87it/s, loss=0.641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.8526\n",
      "Test Metrics: Precision=0.8237, Recall=0.8237, F1=0.8237\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 60, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 6664.32 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 3288.36 examples/s]\n",
      "Map: 100%|██████████| 927/927 [00:00<00:00, 8485.97 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:04<00:00,  1.61it/s, loss=0.733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.0487\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 8/8 [00:04<00:00,  1.66it/s, loss=0.441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.3038\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 8/8 [00:05<00:00,  1.57it/s, loss=0.53] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.2772\n",
      "Test Metrics: Precision=0.8455, Recall=0.8455, F1=0.8455\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 60, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 6495.91 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 3269.56 examples/s]\n",
      "Map: 100%|██████████| 927/927 [00:00<00:00, 9066.07 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:04<00:00,  1.79it/s, loss=0.592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.6807\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 8/8 [00:04<00:00,  1.81it/s, loss=0.373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.5896\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 8/8 [00:04<00:00,  1.78it/s, loss=0.297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.5668\n",
      "Test Metrics: Precision=0.8729, Recall=0.8729, F1=0.8729\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 65, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6245.53 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 3006.67 examples/s]\n",
      "Map: 100%|██████████| 921/921 [00:00<00:00, 8534.39 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  5.63it/s, loss=0.748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.2674\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:01<00:00,  5.58it/s, loss=1.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.7455\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:01<00:00,  5.40it/s, loss=0.283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.8953\n",
      "Test Metrics: Precision=0.8322, Recall=0.8322, F1=0.8322\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 65, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6453.54 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 3268.55 examples/s]\n",
      "Map: 100%|██████████| 921/921 [00:00<00:00, 9059.86 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:05<00:00,  1.68it/s, loss=0.658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.8506\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:05<00:00,  1.78it/s, loss=0.477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.4512\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:05<00:00,  1.75it/s, loss=0.32] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.2789\n",
      "Test Metrics: Precision=0.8518, Recall=0.8518, F1=0.8518\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 65, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6310.73 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 2935.29 examples/s]\n",
      "Map: 100%|██████████| 921/921 [00:00<00:00, 9000.53 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:04<00:00,  1.83it/s, loss=0.725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.9058\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:05<00:00,  1.79it/s, loss=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.2699\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:04<00:00,  1.85it/s, loss=0.681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9709\n",
      "Test Metrics: Precision=0.8739, Recall=0.8739, F1=0.8739\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 65, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65/65 [00:00<00:00, 7041.97 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 3733.12 examples/s]\n",
      "Map: 100%|██████████| 921/921 [00:00<00:00, 8545.73 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  6.20it/s, loss=0.49] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.5155\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:01<00:00,  6.64it/s, loss=0.566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.8741\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:01<00:00,  6.27it/s, loss=0.215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.3747\n",
      "Test Metrics: Precision=0.8457, Recall=0.8457, F1=0.8457\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 65, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65/65 [00:00<00:00, 7095.85 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 4168.65 examples/s]\n",
      "Map: 100%|██████████| 921/921 [00:00<00:00, 8619.20 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:04<00:00,  2.03it/s, loss=0.94] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.4241\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:04<00:00,  1.97it/s, loss=0.996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.0848\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:04<00:00,  1.83it/s, loss=0.159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.4364\n",
      "Test Metrics: Precision=0.8649, Recall=0.8649, F1=0.8649\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 65, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6908.14 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 3761.19 examples/s]\n",
      "Map: 100%|██████████| 921/921 [00:00<00:00, 3981.19 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:04<00:00,  2.17it/s, loss=1.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.5774\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:04<00:00,  2.16it/s, loss=0.159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.8395\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:04<00:00,  2.20it/s, loss=0.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.1225\n",
      "Test Metrics: Precision=0.8778, Recall=0.8778, F1=0.8778\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 65, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65/65 [00:00<00:00, 5769.21 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 3029.89 examples/s]\n",
      "Map: 100%|██████████| 921/921 [00:00<00:00, 8562.10 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  5.15it/s, loss=0.575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.1865\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:01<00:00,  5.33it/s, loss=0.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.0826\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:01<00:00,  5.90it/s, loss=0.813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.5957\n",
      "Test Metrics: Precision=0.8398, Recall=0.8398, F1=0.8398\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 65, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6066.12 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 3767.17 examples/s]\n",
      "Map: 100%|██████████| 921/921 [00:00<00:00, 8592.59 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:04<00:00,  1.87it/s, loss=0.643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.1724\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:05<00:00,  1.69it/s, loss=0.722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.5926\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:04<00:00,  1.85it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.1777\n",
      "Test Metrics: Precision=0.8543, Recall=0.8543, F1=0.8543\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 65, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6420.86 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 3539.50 examples/s]\n",
      "Map: 100%|██████████| 921/921 [00:00<00:00, 8754.99 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:04<00:00,  1.92it/s, loss=0.819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.3769\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:04<00:00,  1.91it/s, loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.6676\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:04<00:00,  1.81it/s, loss=0.388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.7973\n",
      "Test Metrics: Precision=0.8800, Recall=0.8800, F1=0.8800\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 65, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6822.74 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 3847.44 examples/s]\n",
      "Map: 100%|██████████| 921/921 [00:00<00:00, 8611.67 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  6.21it/s, loss=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.8133\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:01<00:00,  6.10it/s, loss=0.421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.1415\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:01<00:00,  6.57it/s, loss=0.776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.1117\n",
      "Test Metrics: Precision=0.8341, Recall=0.8341, F1=0.8341\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 65, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65/65 [00:00<00:00, 7216.82 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 4009.26 examples/s]\n",
      "Map: 100%|██████████| 921/921 [00:00<00:00, 8580.13 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:04<00:00,  1.89it/s, loss=0.848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.9225\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:04<00:00,  2.01it/s, loss=0.523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.1462\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:04<00:00,  1.99it/s, loss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.0044\n",
      "Test Metrics: Precision=0.8548, Recall=0.8548, F1=0.8548\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 65, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6702.14 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 3272.28 examples/s]\n",
      "Map: 100%|██████████| 921/921 [00:00<00:00, 8605.44 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:04<00:00,  1.96it/s, loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.7392\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:04<00:00,  2.22it/s, loss=0.269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.8740\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:03<00:00,  2.30it/s, loss=0.123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.1098\n",
      "Test Metrics: Precision=0.8855, Recall=0.8855, F1=0.8855\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 65, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6703.96 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 3389.86 examples/s]\n",
      "Map: 100%|██████████| 921/921 [00:00<00:00, 8674.30 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  4.90it/s, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.2803\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:01<00:00,  4.84it/s, loss=0.787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.7812\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:01<00:00,  4.60it/s, loss=0.93] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.7893\n",
      "Test Metrics: Precision=0.8282, Recall=0.8282, F1=0.8282\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 65, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6392.41 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 3413.63 examples/s]\n",
      "Map: 100%|██████████| 921/921 [00:00<00:00, 8514.19 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:05<00:00,  1.58it/s, loss=0.785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.6974\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:05<00:00,  1.69it/s, loss=0.456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.4200\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:05<00:00,  1.63it/s, loss=0.225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.0370\n",
      "Test Metrics: Precision=0.8575, Recall=0.8575, F1=0.8575\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 65, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65/65 [00:00<00:00, 6330.51 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 3266.79 examples/s]\n",
      "Map: 100%|██████████| 921/921 [00:00<00:00, 8874.60 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:04<00:00,  1.88it/s, loss=1.36] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.8669\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:05<00:00,  1.65it/s, loss=0.518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.5547\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:04<00:00,  1.92it/s, loss=0.424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4468\n",
      "Test Metrics: Precision=0.8564, Recall=0.8564, F1=0.8564\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 70, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 6086.89 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3353.53 examples/s]\n",
      "Map: 100%|██████████| 915/915 [00:00<00:00, 8619.25 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  5.54it/s, loss=0.879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.8065\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:01<00:00,  5.05it/s, loss=0.882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.9065\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:01<00:00,  5.17it/s, loss=0.541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.6869\n",
      "Test Metrics: Precision=0.8424, Recall=0.8424, F1=0.8424\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 70, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 6158.00 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3728.51 examples/s]\n",
      "Map: 100%|██████████| 915/915 [00:00<00:00, 8739.63 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:05<00:00,  1.65it/s, loss=0.705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.2994\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:05<00:00,  1.66it/s, loss=0.715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.9076\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:05<00:00,  1.59it/s, loss=0.409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.1304\n",
      "Test Metrics: Precision=0.8513, Recall=0.8513, F1=0.8513\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 70, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 5546.34 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3396.39 examples/s]\n",
      "Map: 100%|██████████| 915/915 [00:00<00:00, 8756.46 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:05<00:00,  1.75it/s, loss=0.805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.7189\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:04<00:00,  2.00it/s, loss=0.409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.5798\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:05<00:00,  1.74it/s, loss=0.323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.6372\n",
      "Test Metrics: Precision=0.8819, Recall=0.8819, F1=0.8819\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 70, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 7169.05 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3559.67 examples/s]\n",
      "Map: 100%|██████████| 915/915 [00:00<00:00, 4005.28 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  5.54it/s, loss=0.629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.7079\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:01<00:00,  5.92it/s, loss=0.519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.6439\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:01<00:00,  5.93it/s, loss=0.454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.6454\n",
      "Test Metrics: Precision=0.8485, Recall=0.8485, F1=0.8485\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 70, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 7423.92 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3658.58 examples/s]\n",
      "Map: 100%|██████████| 915/915 [00:00<00:00, 8779.45 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:04<00:00,  1.96it/s, loss=0.753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.0524\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:04<00:00,  1.97it/s, loss=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.8026\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:04<00:00,  2.04it/s, loss=0.439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9303\n",
      "Test Metrics: Precision=0.8522, Recall=0.8522, F1=0.8522\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 70, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 6931.75 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3062.65 examples/s]\n",
      "Map: 100%|██████████| 915/915 [00:00<00:00, 8744.17 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:03<00:00,  2.49it/s, loss=0.769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.3702\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:03<00:00,  2.49it/s, loss=0.308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.7955\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:03<00:00,  2.31it/s, loss=0.373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9867\n",
      "Test Metrics: Precision=0.8647, Recall=0.8647, F1=0.8647\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 70, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 6811.62 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3879.25 examples/s]\n",
      "Map: 100%|██████████| 915/915 [00:00<00:00, 8764.77 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  5.39it/s, loss=0.76] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.4050\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:01<00:00,  5.40it/s, loss=0.479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.9084\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:01<00:00,  5.43it/s, loss=0.414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.8508\n",
      "Test Metrics: Precision=0.8505, Recall=0.8505, F1=0.8505\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 70, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 6611.90 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3681.98 examples/s]\n",
      "Map: 100%|██████████| 915/915 [00:00<00:00, 8632.43 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:05<00:00,  1.74it/s, loss=0.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.5590\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:04<00:00,  1.90it/s, loss=0.598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.5126\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:05<00:00,  1.79it/s, loss=0.628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.5237\n",
      "Test Metrics: Precision=0.8615, Recall=0.8615, F1=0.8615\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 70, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 6827.46 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3698.91 examples/s]\n",
      "Map: 100%|██████████| 915/915 [00:00<00:00, 9175.03 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:04<00:00,  1.95it/s, loss=0.818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.1044\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:04<00:00,  1.89it/s, loss=0.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.6342\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:04<00:00,  1.88it/s, loss=0.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.4582\n",
      "Test Metrics: Precision=0.8798, Recall=0.8798, F1=0.8798\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 70, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 7192.58 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3653.80 examples/s]\n",
      "Map: 100%|██████████| 915/915 [00:00<00:00, 8594.77 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  6.43it/s, loss=0.791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.1167\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:01<00:00,  6.20it/s, loss=0.586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.5636\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:01<00:00,  6.20it/s, loss=0.474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.3789\n",
      "Test Metrics: Precision=0.8499, Recall=0.8499, F1=0.8499\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 70, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 7447.84 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3942.02 examples/s]\n",
      "Map: 100%|██████████| 915/915 [00:00<00:00, 8829.30 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:04<00:00,  2.20it/s, loss=0.654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.0144\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:04<00:00,  2.12it/s, loss=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.6426\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:04<00:00,  2.04it/s, loss=0.315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.7625\n",
      "Test Metrics: Precision=0.8652, Recall=0.8652, F1=0.8652\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 70, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 7294.44 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3213.15 examples/s]\n",
      "Map: 100%|██████████| 915/915 [00:00<00:00, 8637.40 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:04<00:00,  2.20it/s, loss=0.534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.8602\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:04<00:00,  2.22it/s, loss=0.33] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.3993\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:04<00:00,  2.17it/s, loss=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.2576\n",
      "Test Metrics: Precision=0.8788, Recall=0.8788, F1=0.8788\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 70, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 6555.65 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3469.44 examples/s]\n",
      "Map: 100%|██████████| 915/915 [00:00<00:00, 8552.08 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:01<00:00,  4.61it/s, loss=0.86] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.8494\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:01<00:00,  4.64it/s, loss=0.506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.4863\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:01<00:00,  4.72it/s, loss=0.529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.0731\n",
      "Test Metrics: Precision=0.8347, Recall=0.8347, F1=0.8347\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 70, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 6976.55 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3846.47 examples/s]\n",
      "Map: 100%|██████████| 915/915 [00:00<00:00, 8859.51 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:05<00:00,  1.52it/s, loss=0.794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.3025\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:05<00:00,  1.54it/s, loss=0.465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.6467\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:06<00:00,  1.46it/s, loss=0.537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.7661\n",
      "Test Metrics: Precision=0.8543, Recall=0.8543, F1=0.8543\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 70, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 7214.50 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3501.71 examples/s]\n",
      "Map: 100%|██████████| 915/915 [00:00<00:00, 8801.12 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9/9 [00:05<00:00,  1.76it/s, loss=0.684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.5723\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9/9 [00:04<00:00,  1.87it/s, loss=0.549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.8971\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9/9 [00:05<00:00,  1.65it/s, loss=0.317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.6398\n",
      "Test Metrics: Precision=0.8802, Recall=0.8802, F1=0.8802\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 75, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 6815.57 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 4648.97 examples/s]\n",
      "Map: 100%|██████████| 909/909 [00:00<00:00, 8815.96 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:01<00:00,  6.08it/s, loss=0.611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.7285\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:01<00:00,  5.59it/s, loss=0.581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.3322\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:01<00:00,  5.42it/s, loss=0.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.2917\n",
      "Test Metrics: Precision=0.8555, Recall=0.8555, F1=0.8555\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 75, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 6770.98 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 4389.18 examples/s]\n",
      "Map: 100%|██████████| 909/909 [00:00<00:00, 8839.22 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:05<00:00,  1.82it/s, loss=0.787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.8362\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:05<00:00,  1.93it/s, loss=0.395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.6199\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:05<00:00,  1.96it/s, loss=0.324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4308\n",
      "Test Metrics: Precision=0.8572, Recall=0.8572, F1=0.8572\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 75, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 6477.09 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 4139.12 examples/s]\n",
      "Map: 100%|██████████| 909/909 [00:00<00:00, 9130.54 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:04<00:00,  2.01it/s, loss=0.584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.9638\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:05<00:00,  1.88it/s, loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.0583\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:05<00:00,  1.94it/s, loss=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.0265\n",
      "Test Metrics: Precision=0.8768, Recall=0.8768, F1=0.8768\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 75, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 7553.31 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3440.59 examples/s]\n",
      "Map: 100%|██████████| 909/909 [00:00<00:00, 8848.76 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:01<00:00,  5.79it/s, loss=0.758]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.6480\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:01<00:00,  6.08it/s, loss=0.486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.0561\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:01<00:00,  6.24it/s, loss=0.708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.0240\n",
      "Test Metrics: Precision=0.8469, Recall=0.8469, F1=0.8469\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 75, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 7575.14 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3359.74 examples/s]\n",
      "Map: 100%|██████████| 909/909 [00:00<00:00, 8795.67 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:05<00:00,  1.99it/s, loss=0.659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.1825\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:05<00:00,  1.97it/s, loss=0.601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.2401\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:04<00:00,  2.03it/s, loss=0.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.0120\n",
      "Test Metrics: Precision=0.8610, Recall=0.8610, F1=0.8610\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 75, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 7328.60 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3018.79 examples/s]\n",
      "Map: 100%|██████████| 909/909 [00:00<00:00, 9035.49 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:04<00:00,  2.34it/s, loss=0.533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.0036\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:04<00:00,  2.15it/s, loss=0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.3211\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:04<00:00,  2.16it/s, loss=0.423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.4871\n",
      "Test Metrics: Precision=0.8883, Recall=0.8883, F1=0.8883\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 75, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 6778.86 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3861.68 examples/s]\n",
      "Map: 100%|██████████| 909/909 [00:00<00:00, 8646.44 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:01<00:00,  6.11it/s, loss=0.632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.4948\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:01<00:00,  5.91it/s, loss=0.395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.9386\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:01<00:00,  5.55it/s, loss=0.446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.5738\n",
      "Test Metrics: Precision=0.8440, Recall=0.8440, F1=0.8440\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 75, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 6963.58 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3597.58 examples/s]\n",
      "Map: 100%|██████████| 909/909 [00:00<00:00, 8871.80 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:05<00:00,  1.98it/s, loss=0.498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.8292\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:05<00:00,  1.81it/s, loss=0.465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.8150\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:05<00:00,  1.83it/s, loss=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.5755\n",
      "Test Metrics: Precision=0.8609, Recall=0.8609, F1=0.8609\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 75, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 6718.91 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3337.82 examples/s]\n",
      "Map: 100%|██████████| 909/909 [00:00<00:00, 9099.51 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:05<00:00,  1.95it/s, loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.1994\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:04<00:00,  2.15it/s, loss=0.352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.0194\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:05<00:00,  1.93it/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.8992\n",
      "Test Metrics: Precision=0.8767, Recall=0.8767, F1=0.8767\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 75, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 7294.27 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 2570.46 examples/s]\n",
      "Map: 100%|██████████| 909/909 [00:00<00:00, 8727.93 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:01<00:00,  6.43it/s, loss=0.858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.7041\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:01<00:00,  6.54it/s, loss=0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.9300\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:01<00:00,  6.62it/s, loss=0.318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4087\n",
      "Test Metrics: Precision=0.8560, Recall=0.8560, F1=0.8560\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 75, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 7280.43 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 2533.10 examples/s]\n",
      "Map: 100%|██████████| 909/909 [00:00<00:00, 8757.70 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:04<00:00,  2.14it/s, loss=0.727]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.7281\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:04<00:00,  2.23it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.6355\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:04<00:00,  2.16it/s, loss=0.434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.0418\n",
      "Test Metrics: Precision=0.8656, Recall=0.8656, F1=0.8656\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 75, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 7316.50 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 2416.82 examples/s]\n",
      "Map: 100%|██████████| 909/909 [00:00<00:00, 8800.90 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:04<00:00,  2.42it/s, loss=0.524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.3069\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:04<00:00,  2.18it/s, loss=0.588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.5264\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:04<00:00,  2.29it/s, loss=0.232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.8017\n",
      "Test Metrics: Precision=0.8791, Recall=0.8791, F1=0.8791\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 75, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 7103.37 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 4457.92 examples/s]\n",
      "Map: 100%|██████████| 909/909 [00:00<00:00, 8661.00 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:01<00:00,  5.18it/s, loss=0.632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.8033\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:01<00:00,  5.36it/s, loss=0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.6870\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:01<00:00,  5.16it/s, loss=0.547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4496\n",
      "Test Metrics: Precision=0.8433, Recall=0.8433, F1=0.8433\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 75, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 7263.79 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 4425.93 examples/s]\n",
      "Map: 100%|██████████| 909/909 [00:00<00:00, 8648.85 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:06<00:00,  1.62it/s, loss=0.527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.2072\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:05<00:00,  1.67it/s, loss=0.594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.8436\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:05<00:00,  1.67it/s, loss=0.605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.8695\n",
      "Test Metrics: Precision=0.8276, Recall=0.8276, F1=0.8276\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 75, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 6862.11 examples/s]\n",
      "Map: 100%|██████████| 15/15 [00:00<00:00, 3949.93 examples/s]\n",
      "Map: 100%|██████████| 909/909 [00:00<00:00, 8812.76 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:05<00:00,  1.87it/s, loss=0.918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.2581\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:05<00:00,  1.78it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.7175\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:05<00:00,  1.84it/s, loss=0.208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9415\n",
      "Test Metrics: Precision=0.8853, Recall=0.8853, F1=0.8853\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 80, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6790.20 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 3576.09 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:00<00:00, 8836.46 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:01<00:00,  5.51it/s, loss=0.758]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.8995\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:01<00:00,  5.08it/s, loss=0.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.7220\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:01<00:00,  5.52it/s, loss=0.462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4011\n",
      "Test Metrics: Precision=0.8447, Recall=0.8447, F1=0.8447\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 80, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6961.21 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 3624.57 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:00<00:00, 8797.32 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:05<00:00,  1.72it/s, loss=0.549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.9469\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:05<00:00,  1.72it/s, loss=0.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.0573\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:05<00:00,  1.67it/s, loss=0.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.1860\n",
      "Test Metrics: Precision=0.8675, Recall=0.8675, F1=0.8675\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 80, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6769.92 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 3257.71 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:00<00:00, 9047.18 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:05<00:00,  1.97it/s, loss=0.559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.1742\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:05<00:00,  1.81it/s, loss=0.473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.6152\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:05<00:00,  1.82it/s, loss=0.437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.5696\n",
      "Test Metrics: Precision=0.8851, Recall=0.8851, F1=0.8851\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 80, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7872.75 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 4379.33 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:00<00:00, 8962.36 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:01<00:00,  5.95it/s, loss=0.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.5291\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:01<00:00,  5.64it/s, loss=0.563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.0383\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:01<00:00,  5.75it/s, loss=0.485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.8067\n",
      "Test Metrics: Precision=0.8550, Recall=0.8550, F1=0.8550\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 80, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7509.44 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 3480.75 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:00<00:00, 8906.07 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:05<00:00,  1.94it/s, loss=0.757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.5550\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:05<00:00,  1.92it/s, loss=0.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.3112\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:05<00:00,  1.84it/s, loss=0.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.1967\n",
      "Test Metrics: Precision=0.8407, Recall=0.8407, F1=0.8407\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 80, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7370.87 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 3817.34 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:00<00:00, 8822.65 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:05<00:00,  2.00it/s, loss=0.526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.3606\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:04<00:00,  2.08it/s, loss=0.564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.2039\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:04<00:00,  2.10it/s, loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.1358\n",
      "Test Metrics: Precision=0.8948, Recall=0.8948, F1=0.8948\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 80, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7123.48 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 4175.51 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:00<00:00, 8951.09 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:01<00:00,  5.01it/s, loss=0.888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.4272\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:01<00:00,  5.02it/s, loss=0.813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.9568\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:02<00:00,  4.95it/s, loss=0.513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.3639\n",
      "Test Metrics: Precision=0.8514, Recall=0.8514, F1=0.8514\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 80, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7254.38 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 4123.18 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:00<00:00, 8988.52 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:06<00:00,  1.61it/s, loss=0.533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.2579\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:05<00:00,  1.69it/s, loss=0.517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.1106\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:06<00:00,  1.61it/s, loss=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.0819\n",
      "Test Metrics: Precision=0.8689, Recall=0.8689, F1=0.8689\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 80, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6673.25 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 3361.66 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:00<00:00, 8556.86 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:05<00:00,  1.90it/s, loss=0.869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.6904\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:05<00:00,  1.83it/s, loss=0.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.2360\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:05<00:00,  1.77it/s, loss=0.406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9914\n",
      "Test Metrics: Precision=0.8809, Recall=0.8809, F1=0.8809\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 80, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7296.82 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 4543.29 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:00<00:00, 4005.43 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:01<00:00,  6.07it/s, loss=0.722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.1486\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:01<00:00,  5.91it/s, loss=0.396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.8274\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:01<00:00,  6.08it/s, loss=0.642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.7209\n",
      "Test Metrics: Precision=0.8511, Recall=0.8511, F1=0.8511\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 80, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7294.92 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 4489.49 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:00<00:00, 8662.39 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:05<00:00,  1.89it/s, loss=0.717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.0293\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:05<00:00,  1.97it/s, loss=0.591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.9325\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:04<00:00,  2.03it/s, loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.1588\n",
      "Test Metrics: Precision=0.8626, Recall=0.8626, F1=0.8626\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 80, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 7259.41 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 4261.69 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:00<00:00, 9022.24 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:05<00:00,  1.96it/s, loss=0.534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.3452\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:04<00:00,  2.06it/s, loss=0.465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.3407\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:04<00:00,  2.20it/s, loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.3380\n",
      "Test Metrics: Precision=0.8848, Recall=0.8848, F1=0.8848\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 80, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6961.93 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 4047.82 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:00<00:00, 8747.19 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:02<00:00,  4.89it/s, loss=0.746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.9599\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:02<00:00,  4.92it/s, loss=0.782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.8266\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:02<00:00,  4.54it/s, loss=0.616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3532\n",
      "Test Metrics: Precision=0.8145, Recall=0.8145, F1=0.8145\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 80, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6980.76 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 4025.73 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:00<00:00, 8606.29 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:06<00:00,  1.59it/s, loss=0.573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.8944\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:05<00:00,  1.74it/s, loss=0.423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.8986\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:06<00:00,  1.48it/s, loss=0.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4992\n",
      "Test Metrics: Precision=0.8537, Recall=0.8537, F1=0.8537\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 80, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 6722.18 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 3645.44 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:00<00:00, 8994.45 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:05<00:00,  1.72it/s, loss=0.539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.6929\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:06<00:00,  1.66it/s, loss=0.376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.5963\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:05<00:00,  1.86it/s, loss=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.3554\n",
      "Test Metrics: Precision=0.8948, Recall=0.8948, F1=0.8948\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 85, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 6969.87 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 3798.58 examples/s]\n",
      "Map: 100%|██████████| 897/897 [00:00<00:00, 4057.80 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 11/11 [00:01<00:00,  6.02it/s, loss=0.828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.4929\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 11/11 [00:02<00:00,  5.33it/s, loss=0.749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.4429\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 11/11 [00:01<00:00,  5.87it/s, loss=0.49] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.0136\n",
      "Test Metrics: Precision=0.8571, Recall=0.8571, F1=0.8571\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 85, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 7166.58 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 3466.70 examples/s]\n",
      "Map: 100%|██████████| 897/897 [00:00<00:00, 8832.83 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 11/11 [00:06<00:00,  1.80it/s, loss=0.647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.8913\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 11/11 [00:05<00:00,  1.87it/s, loss=0.514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.0979\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 11/11 [00:05<00:00,  1.87it/s, loss=0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9888\n",
      "Test Metrics: Precision=0.8744, Recall=0.8744, F1=0.8744\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 85, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 7013.75 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 3501.43 examples/s]\n",
      "Map: 100%|██████████| 897/897 [00:00<00:00, 8918.51 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 11/11 [00:05<00:00,  1.91it/s, loss=0.348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.8490\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 11/11 [00:05<00:00,  1.95it/s, loss=0.643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.7882\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 11/11 [00:05<00:00,  1.92it/s, loss=0.399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9067\n",
      "Test Metrics: Precision=0.8837, Recall=0.8837, F1=0.8837\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 85, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 7856.06 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 4552.91 examples/s]\n",
      "Map: 100%|██████████| 897/897 [00:00<00:00, 8937.02 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 11/11 [00:01<00:00,  6.05it/s, loss=0.781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.3980\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 11/11 [00:01<00:00,  5.87it/s, loss=0.511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.2789\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 11/11 [00:01<00:00,  6.14it/s, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.9067\n",
      "Test Metrics: Precision=0.8580, Recall=0.8580, F1=0.8580\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 85, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 7861.43 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 4651.52 examples/s]\n",
      "Map: 100%|██████████| 897/897 [00:00<00:00, 8986.12 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 11/11 [00:05<00:00,  1.97it/s, loss=0.638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.2221\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 11/11 [00:05<00:00,  1.92it/s, loss=0.379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.0010\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 11/11 [00:05<00:00,  2.00it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.8610\n",
      "Test Metrics: Precision=0.8706, Recall=0.8706, F1=0.8706\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 85, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 7357.37 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 3809.54 examples/s]\n",
      "Map: 100%|██████████| 897/897 [00:00<00:00, 8948.84 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 11/11 [00:05<00:00,  2.08it/s, loss=0.747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.7431\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 11/11 [00:05<00:00,  2.15it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.3053\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 11/11 [00:05<00:00,  2.08it/s, loss=0.206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.2758\n",
      "Test Metrics: Precision=0.8948, Recall=0.8948, F1=0.8948\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 85, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 6994.48 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 4249.04 examples/s]\n",
      "Map: 100%|██████████| 897/897 [00:00<00:00, 4198.36 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 11/11 [00:02<00:00,  5.21it/s, loss=0.767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.0036\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 11/11 [00:01<00:00,  5.51it/s, loss=0.505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.3691\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 11/11 [00:02<00:00,  5.28it/s, loss=0.553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.1370\n",
      "Test Metrics: Precision=0.8554, Recall=0.8554, F1=0.8554\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 85, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 7097.95 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 4518.01 examples/s]\n",
      "Map: 100%|██████████| 897/897 [00:00<00:00, 8843.75 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 11/11 [00:06<00:00,  1.70it/s, loss=0.733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.2456\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 11/11 [00:06<00:00,  1.69it/s, loss=0.434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.7141\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 11/11 [00:06<00:00,  1.67it/s, loss=0.486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4940\n",
      "Test Metrics: Precision=0.8546, Recall=0.8546, F1=0.8546\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 85, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 6752.32 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 4119.43 examples/s]\n",
      "Map: 100%|██████████| 897/897 [00:00<00:00, 8887.64 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 11/11 [00:06<00:00,  1.83it/s, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.2544\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 11/11 [00:05<00:00,  2.01it/s, loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.4729\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 11/11 [00:05<00:00,  1.95it/s, loss=0.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.5952\n",
      "Test Metrics: Precision=0.8947, Recall=0.8947, F1=0.8947\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 85, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 7472.87 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 3597.90 examples/s]\n",
      "Map: 100%|██████████| 897/897 [00:00<00:00, 8698.33 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 11/11 [00:01<00:00,  5.87it/s, loss=0.456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.8749\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 11/11 [00:01<00:00,  6.43it/s, loss=0.698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.3999\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 11/11 [00:01<00:00,  5.69it/s, loss=0.502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.0044\n",
      "Test Metrics: Precision=0.8489, Recall=0.8489, F1=0.8489\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 85, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 6859.11 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 4028.43 examples/s]\n",
      "Map: 100%|██████████| 897/897 [00:00<00:00, 8615.67 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 11/11 [00:05<00:00,  2.07it/s, loss=0.703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.3892\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 11/11 [00:05<00:00,  1.95it/s, loss=0.416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.6237\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 11/11 [00:05<00:00,  2.02it/s, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9060\n",
      "Test Metrics: Precision=0.8692, Recall=0.8692, F1=0.8692\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 85, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 7344.78 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 3906.38 examples/s]\n",
      "Map: 100%|██████████| 897/897 [00:00<00:00, 8826.32 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 11/11 [00:05<00:00,  2.14it/s, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.5551\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 11/11 [00:05<00:00,  1.96it/s, loss=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.0650\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 11/11 [00:05<00:00,  2.09it/s, loss=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.1654\n",
      "Test Metrics: Precision=0.8990, Recall=0.8990, F1=0.8990\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 85, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 7139.46 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 4132.80 examples/s]\n",
      "Map: 100%|██████████| 897/897 [00:00<00:00, 8914.18 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 11/11 [00:02<00:00,  4.55it/s, loss=0.921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.3059\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 11/11 [00:02<00:00,  4.67it/s, loss=0.454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.6207\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 11/11 [00:02<00:00,  4.73it/s, loss=0.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0807\n",
      "Test Metrics: Precision=0.8371, Recall=0.8371, F1=0.8371\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 85, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 7170.33 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 4148.43 examples/s]\n",
      "Map: 100%|██████████| 897/897 [00:00<00:00, 8585.14 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 11/11 [00:07<00:00,  1.54it/s, loss=0.579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.0637\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 11/11 [00:06<00:00,  1.60it/s, loss=0.724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.4260\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 11/11 [00:07<00:00,  1.54it/s, loss=0.448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.9271\n",
      "Test Metrics: Precision=0.8631, Recall=0.8631, F1=0.8631\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 85, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 7121.63 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 3723.21 examples/s]\n",
      "Map: 100%|██████████| 897/897 [00:00<00:00, 9177.21 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 11/11 [00:05<00:00,  1.86it/s, loss=0.655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.2808\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 11/11 [00:06<00:00,  1.67it/s, loss=0.644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.5113\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 11/11 [00:06<00:00,  1.65it/s, loss=0.554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.3398\n",
      "Test Metrics: Precision=0.8834, Recall=0.8834, F1=0.8834\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 90, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7396.20 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 4884.04 examples/s]\n",
      "Map: 100%|██████████| 891/891 [00:00<00:00, 8664.11 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:02<00:00,  5.60it/s, loss=0.624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.5682\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:02<00:00,  5.14it/s, loss=0.475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.6673\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:02<00:00,  5.37it/s, loss=0.587]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.5161\n",
      "Test Metrics: Precision=0.8562, Recall=0.8562, F1=0.8562\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 90, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7369.20 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 4489.89 examples/s]\n",
      "Map: 100%|██████████| 891/891 [00:00<00:00, 8777.56 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:06<00:00,  1.93it/s, loss=0.43] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.5568\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:06<00:00,  1.81it/s, loss=0.588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.9528\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:06<00:00,  1.90it/s, loss=0.545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.9717\n",
      "Test Metrics: Precision=0.8704, Recall=0.8704, F1=0.8704\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 90, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 6980.29 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 4579.77 examples/s]\n",
      "Map: 100%|██████████| 891/891 [00:00<00:00, 8995.37 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:06<00:00,  1.95it/s, loss=0.313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.4931\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:05<00:00,  2.11it/s, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.6299\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:05<00:00,  2.04it/s, loss=0.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.2694\n",
      "Test Metrics: Precision=0.8914, Recall=0.8914, F1=0.8914\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 90, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7622.93 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 4184.31 examples/s]\n",
      "Map: 100%|██████████| 891/891 [00:00<00:00, 8568.28 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:01<00:00,  6.23it/s, loss=0.745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.4625\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:01<00:00,  6.39it/s, loss=0.218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.5870\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:01<00:00,  6.32it/s, loss=0.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4239\n",
      "Test Metrics: Precision=0.8569, Recall=0.8569, F1=0.8569\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 90, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 8082.20 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 4978.07 examples/s]\n",
      "Map: 100%|██████████| 891/891 [00:00<00:00, 8702.14 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:05<00:00,  2.11it/s, loss=0.826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.7772\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:06<00:00,  1.96it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.4148\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:05<00:00,  2.09it/s, loss=0.452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4319\n",
      "Test Metrics: Precision=0.8673, Recall=0.8673, F1=0.8673\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 90, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7989.32 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 4285.73 examples/s]\n",
      "Map: 100%|██████████| 891/891 [00:00<00:00, 8977.43 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:05<00:00,  2.24it/s, loss=0.565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.8560\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:05<00:00,  2.17it/s, loss=0.887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.1053\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:05<00:00,  2.13it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.2321\n",
      "Test Metrics: Precision=0.9015, Recall=0.9015, F1=0.9015\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 90, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7237.38 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 3720.92 examples/s]\n",
      "Map: 100%|██████████| 891/891 [00:00<00:00, 8998.44 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:02<00:00,  5.40it/s, loss=0.973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.7491\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:02<00:00,  5.27it/s, loss=0.303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.3987\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:02<00:00,  5.16it/s, loss=0.557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.1942\n",
      "Test Metrics: Precision=0.8518, Recall=0.8518, F1=0.8518\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 90, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 6918.00 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 3487.34 examples/s]\n",
      "Map: 100%|██████████| 891/891 [00:00<00:00, 8697.82 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:07<00:00,  1.70it/s, loss=0.424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.4358\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:06<00:00,  1.87it/s, loss=0.882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.9899\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:06<00:00,  1.79it/s, loss=0.346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.3198\n",
      "Test Metrics: Precision=0.8780, Recall=0.8780, F1=0.8780\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 90, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 6751.09 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 3152.56 examples/s]\n",
      "Map: 100%|██████████| 891/891 [00:00<00:00, 8818.67 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:06<00:00,  1.72it/s, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.8776\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:06<00:00,  1.89it/s, loss=0.606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.0031\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:05<00:00,  2.04it/s, loss=0.105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.5547\n",
      "Test Metrics: Precision=0.8972, Recall=0.8972, F1=0.8972\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 90, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 6607.40 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 3155.98 examples/s]\n",
      "Map: 100%|██████████| 891/891 [00:00<00:00, 8568.36 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:01<00:00,  6.09it/s, loss=0.612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.4691\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:02<00:00,  5.54it/s, loss=0.498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.0823\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:02<00:00,  5.94it/s, loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4065\n",
      "Test Metrics: Precision=0.8518, Recall=0.8518, F1=0.8518\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 90, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 6922.31 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 3909.76 examples/s]\n",
      "Map: 100%|██████████| 891/891 [00:00<00:00, 8600.94 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:06<00:00,  1.72it/s, loss=0.624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.6538\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:06<00:00,  1.98it/s, loss=0.563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.1696\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:06<00:00,  1.86it/s, loss=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.8134\n",
      "Test Metrics: Precision=0.8769, Recall=0.8769, F1=0.8769\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 90, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7470.26 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 3954.82 examples/s]\n",
      "Map: 100%|██████████| 891/891 [00:00<00:00, 9050.33 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:05<00:00,  2.03it/s, loss=0.858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.6895\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:05<00:00,  2.09it/s, loss=0.113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.7634\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:05<00:00,  2.00it/s, loss=0.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9812\n",
      "Test Metrics: Precision=0.8814, Recall=0.8814, F1=0.8814\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 90, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7451.98 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 4781.35 examples/s]\n",
      "Map: 100%|██████████| 891/891 [00:00<00:00, 8956.20 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:02<00:00,  5.07it/s, loss=1.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.9110\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:02<00:00,  4.80it/s, loss=0.539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.6398\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:02<00:00,  4.93it/s, loss=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2133\n",
      "Test Metrics: Precision=0.8242, Recall=0.8242, F1=0.8242\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 90, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7076.47 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 4368.06 examples/s]\n",
      "Map: 100%|██████████| 891/891 [00:00<00:00, 8469.81 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:07<00:00,  1.69it/s, loss=0.995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.4094\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:07<00:00,  1.51it/s, loss=0.576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.3648\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:07<00:00,  1.63it/s, loss=0.429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.3490\n",
      "Test Metrics: Precision=0.8587, Recall=0.8587, F1=0.8587\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 90, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90/90 [00:00<00:00, 7057.69 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 4147.76 examples/s]\n",
      "Map: 100%|██████████| 891/891 [00:00<00:00, 8766.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:06<00:00,  1.87it/s, loss=0.602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.5714\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:06<00:00,  1.72it/s, loss=0.947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.2619\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:06<00:00,  1.85it/s, loss=0.494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.5644\n",
      "Test Metrics: Precision=0.8862, Recall=0.8862, F1=0.8862\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 95, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7135.21 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 4402.37 examples/s]\n",
      "Map: 100%|██████████| 885/885 [00:00<00:00, 8480.37 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:02<00:00,  5.18it/s, loss=0.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.8161\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:02<00:00,  5.27it/s, loss=0.497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.3672\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:02<00:00,  5.05it/s, loss=0.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.1494\n",
      "Test Metrics: Precision=0.8625, Recall=0.8625, F1=0.8625\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 95, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7125.52 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 4601.41 examples/s]\n",
      "Map: 100%|██████████| 885/885 [00:00<00:00, 8857.53 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:07<00:00,  1.69it/s, loss=0.688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.3681\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:06<00:00,  1.78it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.1920\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:07<00:00,  1.69it/s, loss=0.49] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.2796\n",
      "Test Metrics: Precision=0.8591, Recall=0.8591, F1=0.8591\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 95, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7111.65 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 4061.14 examples/s]\n",
      "Map: 100%|██████████| 885/885 [00:00<00:00, 8982.51 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:06<00:00,  1.94it/s, loss=0.476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.1011\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:06<00:00,  1.93it/s, loss=0.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.7517\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:06<00:00,  1.80it/s, loss=0.283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.7594\n",
      "Test Metrics: Precision=0.8996, Recall=0.8996, F1=0.8996\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 95, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 95/95 [00:00<00:00, 8020.02 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 4909.85 examples/s]\n",
      "Map: 100%|██████████| 885/885 [00:00<00:00, 8601.86 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:02<00:00,  5.57it/s, loss=0.662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.9022\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:02<00:00,  5.61it/s, loss=0.496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.5502\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:02<00:00,  5.80it/s, loss=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4561\n",
      "Test Metrics: Precision=0.8604, Recall=0.8604, F1=0.8604\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 95, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7950.26 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 3715.75 examples/s]\n",
      "Map: 100%|██████████| 885/885 [00:00<00:00, 8336.98 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:06<00:00,  1.85it/s, loss=0.627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.9853\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:06<00:00,  1.81it/s, loss=0.401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.9142\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:06<00:00,  1.84it/s, loss=0.447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.7477\n",
      "Test Metrics: Precision=0.8624, Recall=0.8624, F1=0.8624\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 95, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7647.67 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 4410.66 examples/s]\n",
      "Map: 100%|██████████| 885/885 [00:00<00:00, 8597.67 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:06<00:00,  1.84it/s, loss=0.397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.1373\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:05<00:00,  2.02it/s, loss=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.3389\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:06<00:00,  1.97it/s, loss=0.175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.1446\n",
      "Test Metrics: Precision=0.9008, Recall=0.9008, F1=0.9008\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 95, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7235.50 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 4239.83 examples/s]\n",
      "Map: 100%|██████████| 885/885 [00:00<00:00, 8807.26 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:02<00:00,  5.03it/s, loss=0.983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.0836\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:02<00:00,  4.41it/s, loss=0.837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.1794\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:02<00:00,  4.75it/s, loss=0.614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5956\n",
      "Test Metrics: Precision=0.8491, Recall=0.8491, F1=0.8491\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 95, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7098.48 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 4293.28 examples/s]\n",
      "Map: 100%|██████████| 885/885 [00:00<00:00, 8894.22 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:07<00:00,  1.54it/s, loss=0.677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.9161\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:07<00:00,  1.62it/s, loss=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.5753\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:08<00:00,  1.48it/s, loss=0.488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4983\n",
      "Test Metrics: Precision=0.8543, Recall=0.8543, F1=0.8543\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 95, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 95/95 [00:00<00:00, 6622.44 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 3968.32 examples/s]\n",
      "Map: 100%|██████████| 885/885 [00:00<00:00, 8996.99 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:07<00:00,  1.62it/s, loss=0.64] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.2538\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:07<00:00,  1.67it/s, loss=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.2346\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:07<00:00,  1.66it/s, loss=0.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9152\n",
      "Test Metrics: Precision=0.9004, Recall=0.9004, F1=0.9004\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 95, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7762.39 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 4594.25 examples/s]\n",
      "Map: 100%|██████████| 885/885 [00:00<00:00, 8478.30 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:02<00:00,  5.89it/s, loss=0.878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.4913\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:02<00:00,  5.63it/s, loss=0.362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.3195\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:02<00:00,  5.66it/s, loss=0.358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.1219\n",
      "Test Metrics: Precision=0.8557, Recall=0.8557, F1=0.8557\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 95, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7328.11 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 4879.19 examples/s]\n",
      "Map: 100%|██████████| 885/885 [00:00<00:00, 8800.28 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:06<00:00,  1.82it/s, loss=0.396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.4843\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:06<00:00,  1.84it/s, loss=0.352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.4313\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:06<00:00,  1.79it/s, loss=0.44] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.3957\n",
      "Test Metrics: Precision=0.8696, Recall=0.8696, F1=0.8696\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 95, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7254.60 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 4274.39 examples/s]\n",
      "Map: 100%|██████████| 885/885 [00:00<00:00, 8799.93 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:05<00:00,  2.01it/s, loss=0.471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.8265\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:06<00:00,  1.83it/s, loss=0.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.7911\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:06<00:00,  1.91it/s, loss=0.408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9234\n",
      "Test Metrics: Precision=0.8841, Recall=0.8841, F1=0.8841\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 95, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 95/95 [00:00<00:00, 5720.71 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 3484.10 examples/s]\n",
      "Map: 100%|██████████| 885/885 [00:00<00:00, 8624.90 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:02<00:00,  4.58it/s, loss=0.694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.0491\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:02<00:00,  4.49it/s, loss=0.556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.9422\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:02<00:00,  4.74it/s, loss=0.612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.4974\n",
      "Test Metrics: Precision=0.8451, Recall=0.8451, F1=0.8451\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 95, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7639.61 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 4363.56 examples/s]\n",
      "Map: 100%|██████████| 885/885 [00:00<00:00, 9020.34 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:08<00:00,  1.46it/s, loss=0.713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.3238\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:07<00:00,  1.62it/s, loss=0.314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.3012\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:07<00:00,  1.57it/s, loss=0.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.9835\n",
      "Test Metrics: Precision=0.8636, Recall=0.8636, F1=0.8636\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 95, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 95/95 [00:00<00:00, 7148.14 examples/s]\n",
      "Map: 100%|██████████| 19/19 [00:00<00:00, 3850.03 examples/s]\n",
      "Map: 100%|██████████| 885/885 [00:00<00:00, 9135.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12/12 [00:06<00:00,  1.74it/s, loss=0.473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.0451\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12/12 [00:06<00:00,  1.72it/s, loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.8887\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12/12 [00:06<00:00,  1.85it/s, loss=0.479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.7744\n",
      "Test Metrics: Precision=0.9001, Recall=0.9001, F1=0.9001\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 100, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7794.22 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 5620.51 examples/s]\n",
      "Map: 100%|██████████| 879/879 [00:00<00:00, 8789.02 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 13/13 [00:02<00:00,  5.48it/s, loss=0.574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.6611\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 13/13 [00:02<00:00,  5.48it/s, loss=0.449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.0177\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 13/13 [00:02<00:00,  5.54it/s, loss=0.411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.5929\n",
      "Test Metrics: Precision=0.8689, Recall=0.8689, F1=0.8689\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 100, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7416.59 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4855.36 examples/s]\n",
      "Map: 100%|██████████| 879/879 [00:00<00:00, 8758.83 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 13/13 [00:07<00:00,  1.70it/s, loss=0.686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.9319\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 13/13 [00:07<00:00,  1.79it/s, loss=0.394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.6462\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 13/13 [00:06<00:00,  1.96it/s, loss=0.309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.6185\n",
      "Test Metrics: Precision=0.8781, Recall=0.8781, F1=0.8781\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 100, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7667.83 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4795.96 examples/s]\n",
      "Map: 100%|██████████| 879/879 [00:00<00:00, 4068.31 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 13/13 [00:06<00:00,  1.90it/s, loss=0.426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.9702\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 13/13 [00:06<00:00,  1.87it/s, loss=0.345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.8257\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 13/13 [00:06<00:00,  1.93it/s, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.6925\n",
      "Test Metrics: Precision=0.8884, Recall=0.8884, F1=0.8884\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 100, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8086.34 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4344.40 examples/s]\n",
      "Map: 100%|██████████| 879/879 [00:00<00:00, 8557.96 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 13/13 [00:02<00:00,  5.73it/s, loss=0.631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.8926\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 13/13 [00:02<00:00,  6.15it/s, loss=0.49] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.8844\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 13/13 [00:02<00:00,  6.01it/s, loss=0.615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4240\n",
      "Test Metrics: Precision=0.8583, Recall=0.8583, F1=0.8583\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 100, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8388.27 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4400.70 examples/s]\n",
      "Map: 100%|██████████| 879/879 [00:00<00:00, 8830.60 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 13/13 [00:06<00:00,  2.02it/s, loss=0.303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.2222\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 13/13 [00:06<00:00,  1.90it/s, loss=0.373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.8058\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 13/13 [00:06<00:00,  1.96it/s, loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.5500\n",
      "Test Metrics: Precision=0.8700, Recall=0.8700, F1=0.8700\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 100, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8281.28 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4068.58 examples/s]\n",
      "Map: 100%|██████████| 879/879 [00:00<00:00, 8804.81 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 13/13 [00:06<00:00,  1.98it/s, loss=0.411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.9359\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 13/13 [00:06<00:00,  2.08it/s, loss=0.151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.6818\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 13/13 [00:06<00:00,  2.04it/s, loss=0.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.6651\n",
      "Test Metrics: Precision=0.9021, Recall=0.9021, F1=0.9021\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 100, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7269.66 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4706.62 examples/s]\n",
      "Map: 100%|██████████| 879/879 [00:00<00:00, 8710.39 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 13/13 [00:02<00:00,  5.02it/s, loss=0.731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.0902\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 13/13 [00:02<00:00,  5.44it/s, loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.8005\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 13/13 [00:02<00:00,  5.19it/s, loss=0.508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.6374\n",
      "Test Metrics: Precision=0.8678, Recall=0.8678, F1=0.8678\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 100, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7400.10 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4731.84 examples/s]\n",
      "Map: 100%|██████████| 879/879 [00:00<00:00, 8805.69 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 13/13 [00:08<00:00,  1.60it/s, loss=0.777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.2190\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 13/13 [00:07<00:00,  1.69it/s, loss=0.404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.4743\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 13/13 [00:07<00:00,  1.80it/s, loss=0.515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.2639\n",
      "Test Metrics: Precision=0.8682, Recall=0.8682, F1=0.8682\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 100, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7549.14 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4895.60 examples/s]\n",
      "Map: 100%|██████████| 879/879 [00:00<00:00, 9148.92 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 13/13 [00:07<00:00,  1.79it/s, loss=0.466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.2214\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 13/13 [00:07<00:00,  1.83it/s, loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.4382\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 13/13 [00:06<00:00,  1.86it/s, loss=0.347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.2951\n",
      "Test Metrics: Precision=0.8992, Recall=0.8992, F1=0.8992\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 100, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7866.14 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 5201.91 examples/s]\n",
      "Map: 100%|██████████| 879/879 [00:00<00:00, 8662.64 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 13/13 [00:02<00:00,  6.24it/s, loss=0.677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.3246\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 13/13 [00:02<00:00,  5.87it/s, loss=0.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.8879\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 13/13 [00:02<00:00,  6.08it/s, loss=0.423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4297\n",
      "Test Metrics: Precision=0.8580, Recall=0.8580, F1=0.8580\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 100, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7435.79 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4734.78 examples/s]\n",
      "Map: 100%|██████████| 879/879 [00:00<00:00, 8333.60 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 13/13 [00:06<00:00,  1.93it/s, loss=0.56] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.8625\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 13/13 [00:07<00:00,  1.82it/s, loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.6694\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 13/13 [00:07<00:00,  1.84it/s, loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4203\n",
      "Test Metrics: Precision=0.8698, Recall=0.8698, F1=0.8698\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 100, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7609.96 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4668.64 examples/s]\n",
      "Map: 100%|██████████| 879/879 [00:00<00:00, 8843.46 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 13/13 [00:06<00:00,  2.03it/s, loss=0.522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.9090\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 13/13 [00:06<00:00,  2.00it/s, loss=0.33] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.3013\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 13/13 [00:06<00:00,  2.06it/s, loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.1501\n",
      "Test Metrics: Precision=0.9084, Recall=0.9084, F1=0.9084\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 100, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7763.35 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4790.75 examples/s]\n",
      "Map: 100%|██████████| 879/879 [00:00<00:00, 8873.66 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 13/13 [00:02<00:00,  4.85it/s, loss=0.791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.2762\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 13/13 [00:02<00:00,  4.91it/s, loss=0.598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.6349\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 13/13 [00:02<00:00,  5.00it/s, loss=0.44] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3325\n",
      "Test Metrics: Precision=0.8515, Recall=0.8515, F1=0.8515\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 100, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7792.63 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4897.31 examples/s]\n",
      "Map: 100%|██████████| 879/879 [00:00<00:00, 4011.36 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 13/13 [00:08<00:00,  1.57it/s, loss=0.528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.1389\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 13/13 [00:08<00:00,  1.62it/s, loss=0.572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.1504\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 13/13 [00:07<00:00,  1.66it/s, loss=0.47] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0673\n",
      "Test Metrics: Precision=0.8617, Recall=0.8617, F1=0.8617\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 100, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7773.71 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 4201.66 examples/s]\n",
      "Map: 100%|██████████| 879/879 [00:00<00:00, 9170.40 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 13/13 [00:06<00:00,  1.87it/s, loss=0.423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.6119\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 13/13 [00:06<00:00,  1.96it/s, loss=0.439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.4203\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 13/13 [00:07<00:00,  1.85it/s, loss=0.255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9968\n",
      "Test Metrics: Precision=0.9019, Recall=0.9019, F1=0.9019\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 105, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 7834.10 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 4548.67 examples/s]\n",
      "Map: 100%|██████████| 873/873 [00:00<00:00, 9001.21 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:02<00:00,  5.56it/s, loss=0.489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.8711\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:02<00:00,  5.35it/s, loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.5396\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:02<00:00,  5.86it/s, loss=0.724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.1714\n",
      "Test Metrics: Precision=0.8625, Recall=0.8625, F1=0.8625\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 105, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 7452.57 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 4406.88 examples/s]\n",
      "Map: 100%|██████████| 873/873 [00:00<00:00, 8830.54 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:07<00:00,  1.92it/s, loss=0.426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.7227\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:07<00:00,  1.90it/s, loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.1458\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:07<00:00,  1.85it/s, loss=0.458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.2276\n",
      "Test Metrics: Precision=0.8714, Recall=0.8714, F1=0.8714\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 105, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 7637.51 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 4148.08 examples/s]\n",
      "Map: 100%|██████████| 873/873 [00:00<00:00, 9113.67 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:07<00:00,  2.00it/s, loss=0.482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.9540\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:07<00:00,  1.92it/s, loss=0.112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.6395\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:07<00:00,  1.99it/s, loss=0.42] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.1803\n",
      "Test Metrics: Precision=0.9033, Recall=0.9033, F1=0.9033\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 105, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 8594.55 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 5300.62 examples/s]\n",
      "Map: 100%|██████████| 873/873 [00:00<00:00, 8901.32 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:02<00:00,  6.44it/s, loss=0.584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.4750\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:02<00:00,  6.09it/s, loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.7761\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:02<00:00,  6.09it/s, loss=0.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.3356\n",
      "Test Metrics: Precision=0.8654, Recall=0.8654, F1=0.8654\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 105, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 8376.96 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 4801.59 examples/s]\n",
      "Map: 100%|██████████| 873/873 [00:00<00:00, 8719.51 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:06<00:00,  2.10it/s, loss=0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.0651\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:06<00:00,  2.07it/s, loss=0.885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.2596\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:06<00:00,  2.13it/s, loss=0.635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.9508\n",
      "Test Metrics: Precision=0.8811, Recall=0.8811, F1=0.8811\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 105, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 7912.64 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 4765.48 examples/s]\n",
      "Map: 100%|██████████| 873/873 [00:00<00:00, 8814.81 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:06<00:00,  2.11it/s, loss=0.318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.2596\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:06<00:00,  2.02it/s, loss=0.146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.5649\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:06<00:00,  2.04it/s, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.5289\n",
      "Test Metrics: Precision=0.8952, Recall=0.8952, F1=0.8952\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 105, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 7307.39 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 4744.69 examples/s]\n",
      "Map: 100%|██████████| 873/873 [00:00<00:00, 8936.62 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:02<00:00,  5.70it/s, loss=0.62] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.8866\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:02<00:00,  5.22it/s, loss=0.421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.5464\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:02<00:00,  5.57it/s, loss=2.69] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.3449\n",
      "Test Metrics: Precision=0.8654, Recall=0.8654, F1=0.8654\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 105, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 7126.82 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 4665.52 examples/s]\n",
      "Map: 100%|██████████| 873/873 [00:00<00:00, 9097.10 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:08<00:00,  1.69it/s, loss=0.671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.3807\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:08<00:00,  1.64it/s, loss=0.998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.1351\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:08<00:00,  1.68it/s, loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.2522\n",
      "Test Metrics: Precision=0.8736, Recall=0.8736, F1=0.8736\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 105, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 7520.27 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 4445.81 examples/s]\n",
      "Map: 100%|██████████| 873/873 [00:00<00:00, 9139.79 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:07<00:00,  1.90it/s, loss=0.422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.5994\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:07<00:00,  1.90it/s, loss=0.453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.7241\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:07<00:00,  1.84it/s, loss=0.134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9927\n",
      "Test Metrics: Precision=0.9056, Recall=0.9056, F1=0.9056\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 105, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 8237.35 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 4165.34 examples/s]\n",
      "Map: 100%|██████████| 873/873 [00:00<00:00, 9023.83 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:02<00:00,  6.24it/s, loss=0.702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.0352\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:02<00:00,  6.05it/s, loss=1.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.1119\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:02<00:00,  5.89it/s, loss=0.317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.7606\n",
      "Test Metrics: Precision=0.8584, Recall=0.8584, F1=0.8584\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 105, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 8164.36 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 4024.88 examples/s]\n",
      "Map: 100%|██████████| 873/873 [00:00<00:00, 8837.42 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:07<00:00,  1.91it/s, loss=0.0704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.0683\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:06<00:00,  2.09it/s, loss=0.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.1801\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:06<00:00,  2.03it/s, loss=0.508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.9442\n",
      "Test Metrics: Precision=0.8740, Recall=0.8740, F1=0.8740\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 105, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 7685.90 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 3992.58 examples/s]\n",
      "Map: 100%|██████████| 873/873 [00:00<00:00, 9051.11 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:06<00:00,  2.11it/s, loss=0.154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.2561\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:06<00:00,  2.10it/s, loss=0.479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.0118\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:06<00:00,  2.04it/s, loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.7756\n",
      "Test Metrics: Precision=0.8832, Recall=0.8832, F1=0.8832\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 105, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 7437.97 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 4712.70 examples/s]\n",
      "Map: 100%|██████████| 873/873 [00:00<00:00, 8559.30 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:02<00:00,  5.06it/s, loss=0.511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.3921\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:02<00:00,  5.00it/s, loss=0.75] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.8986\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:02<00:00,  5.24it/s, loss=0.756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.6502\n",
      "Test Metrics: Precision=0.8650, Recall=0.8650, F1=0.8650\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 105, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 7657.03 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 4835.07 examples/s]\n",
      "Map: 100%|██████████| 873/873 [00:00<00:00, 8858.53 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:08<00:00,  1.67it/s, loss=0.881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.3595\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:08<00:00,  1.61it/s, loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.4012\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:08<00:00,  1.63it/s, loss=0.902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.8604\n",
      "Test Metrics: Precision=0.8753, Recall=0.8753, F1=0.8753\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 105, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 7046.21 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 4341.72 examples/s]\n",
      "Map: 100%|██████████| 873/873 [00:00<00:00, 8812.45 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:07<00:00,  1.84it/s, loss=0.523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.2837\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:07<00:00,  1.97it/s, loss=0.606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.9055\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:07<00:00,  1.86it/s, loss=0.97] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.1457\n",
      "Test Metrics: Precision=0.9069, Recall=0.9069, F1=0.9069\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 110, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 110/110 [00:00<00:00, 7737.79 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 4568.51 examples/s]\n",
      "Map: 100%|██████████| 867/867 [00:00<00:00, 8782.07 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:02<00:00,  5.36it/s, loss=0.659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.6468\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:02<00:00,  5.51it/s, loss=0.35] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.1439\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:02<00:00,  5.26it/s, loss=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.6851\n",
      "Test Metrics: Precision=0.8686, Recall=0.8686, F1=0.8686\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 110, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 110/110 [00:00<00:00, 7875.15 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 5016.02 examples/s]\n",
      "Map: 100%|██████████| 867/867 [00:00<00:00, 8838.68 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:08<00:00,  1.74it/s, loss=0.408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.8261\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:07<00:00,  1.87it/s, loss=0.417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.7756\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:07<00:00,  1.82it/s, loss=0.287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4792\n",
      "Test Metrics: Precision=0.8804, Recall=0.8804, F1=0.8804\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 110, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 110/110 [00:00<00:00, 7839.29 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 4668.12 examples/s]\n",
      "Map: 100%|██████████| 867/867 [00:00<00:00, 9061.05 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:07<00:00,  1.87it/s, loss=0.523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.6393\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:07<00:00,  1.88it/s, loss=0.418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.6058\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:07<00:00,  1.93it/s, loss=0.353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4520\n",
      "Test Metrics: Precision=0.8996, Recall=0.8996, F1=0.8996\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 110, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 110/110 [00:00<00:00, 8342.04 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 4349.30 examples/s]\n",
      "Map: 100%|██████████| 867/867 [00:00<00:00, 8879.60 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:02<00:00,  6.01it/s, loss=0.454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.8014\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:02<00:00,  5.80it/s, loss=0.523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.5776\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:02<00:00,  5.95it/s, loss=0.392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.2219\n",
      "Test Metrics: Precision=0.8652, Recall=0.8652, F1=0.8652\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 110, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 110/110 [00:00<00:00, 8273.09 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 4018.76 examples/s]\n",
      "Map: 100%|██████████| 867/867 [00:00<00:00, 8778.34 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:07<00:00,  1.89it/s, loss=0.461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.5421\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:07<00:00,  1.92it/s, loss=0.521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.6255\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:07<00:00,  1.89it/s, loss=0.435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.5461\n",
      "Test Metrics: Precision=0.8790, Recall=0.8790, F1=0.8790\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 110, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 110/110 [00:00<00:00, 8286.46 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 4038.63 examples/s]\n",
      "Map: 100%|██████████| 867/867 [00:00<00:00, 9029.12 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:06<00:00,  2.01it/s, loss=0.922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.7628\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:06<00:00,  2.08it/s, loss=0.173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.4999\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:06<00:00,  2.02it/s, loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.2569\n",
      "Test Metrics: Precision=0.9126, Recall=0.9126, F1=0.9126\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 110, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 110/110 [00:00<00:00, 7344.72 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 4423.10 examples/s]\n",
      "Map: 100%|██████████| 867/867 [00:00<00:00, 3985.27 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:02<00:00,  5.09it/s, loss=0.639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.2716\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:02<00:00,  5.03it/s, loss=0.388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.8105\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:02<00:00,  5.21it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3237\n",
      "Test Metrics: Precision=0.8565, Recall=0.8565, F1=0.8565\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 110, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 110/110 [00:00<00:00, 7349.52 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 3983.02 examples/s]\n",
      "Map: 100%|██████████| 867/867 [00:00<00:00, 8597.28 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:08<00:00,  1.63it/s, loss=0.704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.7119\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:08<00:00,  1.71it/s, loss=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.3909\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:08<00:00,  1.73it/s, loss=0.422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.1927\n",
      "Test Metrics: Precision=0.8782, Recall=0.8782, F1=0.8782\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 110, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 110/110 [00:00<00:00, 7545.81 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 4293.84 examples/s]\n",
      "Map: 100%|██████████| 867/867 [00:00<00:00, 8968.51 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:07<00:00,  2.00it/s, loss=0.505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.2136\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:07<00:00,  1.81it/s, loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.5024\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:07<00:00,  1.79it/s, loss=0.348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4374\n",
      "Test Metrics: Precision=0.8898, Recall=0.8898, F1=0.8898\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 110, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 110/110 [00:00<00:00, 8099.67 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 4521.50 examples/s]\n",
      "Map: 100%|██████████| 867/867 [00:00<00:00, 8640.94 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:02<00:00,  6.01it/s, loss=0.698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.1117\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:02<00:00,  5.99it/s, loss=0.447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.8128\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:02<00:00,  6.39it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.3094\n",
      "Test Metrics: Precision=0.8702, Recall=0.8702, F1=0.8702\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 110, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 110/110 [00:00<00:00, 8015.10 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 4630.17 examples/s]\n",
      "Map: 100%|██████████| 867/867 [00:00<00:00, 8697.17 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:07<00:00,  1.85it/s, loss=0.402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.6652\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:06<00:00,  2.00it/s, loss=0.291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.5310\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:07<00:00,  1.94it/s, loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.2064\n",
      "Test Metrics: Precision=0.8857, Recall=0.8857, F1=0.8857\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 110, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 110/110 [00:00<00:00, 7524.64 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 4349.30 examples/s]\n",
      "Map: 100%|██████████| 867/867 [00:00<00:00, 8846.96 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:06<00:00,  2.04it/s, loss=0.42] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.0671\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:06<00:00,  2.24it/s, loss=0.308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.5050\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:06<00:00,  2.06it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.4251\n",
      "Test Metrics: Precision=0.9091, Recall=0.9091, F1=0.9091\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 110, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 110/110 [00:00<00:00, 7662.48 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 4481.09 examples/s]\n",
      "Map: 100%|██████████| 867/867 [00:00<00:00, 4036.23 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:02<00:00,  5.15it/s, loss=0.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.3984\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:02<00:00,  4.88it/s, loss=0.455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.9922\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:02<00:00,  5.00it/s, loss=0.442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.4338\n",
      "Test Metrics: Precision=0.8640, Recall=0.8640, F1=0.8640\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 110, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 110/110 [00:00<00:00, 7544.70 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 4424.16 examples/s]\n",
      "Map: 100%|██████████| 867/867 [00:00<00:00, 8749.07 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:08<00:00,  1.64it/s, loss=0.407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.2333\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:09<00:00,  1.55it/s, loss=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.3608\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:08<00:00,  1.63it/s, loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.3787\n",
      "Test Metrics: Precision=0.8802, Recall=0.8802, F1=0.8802\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 110, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 110/110 [00:00<00:00, 7349.64 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 4046.25 examples/s]\n",
      "Map: 100%|██████████| 867/867 [00:00<00:00, 8817.61 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 14/14 [00:07<00:00,  1.88it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.3391\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 14/14 [00:07<00:00,  1.75it/s, loss=0.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.5264\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 14/14 [00:07<00:00,  1.82it/s, loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.2502\n",
      "Test Metrics: Precision=0.9017, Recall=0.9017, F1=0.9017\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 115, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115/115 [00:00<00:00, 7966.32 examples/s]\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 5225.84 examples/s]\n",
      "Map: 100%|██████████| 861/861 [00:00<00:00, 8623.76 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:02<00:00,  5.61it/s, loss=0.712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.3077\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:02<00:00,  5.46it/s, loss=0.365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.4822\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:02<00:00,  5.77it/s, loss=0.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0426\n",
      "Test Metrics: Precision=0.8648, Recall=0.8648, F1=0.8648\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 115, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115/115 [00:00<00:00, 7608.56 examples/s]\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 5062.93 examples/s]\n",
      "Map: 100%|██████████| 861/861 [00:00<00:00, 8571.26 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:08<00:00,  1.79it/s, loss=0.703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.4372\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:07<00:00,  1.88it/s, loss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.5014\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:07<00:00,  1.94it/s, loss=0.8]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0922\n",
      "Test Metrics: Precision=0.8723, Recall=0.8723, F1=0.8723\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 115, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115/115 [00:00<00:00, 7336.83 examples/s]\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 5485.87 examples/s]\n",
      "Map: 100%|██████████| 861/861 [00:00<00:00, 8969.18 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:07<00:00,  1.98it/s, loss=0.407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.6986\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:07<00:00,  1.90it/s, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.9840\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:07<00:00,  1.96it/s, loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.6197\n",
      "Test Metrics: Precision=0.8995, Recall=0.8995, F1=0.8995\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 115, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115/115 [00:00<00:00, 8635.20 examples/s]\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 4788.49 examples/s]\n",
      "Map: 100%|██████████| 861/861 [00:00<00:00, 8800.70 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:02<00:00,  6.12it/s, loss=0.975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.9582\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:02<00:00,  6.22it/s, loss=0.683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.2528\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:02<00:00,  5.98it/s, loss=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4395\n",
      "Test Metrics: Precision=0.8691, Recall=0.8691, F1=0.8691\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 115, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115/115 [00:00<00:00, 8544.04 examples/s]\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 4733.75 examples/s]\n",
      "Map: 100%|██████████| 861/861 [00:00<00:00, 8671.56 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:07<00:00,  1.91it/s, loss=0.35] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.3907\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:07<00:00,  2.10it/s, loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.4852\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:07<00:00,  2.00it/s, loss=0.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.5221\n",
      "Test Metrics: Precision=0.8839, Recall=0.8839, F1=0.8839\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 115, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115/115 [00:00<00:00, 8251.42 examples/s]\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 4193.94 examples/s]\n",
      "Map: 100%|██████████| 861/861 [00:00<00:00, 8840.81 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:07<00:00,  2.10it/s, loss=0.42] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.9144\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:07<00:00,  2.00it/s, loss=0.281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.1614\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:07<00:00,  2.04it/s, loss=0.222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.7662\n",
      "Test Metrics: Precision=0.9048, Recall=0.9048, F1=0.9048\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 115, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115/115 [00:00<00:00, 7254.40 examples/s]\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 3733.61 examples/s]\n",
      "Map: 100%|██████████| 861/861 [00:00<00:00, 8703.53 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:02<00:00,  5.45it/s, loss=0.726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.0462\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:02<00:00,  5.24it/s, loss=0.647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.1427\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:02<00:00,  5.39it/s, loss=0.341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.2469\n",
      "Test Metrics: Precision=0.8658, Recall=0.8658, F1=0.8658\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 115, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115/115 [00:00<00:00, 7026.56 examples/s]\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 4451.94 examples/s]\n",
      "Map: 100%|██████████| 861/861 [00:00<00:00, 8611.38 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:08<00:00,  1.73it/s, loss=0.638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.2777\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:08<00:00,  1.77it/s, loss=0.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.2371\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:08<00:00,  1.68it/s, loss=0.617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.9751\n",
      "Test Metrics: Precision=0.8749, Recall=0.8749, F1=0.8749\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 115, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115/115 [00:00<00:00, 7326.57 examples/s]\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 4691.16 examples/s]\n",
      "Map: 100%|██████████| 861/861 [00:00<00:00, 9084.27 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:08<00:00,  1.86it/s, loss=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.7882\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:07<00:00,  1.91it/s, loss=0.273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.5556\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:08<00:00,  1.85it/s, loss=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.1561\n",
      "Test Metrics: Precision=0.9054, Recall=0.9054, F1=0.9054\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 115, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115/115 [00:00<00:00, 8439.39 examples/s]\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 5438.55 examples/s]\n",
      "Map: 100%|██████████| 861/861 [00:00<00:00, 8859.66 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:02<00:00,  6.25it/s, loss=0.712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.5119\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:02<00:00,  6.29it/s, loss=0.676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.6023\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:02<00:00,  6.38it/s, loss=0.356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0566\n",
      "Test Metrics: Precision=0.8658, Recall=0.8658, F1=0.8658\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 115, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115/115 [00:00<00:00, 8118.10 examples/s]\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 4329.85 examples/s]\n",
      "Map: 100%|██████████| 861/861 [00:00<00:00, 8569.86 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:07<00:00,  2.06it/s, loss=0.814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.2612\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:07<00:00,  1.96it/s, loss=0.395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.1508\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:07<00:00,  1.93it/s, loss=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.8455\n",
      "Test Metrics: Precision=0.8759, Recall=0.8759, F1=0.8759\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 115, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115/115 [00:00<00:00, 8294.27 examples/s]\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 4821.76 examples/s]\n",
      "Map: 100%|██████████| 861/861 [00:00<00:00, 8886.76 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:07<00:00,  2.13it/s, loss=0.53] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.6138\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:07<00:00,  2.07it/s, loss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.8491\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:07<00:00,  2.14it/s, loss=0.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.1987\n",
      "Test Metrics: Precision=0.9042, Recall=0.9042, F1=0.9042\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 115, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115/115 [00:00<00:00, 7590.84 examples/s]\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 4214.09 examples/s]\n",
      "Map: 100%|██████████| 861/861 [00:00<00:00, 8816.49 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:02<00:00,  5.43it/s, loss=0.784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.4380\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:02<00:00,  5.03it/s, loss=1.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.0277\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:02<00:00,  5.09it/s, loss=0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.6759\n",
      "Test Metrics: Precision=0.8629, Recall=0.8629, F1=0.8629\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 115, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115/115 [00:00<00:00, 7449.00 examples/s]\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 5048.88 examples/s]\n",
      "Map: 100%|██████████| 861/861 [00:00<00:00, 8727.55 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:08<00:00,  1.70it/s, loss=0.459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.8310\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:09<00:00,  1.58it/s, loss=0.561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.8978\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:08<00:00,  1.67it/s, loss=0.481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.6147\n",
      "Test Metrics: Precision=0.8792, Recall=0.8792, F1=0.8792\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 115, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 115/115 [00:00<00:00, 7894.74 examples/s]\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 4437.80 examples/s]\n",
      "Map: 100%|██████████| 861/861 [00:00<00:00, 8956.19 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:07<00:00,  1.92it/s, loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.7084\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:07<00:00,  1.91it/s, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.9510\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:08<00:00,  1.84it/s, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.2386\n",
      "Test Metrics: Precision=0.9091, Recall=0.9091, F1=0.9091\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 120, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 7843.97 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 4996.44 examples/s]\n",
      "Map: 100%|██████████| 855/855 [00:00<00:00, 8855.78 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:02<00:00,  5.44it/s, loss=0.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.5748\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:02<00:00,  5.22it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.6570\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:02<00:00,  5.44it/s, loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.2887\n",
      "Test Metrics: Precision=0.8730, Recall=0.8730, F1=0.8730\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 120, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 7973.84 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 5220.31 examples/s]\n",
      "Map: 100%|██████████| 855/855 [00:00<00:00, 8894.44 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:08<00:00,  1.77it/s, loss=0.637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.6631\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:08<00:00,  1.68it/s, loss=0.431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.9957\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:08<00:00,  1.70it/s, loss=0.434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.7224\n",
      "Test Metrics: Precision=0.8750, Recall=0.8750, F1=0.8750\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 120, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 7719.46 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 4338.37 examples/s]\n",
      "Map: 100%|██████████| 855/855 [00:00<00:00, 8844.51 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:08<00:00,  1.83it/s, loss=0.53] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.0228\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:08<00:00,  1.87it/s, loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.2671\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:08<00:00,  1.87it/s, loss=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.6093\n",
      "Test Metrics: Precision=0.9065, Recall=0.9065, F1=0.9065\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 120, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 8548.75 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 5632.77 examples/s]\n",
      "Map: 100%|██████████| 855/855 [00:00<00:00, 8698.14 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:02<00:00,  5.85it/s, loss=0.676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.1817\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:02<00:00,  5.74it/s, loss=0.404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.9751\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:02<00:00,  5.74it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.8924\n",
      "Test Metrics: Precision=0.8671, Recall=0.8671, F1=0.8671\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 120, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 8307.47 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 5446.26 examples/s]\n",
      "Map: 100%|██████████| 855/855 [00:00<00:00, 8810.91 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:07<00:00,  1.92it/s, loss=0.39] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.9407\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:07<00:00,  2.01it/s, loss=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.3972\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:08<00:00,  1.84it/s, loss=0.276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.5786\n",
      "Test Metrics: Precision=0.8857, Recall=0.8857, F1=0.8857\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 120, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 7743.57 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 4851.94 examples/s]\n",
      "Map: 100%|██████████| 855/855 [00:00<00:00, 8658.39 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:07<00:00,  2.02it/s, loss=0.407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.8588\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:07<00:00,  2.09it/s, loss=0.392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.8385\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:07<00:00,  2.02it/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.6838\n",
      "Test Metrics: Precision=0.9065, Recall=0.9065, F1=0.9065\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 120, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 7362.52 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 5717.23 examples/s]\n",
      "Map: 100%|██████████| 855/855 [00:00<00:00, 8508.36 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:03<00:00,  4.96it/s, loss=0.522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.3838\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:02<00:00,  5.21it/s, loss=0.402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.7397\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:02<00:00,  5.18it/s, loss=0.384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0577\n",
      "Test Metrics: Precision=0.8679, Recall=0.8679, F1=0.8679\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 120, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 7762.68 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 6677.06 examples/s]\n",
      "Map: 100%|██████████| 855/855 [00:00<00:00, 8840.39 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:09<00:00,  1.56it/s, loss=0.416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.1062\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:08<00:00,  1.68it/s, loss=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.3867\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:08<00:00,  1.77it/s, loss=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.8180\n",
      "Test Metrics: Precision=0.8816, Recall=0.8816, F1=0.8816\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 120, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 7286.21 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 5993.29 examples/s]\n",
      "Map: 100%|██████████| 855/855 [00:00<00:00, 9161.21 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:08<00:00,  1.73it/s, loss=0.522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.1848\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:08<00:00,  1.79it/s, loss=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.7458\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:08<00:00,  1.76it/s, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4048\n",
      "Test Metrics: Precision=0.9042, Recall=0.9042, F1=0.9042\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 120, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 8139.27 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 4447.24 examples/s]\n",
      "Map: 100%|██████████| 855/855 [00:00<00:00, 8470.72 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:02<00:00,  6.10it/s, loss=0.706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.6269\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:02<00:00,  5.74it/s, loss=0.43] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.1965\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:02<00:00,  5.79it/s, loss=0.319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.8181\n",
      "Test Metrics: Precision=0.8673, Recall=0.8673, F1=0.8673\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 120, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 8165.15 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 4708.51 examples/s]\n",
      "Map: 100%|██████████| 855/855 [00:00<00:00, 8702.30 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:07<00:00,  1.97it/s, loss=0.559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.6935\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:08<00:00,  1.87it/s, loss=0.34] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.0724\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:08<00:00,  1.84it/s, loss=0.268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.9027\n",
      "Test Metrics: Precision=0.8748, Recall=0.8748, F1=0.8748\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 120, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 8129.68 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 4227.24 examples/s]\n",
      "Map: 100%|██████████| 855/855 [00:00<00:00, 8694.93 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:07<00:00,  2.02it/s, loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 11.2292\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:07<00:00,  2.05it/s, loss=0.179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.9508\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:07<00:00,  1.91it/s, loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.7227\n",
      "Test Metrics: Precision=0.9030, Recall=0.9030, F1=0.9030\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 120, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 7958.08 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 4751.63 examples/s]\n",
      "Map: 100%|██████████| 855/855 [00:00<00:00, 8922.14 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:03<00:00,  4.88it/s, loss=0.696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.3683\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:02<00:00,  5.29it/s, loss=0.642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.4492\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:03<00:00,  4.90it/s, loss=0.47] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8362\n",
      "Test Metrics: Precision=0.8628, Recall=0.8628, F1=0.8628\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 120, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 7532.65 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 4762.87 examples/s]\n",
      "Map: 100%|██████████| 855/855 [00:00<00:00, 8795.16 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:09<00:00,  1.54it/s, loss=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.7061\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:08<00:00,  1.79it/s, loss=0.356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.7074\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:09<00:00,  1.59it/s, loss=0.385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.9423\n",
      "Test Metrics: Precision=0.8800, Recall=0.8800, F1=0.8800\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 120, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 7916.27 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 4185.24 examples/s]\n",
      "Map: 100%|██████████| 855/855 [00:00<00:00, 8770.27 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 15/15 [00:08<00:00,  1.85it/s, loss=0.563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.2168\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 15/15 [00:08<00:00,  1.80it/s, loss=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.7386\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 15/15 [00:08<00:00,  1.85it/s, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.1770\n",
      "Test Metrics: Precision=0.9062, Recall=0.9062, F1=0.9062\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 125, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 125/125 [00:00<00:00, 7796.57 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 5280.10 examples/s]\n",
      "Map: 100%|██████████| 849/849 [00:00<00:00, 8905.15 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16/16 [00:03<00:00,  5.27it/s, loss=0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.4370\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16/16 [00:03<00:00,  5.19it/s, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.9432\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16/16 [00:03<00:00,  5.33it/s, loss=0.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.9382\n",
      "Test Metrics: Precision=0.8713, Recall=0.8713, F1=0.8713\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 125, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 125/125 [00:00<00:00, 7631.22 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 5150.18 examples/s]\n",
      "Map: 100%|██████████| 849/849 [00:00<00:00, 8583.76 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16/16 [00:08<00:00,  1.82it/s, loss=0.63] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.6024\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16/16 [00:09<00:00,  1.73it/s, loss=0.525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.1612\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16/16 [00:09<00:00,  1.71it/s, loss=0.403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.0175\n",
      "Test Metrics: Precision=0.8685, Recall=0.8685, F1=0.8685\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 125, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 125/125 [00:00<00:00, 7436.71 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4930.30 examples/s]\n",
      "Map: 100%|██████████| 849/849 [00:00<00:00, 8913.78 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16/16 [00:08<00:00,  1.79it/s, loss=0.463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.0315\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16/16 [00:08<00:00,  1.95it/s, loss=0.413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.6046\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16/16 [00:08<00:00,  1.79it/s, loss=0.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.1088\n",
      "Test Metrics: Precision=0.9034, Recall=0.9034, F1=0.9034\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 125, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 125/125 [00:00<00:00, 8171.19 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4852.50 examples/s]\n",
      "Map: 100%|██████████| 849/849 [00:00<00:00, 3890.99 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16/16 [00:02<00:00,  7.15it/s, loss=0.657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.4782\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16/16 [00:02<00:00,  7.09it/s, loss=0.578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.8876\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16/16 [00:02<00:00,  6.91it/s, loss=0.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.2677\n",
      "Test Metrics: Precision=0.8631, Recall=0.8631, F1=0.8631\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 125, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 125/125 [00:00<00:00, 8632.67 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4825.03 examples/s]\n",
      "Map: 100%|██████████| 849/849 [00:00<00:00, 8539.34 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16/16 [00:07<00:00,  2.28it/s, loss=0.429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.4390\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16/16 [00:06<00:00,  2.39it/s, loss=0.379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.8633\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16/16 [00:06<00:00,  2.32it/s, loss=0.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4093\n",
      "Test Metrics: Precision=0.8854, Recall=0.8854, F1=0.8854\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 125, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 125/125 [00:00<00:00, 8095.24 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4118.52 examples/s]\n",
      "Map: 100%|██████████| 849/849 [00:00<00:00, 8689.64 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16/16 [00:06<00:00,  2.34it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.8313\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16/16 [00:06<00:00,  2.53it/s, loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.3450\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16/16 [00:06<00:00,  2.43it/s, loss=0.223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.8095\n",
      "Test Metrics: Precision=0.9109, Recall=0.9109, F1=0.9109\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 125, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 125/125 [00:00<00:00, 7341.02 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4992.98 examples/s]\n",
      "Map: 100%|██████████| 849/849 [00:00<00:00, 8858.69 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16/16 [00:02<00:00,  6.12it/s, loss=0.709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.2042\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16/16 [00:02<00:00,  6.15it/s, loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.0792\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16/16 [00:02<00:00,  5.99it/s, loss=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5557\n",
      "Test Metrics: Precision=0.8693, Recall=0.8693, F1=0.8693\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 125, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 125/125 [00:00<00:00, 7288.97 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4952.65 examples/s]\n",
      "Map: 100%|██████████| 849/849 [00:00<00:00, 8680.81 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16/16 [00:07<00:00,  2.18it/s, loss=0.634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.5754\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16/16 [00:08<00:00,  1.99it/s, loss=0.426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.5219\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16/16 [00:07<00:00,  2.05it/s, loss=0.451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.8176\n",
      "Test Metrics: Precision=0.8759, Recall=0.8759, F1=0.8759\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 125, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 125/125 [00:00<00:00, 7279.86 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4780.60 examples/s]\n",
      "Map: 100%|██████████| 849/849 [00:00<00:00, 8188.87 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16/16 [00:08<00:00,  1.95it/s, loss=0.633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.7232\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16/16 [00:07<00:00,  2.23it/s, loss=0.279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.8954\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16/16 [00:07<00:00,  2.17it/s, loss=0.151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.2910\n",
      "Test Metrics: Precision=0.9084, Recall=0.9084, F1=0.9084\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 125, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 125/125 [00:00<00:00, 8074.17 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 3458.25 examples/s]\n",
      "Map: 100%|██████████| 849/849 [00:00<00:00, 3908.87 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16/16 [00:02<00:00,  6.76it/s, loss=0.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.0018\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16/16 [00:02<00:00,  7.11it/s, loss=0.368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.7769\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16/16 [00:02<00:00,  6.94it/s, loss=0.511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0056\n",
      "Test Metrics: Precision=0.8710, Recall=0.8710, F1=0.8710\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 125, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 125/125 [00:00<00:00, 7813.18 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4019.84 examples/s]\n",
      "Map: 100%|██████████| 849/849 [00:00<00:00, 8654.92 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16/16 [00:06<00:00,  2.35it/s, loss=0.591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.9034\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16/16 [00:07<00:00,  2.25it/s, loss=0.608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.5674\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16/16 [00:07<00:00,  2.27it/s, loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.3262\n",
      "Test Metrics: Precision=0.8763, Recall=0.8763, F1=0.8763\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 125, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 125/125 [00:00<00:00, 7898.04 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4013.84 examples/s]\n",
      "Map: 100%|██████████| 849/849 [00:00<00:00, 9050.57 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16/16 [00:06<00:00,  2.36it/s, loss=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.8027\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16/16 [00:06<00:00,  2.52it/s, loss=0.269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.4936\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16/16 [00:06<00:00,  2.39it/s, loss=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.2379\n",
      "Test Metrics: Precision=0.9056, Recall=0.9056, F1=0.9056\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 125, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 125/125 [00:00<00:00, 7866.64 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 5650.57 examples/s]\n",
      "Map: 100%|██████████| 849/849 [00:00<00:00, 8707.07 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16/16 [00:02<00:00,  5.34it/s, loss=0.743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.0487\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16/16 [00:02<00:00,  5.50it/s, loss=0.846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.1968\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16/16 [00:03<00:00,  5.17it/s, loss=0.476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3237\n",
      "Test Metrics: Precision=0.8635, Recall=0.8635, F1=0.8635\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 125, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 125/125 [00:00<00:00, 8169.28 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 5401.69 examples/s]\n",
      "Map: 100%|██████████| 849/849 [00:00<00:00, 8199.24 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16/16 [00:07<00:00,  2.00it/s, loss=0.867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.1459\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16/16 [00:08<00:00,  1.94it/s, loss=0.387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.7888\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16/16 [00:08<00:00,  1.97it/s, loss=0.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.2947\n",
      "Test Metrics: Precision=0.8820, Recall=0.8820, F1=0.8820\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 125, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 125/125 [00:00<00:00, 7868.41 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 4843.53 examples/s]\n",
      "Map: 100%|██████████| 849/849 [00:00<00:00, 8840.61 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16/16 [00:07<00:00,  2.27it/s, loss=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.7960\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16/16 [00:07<00:00,  2.10it/s, loss=0.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.6465\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16/16 [00:07<00:00,  2.14it/s, loss=0.48] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.5618\n",
      "Test Metrics: Precision=0.9098, Recall=0.9098, F1=0.9098\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 130, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 7551.97 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 6047.02 examples/s]\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 3634.97 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:02<00:00,  6.83it/s, loss=0.795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.0553\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:02<00:00,  6.45it/s, loss=0.448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.9793\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:02<00:00,  6.43it/s, loss=0.222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.0272\n",
      "Test Metrics: Precision=0.8719, Recall=0.8719, F1=0.8719\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 130, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 7905.75 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 5969.89 examples/s]\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 8534.08 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:08<00:00,  2.02it/s, loss=0.469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.9254\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:07<00:00,  2.24it/s, loss=0.404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.3276\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:07<00:00,  2.20it/s, loss=0.0821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.5387\n",
      "Test Metrics: Precision=0.8902, Recall=0.8902, F1=0.8902\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 130, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 7680.47 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 6167.75 examples/s]\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 8829.67 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:07<00:00,  2.42it/s, loss=0.468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.3632\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:07<00:00,  2.35it/s, loss=0.728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.1219\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:06<00:00,  2.45it/s, loss=0.458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4149\n",
      "Test Metrics: Precision=0.9082, Recall=0.9082, F1=0.9082\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 130, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 8001.58 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 4574.71 examples/s]\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 8262.99 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:02<00:00,  7.65it/s, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.6275\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:02<00:00,  7.67it/s, loss=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.4724\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:02<00:00,  7.53it/s, loss=0.252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.9312\n",
      "Test Metrics: Precision=0.8708, Recall=0.8708, F1=0.8708\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 130, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 8390.16 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 4985.23 examples/s]\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 8606.94 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:06<00:00,  2.47it/s, loss=0.556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.1860\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:06<00:00,  2.47it/s, loss=0.439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.0012\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:06<00:00,  2.48it/s, loss=0.982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8124\n",
      "Test Metrics: Precision=0.8652, Recall=0.8652, F1=0.8652\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 130, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 8256.00 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 4511.50 examples/s]\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 8871.25 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:06<00:00,  2.58it/s, loss=0.437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.8647\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:06<00:00,  2.62it/s, loss=0.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.8112\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:06<00:00,  2.50it/s, loss=0.193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.8291\n",
      "Test Metrics: Precision=0.9062, Recall=0.9062, F1=0.9062\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 130, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 6983.79 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 3795.88 examples/s]\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 8964.98 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:02<00:00,  6.48it/s, loss=0.613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.4107\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:02<00:00,  6.20it/s, loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.4643\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:02<00:00,  6.15it/s, loss=0.301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.7475\n",
      "Test Metrics: Precision=0.8652, Recall=0.8652, F1=0.8652\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 130, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 7462.26 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 4178.87 examples/s]\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 8800.49 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:08<00:00,  1.98it/s, loss=0.678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.6517\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:08<00:00,  2.10it/s, loss=0.513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.8073\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:08<00:00,  2.12it/s, loss=0.534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2125\n",
      "Test Metrics: Precision=0.8611, Recall=0.8611, F1=0.8611\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 130, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 7447.78 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 3769.90 examples/s]\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 9124.33 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:07<00:00,  2.31it/s, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.8912\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:07<00:00,  2.13it/s, loss=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.7685\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:07<00:00,  2.22it/s, loss=0.167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4257\n",
      "Test Metrics: Precision=0.8927, Recall=0.8927, F1=0.8927\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 130, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 7991.61 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 4290.17 examples/s]\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 8610.63 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:02<00:00,  7.68it/s, loss=0.609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.6597\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:02<00:00,  7.16it/s, loss=0.287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.1362\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:02<00:00,  7.04it/s, loss=0.166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.1625\n",
      "Test Metrics: Precision=0.8735, Recall=0.8735, F1=0.8735\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 130, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 7924.94 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 4510.56 examples/s]\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 8555.04 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:07<00:00,  2.40it/s, loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.3028\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:07<00:00,  2.38it/s, loss=0.262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.9250\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:07<00:00,  2.40it/s, loss=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.7288\n",
      "Test Metrics: Precision=0.8874, Recall=0.8874, F1=0.8874\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 130, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 7489.93 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 4498.10 examples/s]\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 8818.57 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:06<00:00,  2.66it/s, loss=0.155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.0532\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:06<00:00,  2.52it/s, loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.5873\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:06<00:00,  2.50it/s, loss=0.0747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.1149\n",
      "Test Metrics: Precision=0.9107, Recall=0.9107, F1=0.9107\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 130, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 7498.58 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 5305.37 examples/s]\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 8675.63 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:02<00:00,  6.18it/s, loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.4600\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:02<00:00,  5.94it/s, loss=0.488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.9895\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:02<00:00,  6.17it/s, loss=1.08] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.0494\n",
      "Test Metrics: Precision=0.8705, Recall=0.8705, F1=0.8705\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 130, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 7700.75 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 5088.27 examples/s]\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 8402.44 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:09<00:00,  1.86it/s, loss=0.433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.8014\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:08<00:00,  2.05it/s, loss=0.399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.4481\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:08<00:00,  1.91it/s, loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.6431\n",
      "Test Metrics: Precision=0.8815, Recall=0.8815, F1=0.8815\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 130, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 7879.36 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 5000.55 examples/s]\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 8897.91 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:07<00:00,  2.18it/s, loss=0.468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.9702\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:07<00:00,  2.21it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.8637\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:07<00:00,  2.42it/s, loss=0.214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.3130\n",
      "Test Metrics: Precision=0.9062, Recall=0.9062, F1=0.9062\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 135, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 135/135 [00:00<00:00, 7368.67 examples/s]\n",
      "Map: 100%|██████████| 27/27 [00:00<00:00, 4374.13 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:00<00:00, 8789.59 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:02<00:00,  6.37it/s, loss=0.52] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.5454\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:02<00:00,  6.27it/s, loss=0.579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.2373\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:02<00:00,  6.47it/s, loss=0.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5708\n",
      "Test Metrics: Precision=0.8741, Recall=0.8741, F1=0.8741\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 135, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 135/135 [00:00<00:00, 7451.00 examples/s]\n",
      "Map: 100%|██████████| 27/27 [00:00<00:00, 4304.96 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:00<00:00, 3930.06 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:08<00:00,  2.04it/s, loss=0.473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.2546\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:08<00:00,  1.97it/s, loss=0.521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.2767\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:08<00:00,  2.01it/s, loss=0.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.4295\n",
      "Test Metrics: Precision=0.8905, Recall=0.8905, F1=0.8905\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 135, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 135/135 [00:00<00:00, 7260.40 examples/s]\n",
      "Map: 100%|██████████| 27/27 [00:00<00:00, 3921.27 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:00<00:00, 8892.52 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:07<00:00,  2.13it/s, loss=0.489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.3008\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:08<00:00,  2.09it/s, loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.5206\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:07<00:00,  2.15it/s, loss=0.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.8445\n",
      "Test Metrics: Precision=0.9107, Recall=0.9107, F1=0.9107\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 135, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 135/135 [00:00<00:00, 8175.20 examples/s]\n",
      "Map: 100%|██████████| 27/27 [00:00<00:00, 5374.76 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:00<00:00, 8665.94 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:02<00:00,  7.04it/s, loss=0.475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.9923\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:02<00:00,  6.39it/s, loss=0.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.0143\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:02<00:00,  6.75it/s, loss=0.391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.9299\n",
      "Test Metrics: Precision=0.8783, Recall=0.8783, F1=0.8783\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 135, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 135/135 [00:00<00:00, 7992.42 examples/s]\n",
      "Map: 100%|██████████| 27/27 [00:00<00:00, 4576.71 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:00<00:00, 8479.22 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:07<00:00,  2.39it/s, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.0136\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:07<00:00,  2.35it/s, loss=0.432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.6265\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:07<00:00,  2.30it/s, loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.2549\n",
      "Test Metrics: Precision=0.8912, Recall=0.8912, F1=0.8912\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 135, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 135/135 [00:00<00:00, 8270.49 examples/s]\n",
      "Map: 100%|██████████| 27/27 [00:00<00:00, 5722.97 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:00<00:00, 8717.50 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:07<00:00,  2.41it/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.1022\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:07<00:00,  2.38it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.3924\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:06<00:00,  2.44it/s, loss=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.0430\n",
      "Test Metrics: Precision=0.9067, Recall=0.9067, F1=0.9067\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 135, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 135/135 [00:00<00:00, 7378.66 examples/s]\n",
      "Map: 100%|██████████| 27/27 [00:00<00:00, 6092.76 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:00<00:00, 8703.39 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:03<00:00,  5.39it/s, loss=0.522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.4987\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:02<00:00,  5.75it/s, loss=0.471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.7455\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:03<00:00,  5.60it/s, loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8522\n",
      "Test Metrics: Precision=0.8697, Recall=0.8697, F1=0.8697\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 135, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 135/135 [00:00<00:00, 7380.30 examples/s]\n",
      "Map: 100%|██████████| 27/27 [00:00<00:00, 5218.96 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:00<00:00, 8452.27 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:08<00:00,  1.91it/s, loss=0.415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.3898\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:08<00:00,  2.07it/s, loss=0.269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.5596\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:08<00:00,  1.93it/s, loss=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.2275\n",
      "Test Metrics: Precision=0.8907, Recall=0.8907, F1=0.8907\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 135, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 135/135 [00:00<00:00, 7233.50 examples/s]\n",
      "Map: 100%|██████████| 27/27 [00:00<00:00, 5336.01 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:00<00:00, 8565.74 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:07<00:00,  2.21it/s, loss=0.725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.5508\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:07<00:00,  2.14it/s, loss=0.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.6545\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:08<00:00,  2.07it/s, loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.5645\n",
      "Test Metrics: Precision=0.9152, Recall=0.9152, F1=0.9152\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 135, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 135/135 [00:00<00:00, 7810.51 examples/s]\n",
      "Map: 100%|██████████| 27/27 [00:00<00:00, 5622.95 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:00<00:00, 8488.26 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:02<00:00,  7.22it/s, loss=0.477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.1017\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:02<00:00,  6.82it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.5622\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:02<00:00,  6.23it/s, loss=0.291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0138\n",
      "Test Metrics: Precision=0.8725, Recall=0.8725, F1=0.8725\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 135, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 135/135 [00:00<00:00, 8141.94 examples/s]\n",
      "Map: 100%|██████████| 27/27 [00:00<00:00, 5949.68 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:00<00:00, 8713.47 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:07<00:00,  2.24it/s, loss=0.512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.1069\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:07<00:00,  2.35it/s, loss=0.392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.6690\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:07<00:00,  2.28it/s, loss=0.216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.3961\n",
      "Test Metrics: Precision=0.8810, Recall=0.8810, F1=0.8810\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 135, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 135/135 [00:00<00:00, 8015.50 examples/s]\n",
      "Map: 100%|██████████| 27/27 [00:00<00:00, 5198.60 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:00<00:00, 8498.57 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:08<00:00,  1.96it/s, loss=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.8476\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:08<00:00,  2.05it/s, loss=0.279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.2297\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:09<00:00,  1.84it/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.6788\n",
      "Test Metrics: Precision=0.9025, Recall=0.9025, F1=0.9025\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 135, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 135/135 [00:00<00:00, 7925.63 examples/s]\n",
      "Map: 100%|██████████| 27/27 [00:00<00:00, 4882.35 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:00<00:00, 7972.73 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:03<00:00,  4.37it/s, loss=0.865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.8645\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:03<00:00,  4.69it/s, loss=0.287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.6999\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:03<00:00,  4.69it/s, loss=0.551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.7769\n",
      "Test Metrics: Precision=0.8689, Recall=0.8689, F1=0.8689\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 135, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 135/135 [00:00<00:00, 7747.04 examples/s]\n",
      "Map: 100%|██████████| 27/27 [00:00<00:00, 5130.76 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:00<00:00, 8844.89 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:10<00:00,  1.62it/s, loss=0.924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.7497\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:11<00:00,  1.52it/s, loss=0.414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.1104\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:10<00:00,  1.55it/s, loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.0174\n",
      "Test Metrics: Precision=0.8701, Recall=0.8701, F1=0.8701\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 135, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 135/135 [00:00<00:00, 7336.21 examples/s]\n",
      "Map: 100%|██████████| 27/27 [00:00<00:00, 3894.57 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:00<00:00, 8555.26 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 17/17 [00:09<00:00,  1.85it/s, loss=0.417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.0629\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 17/17 [00:10<00:00,  1.67it/s, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.2722\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 17/17 [00:09<00:00,  1.72it/s, loss=0.346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.2013\n",
      "Test Metrics: Precision=0.8962, Recall=0.8962, F1=0.8962\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 140, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 140/140 [00:00<00:00, 7428.15 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 5307.09 examples/s]\n",
      "Map: 100%|██████████| 831/831 [00:00<00:00, 8499.60 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 18/18 [00:03<00:00,  5.55it/s, loss=0.653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.7780\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 18/18 [00:03<00:00,  5.41it/s, loss=0.38] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.4999\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 18/18 [00:03<00:00,  5.39it/s, loss=0.401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.9942\n",
      "Test Metrics: Precision=0.8765, Recall=0.8765, F1=0.8765\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 140, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 140/140 [00:00<00:00, 7949.78 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 5011.12 examples/s]\n",
      "Map: 100%|██████████| 831/831 [00:00<00:00, 8852.41 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 18/18 [00:09<00:00,  1.88it/s, loss=0.634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.8638\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 18/18 [00:09<00:00,  1.82it/s, loss=0.603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.2214\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 18/18 [00:09<00:00,  1.82it/s, loss=0.358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.7558\n",
      "Test Metrics: Precision=0.8801, Recall=0.8801, F1=0.8801\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 140, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 140/140 [00:00<00:00, 7773.50 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 4910.54 examples/s]\n",
      "Map: 100%|██████████| 831/831 [00:00<00:00, 8951.51 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 18/18 [00:09<00:00,  1.95it/s, loss=0.449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.7626\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 18/18 [00:09<00:00,  1.93it/s, loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.7369\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 18/18 [00:09<00:00,  1.88it/s, loss=0.229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.2793\n",
      "Test Metrics: Precision=0.9080, Recall=0.9080, F1=0.9080\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 140, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 140/140 [00:00<00:00, 7974.72 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 4854.92 examples/s]\n",
      "Map: 100%|██████████| 831/831 [00:00<00:00, 8523.42 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 18/18 [00:02<00:00,  6.21it/s, loss=0.619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.2520\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 18/18 [00:02<00:00,  6.19it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.2878\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 18/18 [00:02<00:00,  6.02it/s, loss=0.47] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5160\n",
      "Test Metrics: Precision=0.8717, Recall=0.8717, F1=0.8717\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 140, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 140/140 [00:00<00:00, 7779.48 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 4582.15 examples/s]\n",
      "Map: 100%|██████████| 831/831 [00:00<00:00, 8537.43 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 18/18 [00:09<00:00,  1.95it/s, loss=0.478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.5497\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 18/18 [00:09<00:00,  1.97it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.9502\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 18/18 [00:10<00:00,  1.73it/s, loss=0.175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.2661\n",
      "Test Metrics: Precision=0.8859, Recall=0.8859, F1=0.8859\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 140, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 140/140 [00:00<00:00, 7897.18 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 4643.75 examples/s]\n",
      "Map: 100%|██████████| 831/831 [00:00<00:00, 8580.90 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 18/18 [00:07<00:00,  2.36it/s, loss=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.9776\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 18/18 [00:08<00:00,  2.18it/s, loss=0.282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.0184\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 18/18 [00:07<00:00,  2.42it/s, loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.3936\n",
      "Test Metrics: Precision=0.9111, Recall=0.9111, F1=0.9111\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 140, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 140/140 [00:00<00:00, 5320.60 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 3835.92 examples/s]\n",
      "Map: 100%|██████████| 831/831 [00:00<00:00, 6566.87 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 18/18 [00:03<00:00,  5.98it/s, loss=0.805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.7172\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 18/18 [00:03<00:00,  5.88it/s, loss=0.644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.2458\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 18/18 [00:03<00:00,  5.76it/s, loss=0.287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2719\n",
      "Test Metrics: Precision=0.8700, Recall=0.8700, F1=0.8700\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 140, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 140/140 [00:00<00:00, 7294.99 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 5914.91 examples/s]\n",
      "Map: 100%|██████████| 831/831 [00:00<00:00, 8513.78 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 18/18 [00:11<00:00,  1.62it/s, loss=0.232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.0796\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 18/18 [00:10<00:00,  1.68it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.6938\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 18/18 [00:11<00:00,  1.53it/s, loss=0.189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.0429\n",
      "Test Metrics: Precision=0.8959, Recall=0.8959, F1=0.8959\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 140, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 140/140 [00:00<00:00, 7158.48 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 5198.55 examples/s]\n",
      "Map: 100%|██████████| 831/831 [00:00<00:00, 8908.15 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 18/18 [00:10<00:00,  1.73it/s, loss=0.358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.9077\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 18/18 [00:10<00:00,  1.75it/s, loss=0.303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.5571\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 18/18 [00:10<00:00,  1.76it/s, loss=0.159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.7922\n",
      "Test Metrics: Precision=0.9143, Recall=0.9143, F1=0.9143\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 140, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 140/140 [00:00<00:00, 8281.31 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 3772.83 examples/s]\n",
      "Map: 100%|██████████| 831/831 [00:00<00:00, 8684.77 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 18/18 [00:02<00:00,  6.24it/s, loss=0.704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.7201\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 18/18 [00:02<00:00,  6.15it/s, loss=0.256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.4236\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 18/18 [00:02<00:00,  6.04it/s, loss=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5316\n",
      "Test Metrics: Precision=0.8712, Recall=0.8712, F1=0.8712\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 140, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 140/140 [00:00<00:00, 7783.29 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 3510.82 examples/s]\n",
      "Map: 100%|██████████| 831/831 [00:00<00:00, 8443.50 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 18/18 [00:09<00:00,  1.88it/s, loss=0.613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.9959\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 18/18 [00:09<00:00,  1.96it/s, loss=0.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.2831\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 18/18 [00:09<00:00,  1.92it/s, loss=0.255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.7760\n",
      "Test Metrics: Precision=0.8879, Recall=0.8879, F1=0.8879\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 140, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 140/140 [00:00<00:00, 7963.80 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 3576.47 examples/s]\n",
      "Map: 100%|██████████| 831/831 [00:00<00:00, 8958.57 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s, loss=0.34] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.9597\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 18/18 [00:09<00:00,  1.95it/s, loss=0.33] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.9360\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 18/18 [00:08<00:00,  2.04it/s, loss=0.341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4174\n",
      "Test Metrics: Precision=0.9128, Recall=0.9128, F1=0.9128\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 140, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 140/140 [00:00<00:00, 6792.32 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 3910.12 examples/s]\n",
      "Map: 100%|██████████| 831/831 [00:00<00:00, 8577.58 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 18/18 [00:03<00:00,  4.89it/s, loss=0.477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.8172\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 18/18 [00:03<00:00,  4.79it/s, loss=0.528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.2998\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 18/18 [00:03<00:00,  5.08it/s, loss=0.42] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8080\n",
      "Test Metrics: Precision=0.8728, Recall=0.8728, F1=0.8728\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 140, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 140/140 [00:00<00:00, 7730.21 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 4460.16 examples/s]\n",
      "Map: 100%|██████████| 831/831 [00:00<00:00, 8293.22 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 18/18 [00:11<00:00,  1.60it/s, loss=0.489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.9166\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 18/18 [00:11<00:00,  1.62it/s, loss=0.335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.8394\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 18/18 [00:11<00:00,  1.61it/s, loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.4848\n",
      "Test Metrics: Precision=0.8761, Recall=0.8761, F1=0.8761\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 140, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 140/140 [00:00<00:00, 7721.57 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 4373.62 examples/s]\n",
      "Map: 100%|██████████| 831/831 [00:00<00:00, 8431.26 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 18/18 [00:09<00:00,  1.82it/s, loss=0.297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.3403\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s, loss=0.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.1310\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 18/18 [00:10<00:00,  1.75it/s, loss=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.3223\n",
      "Test Metrics: Precision=0.9149, Recall=0.9149, F1=0.9149\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 145, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145/145 [00:00<00:00, 7385.86 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 5073.40 examples/s]\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 8312.86 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:03<00:00,  5.40it/s, loss=0.59] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.7764\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:03<00:00,  5.53it/s, loss=0.387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.3134\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:03<00:00,  5.58it/s, loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.3687\n",
      "Test Metrics: Precision=0.8717, Recall=0.8717, F1=0.8717\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 145, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145/145 [00:00<00:00, 7310.05 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 4935.48 examples/s]\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 8353.19 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:09<00:00,  1.94it/s, loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.8473\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:10<00:00,  1.86it/s, loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.8593\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:10<00:00,  1.78it/s, loss=0.167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.9031\n",
      "Test Metrics: Precision=0.8887, Recall=0.8887, F1=0.8887\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 145, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145/145 [00:00<00:00, 7573.11 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 4708.69 examples/s]\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 8503.53 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:09<00:00,  2.03it/s, loss=0.429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.0919\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:10<00:00,  1.89it/s, loss=0.495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.9393\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:10<00:00,  1.87it/s, loss=0.256] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.9160\n",
      "Test Metrics: Precision=0.9116, Recall=0.9116, F1=0.9116\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 145, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145/145 [00:00<00:00, 8326.13 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 5352.23 examples/s]\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 8443.46 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:03<00:00,  5.96it/s, loss=0.61] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.9283\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:03<00:00,  6.14it/s, loss=0.569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.6017\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:03<00:00,  6.10it/s, loss=0.322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.4622\n",
      "Test Metrics: Precision=0.8745, Recall=0.8745, F1=0.8745\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 145, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145/145 [00:00<00:00, 8179.44 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 5465.75 examples/s]\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 8514.23 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:09<00:00,  1.93it/s, loss=0.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.4468\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:08<00:00,  2.16it/s, loss=0.529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.4012\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:09<00:00,  2.02it/s, loss=0.323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.5738\n",
      "Test Metrics: Precision=0.8980, Recall=0.8980, F1=0.8980\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 145, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145/145 [00:00<00:00, 8278.98 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 5227.78 examples/s]\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 8655.79 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:09<00:00,  2.10it/s, loss=0.573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.2883\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:08<00:00,  2.14it/s, loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.7384\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:09<00:00,  1.98it/s, loss=0.00193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.1461\n",
      "Test Metrics: Precision=0.9132, Recall=0.9132, F1=0.9132\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 145, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145/145 [00:00<00:00, 7494.07 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 4317.88 examples/s]\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 3864.79 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:03<00:00,  5.16it/s, loss=0.815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.0578\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:03<00:00,  4.89it/s, loss=0.282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.8326\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:03<00:00,  5.28it/s, loss=0.157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.8750\n",
      "Test Metrics: Precision=0.8788, Recall=0.8788, F1=0.8788\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 145, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145/145 [00:00<00:00, 6584.46 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 4453.20 examples/s]\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 8352.50 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:11<00:00,  1.63it/s, loss=0.702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.4256\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:11<00:00,  1.61it/s, loss=0.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.9479\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:11<00:00,  1.63it/s, loss=0.124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.9833\n",
      "Test Metrics: Precision=0.8821, Recall=0.8821, F1=0.8821\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 145, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145/145 [00:00<00:00, 7415.40 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 4564.84 examples/s]\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 8737.45 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:10<00:00,  1.77it/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.5916\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:10<00:00,  1.80it/s, loss=0.153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.7651\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:10<00:00,  1.78it/s, loss=0.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3531\n",
      "Test Metrics: Precision=0.8901, Recall=0.8901, F1=0.8901\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 145, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145/145 [00:00<00:00, 8229.80 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 4846.97 examples/s]\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 8365.91 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:03<00:00,  5.73it/s, loss=0.262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.6092\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:03<00:00,  6.26it/s, loss=0.523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.9396\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:03<00:00,  5.86it/s, loss=0.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.4352\n",
      "Test Metrics: Precision=0.8797, Recall=0.8797, F1=0.8797\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 145, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145/145 [00:00<00:00, 7967.38 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 4712.70 examples/s]\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 8361.77 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:08<00:00,  2.11it/s, loss=0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.7749\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:09<00:00,  2.10it/s, loss=0.189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.5917\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:08<00:00,  2.35it/s, loss=0.424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.0954\n",
      "Test Metrics: Precision=0.8945, Recall=0.8945, F1=0.8945\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 145, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145/145 [00:00<00:00, 7330.05 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 5199.62 examples/s]\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 8663.70 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:08<00:00,  2.35it/s, loss=0.571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.1995\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:08<00:00,  2.37it/s, loss=0.0713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.6125\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:07<00:00,  2.44it/s, loss=0.0547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.9421\n",
      "Test Metrics: Precision=0.9068, Recall=0.9068, F1=0.9068\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 145, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145/145 [00:00<00:00, 7621.71 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 5613.83 examples/s]\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 8450.81 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:03<00:00,  5.85it/s, loss=0.417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.6320\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:03<00:00,  5.88it/s, loss=0.214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.6619\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:03<00:00,  5.70it/s, loss=0.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.6593\n",
      "Test Metrics: Precision=0.8721, Recall=0.8721, F1=0.8721\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 145, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145/145 [00:00<00:00, 7726.57 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 6008.44 examples/s]\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 8835.21 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:10<00:00,  1.88it/s, loss=0.733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.0303\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:09<00:00,  1.94it/s, loss=0.541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.5696\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:09<00:00,  1.91it/s, loss=0.574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0576\n",
      "Test Metrics: Precision=0.8901, Recall=0.8901, F1=0.8901\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 145, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 145/145 [00:00<00:00, 7795.80 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 5680.15 examples/s]\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 8760.85 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:08<00:00,  2.15it/s, loss=0.406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.1570\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:08<00:00,  2.14it/s, loss=0.52] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.5217\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:09<00:00,  2.10it/s, loss=0.174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.7386\n",
      "Test Metrics: Precision=0.9113, Recall=0.9113, F1=0.9113\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 150, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 7326.98 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5275.85 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 8165.01 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:03<00:00,  6.02it/s, loss=0.573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.0817\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:02<00:00,  6.36it/s, loss=0.539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.0481\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:02<00:00,  6.35it/s, loss=0.43] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2300\n",
      "Test Metrics: Precision=0.8768, Recall=0.8768, F1=0.8768\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 150, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 7493.66 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5404.57 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 8468.10 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:08<00:00,  2.15it/s, loss=0.577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.5307\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:09<00:00,  2.06it/s, loss=0.353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.9276\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:09<00:00,  2.10it/s, loss=0.223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.4343\n",
      "Test Metrics: Precision=0.8925, Recall=0.8925, F1=0.8925\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 150, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 7787.80 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 4916.35 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 8773.31 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:09<00:00,  2.08it/s, loss=0.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.0393\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:08<00:00,  2.26it/s, loss=0.164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.2451\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:08<00:00,  2.25it/s, loss=0.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.6358\n",
      "Test Metrics: Precision=0.9062, Recall=0.9062, F1=0.9062\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 150, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 8492.79 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 4971.52 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 8596.78 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:02<00:00,  7.01it/s, loss=0.565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.3841\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:02<00:00,  6.73it/s, loss=0.459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.7947\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:02<00:00,  7.27it/s, loss=0.272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.9459\n",
      "Test Metrics: Precision=0.8776, Recall=0.8776, F1=0.8776\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 150, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 8165.53 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 4800.44 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 8181.02 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:08<00:00,  2.31it/s, loss=0.507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.5728\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:08<00:00,  2.22it/s, loss=0.236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.5639\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:08<00:00,  2.24it/s, loss=0.347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.1068\n",
      "Test Metrics: Precision=0.8756, Recall=0.8756, F1=0.8756\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 150, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 8205.36 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 4625.90 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 8858.94 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:07<00:00,  2.45it/s, loss=0.266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.8861\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:07<00:00,  2.53it/s, loss=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.7526\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:07<00:00,  2.48it/s, loss=0.196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.2224\n",
      "Test Metrics: Precision=0.9149, Recall=0.9149, F1=0.9149\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 150, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 7185.23 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 4950.00 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 8798.43 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:03<00:00,  5.89it/s, loss=0.608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.7966\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:03<00:00,  5.40it/s, loss=0.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.9254\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:03<00:00,  5.79it/s, loss=0.401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2575\n",
      "Test Metrics: Precision=0.8749, Recall=0.8749, F1=0.8749\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 150, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 7379.83 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5111.68 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 3786.94 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:11<00:00,  1.69it/s, loss=0.334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.8823\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:11<00:00,  1.63it/s, loss=0.462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.2975\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:11<00:00,  1.62it/s, loss=0.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.7561\n",
      "Test Metrics: Precision=0.8819, Recall=0.8819, F1=0.8819\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 150, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 7553.31 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5148.70 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 9065.04 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:11<00:00,  1.73it/s, loss=0.447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.1942\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:10<00:00,  1.74it/s, loss=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.2879\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:10<00:00,  1.73it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.7237\n",
      "Test Metrics: Precision=0.9155, Recall=0.9155, F1=0.9155\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 150, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 8232.84 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 4393.01 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 8541.12 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:03<00:00,  5.87it/s, loss=0.462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.0922\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:03<00:00,  5.92it/s, loss=0.368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.0388\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:03<00:00,  6.04it/s, loss=0.431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5095\n",
      "Test Metrics: Precision=0.8769, Recall=0.8769, F1=0.8769\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 150, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 7940.95 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 4331.47 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 8550.88 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:09<00:00,  1.93it/s, loss=0.402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.6169\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:09<00:00,  1.93it/s, loss=0.225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.2722\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:10<00:00,  1.81it/s, loss=0.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.3736\n",
      "Test Metrics: Precision=0.9009, Recall=0.9009, F1=0.9009\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 150, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 8255.85 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 3823.90 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 8385.74 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:11<00:00,  1.73it/s, loss=0.33] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 12.7724\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:10<00:00,  1.89it/s, loss=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.3566\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:08<00:00,  2.28it/s, loss=0.151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.6042\n",
      "Test Metrics: Precision=0.9196, Recall=0.9196, F1=0.9196\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 150, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 7826.17 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5312.83 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 8553.30 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:03<00:00,  5.74it/s, loss=0.6]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.9194\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:03<00:00,  5.28it/s, loss=0.645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.0471\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:03<00:00,  5.86it/s, loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.4016\n",
      "Test Metrics: Precision=0.8665, Recall=0.8665, F1=0.8665\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 150, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 7838.84 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5436.56 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 8641.18 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:09<00:00,  1.91it/s, loss=0.397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.4666\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:09<00:00,  1.97it/s, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.4048\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:10<00:00,  1.86it/s, loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.4867\n",
      "Test Metrics: Precision=0.8916, Recall=0.8916, F1=0.8916\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 150, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 7938.95 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 5015.51 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 8965.74 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 19/19 [00:08<00:00,  2.20it/s, loss=0.484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.2326\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 19/19 [00:08<00:00,  2.15it/s, loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.7881\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 19/19 [00:09<00:00,  2.05it/s, loss=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.8824\n",
      "Test Metrics: Precision=0.9120, Recall=0.9120, F1=0.9120\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 155, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 155/155 [00:00<00:00, 7433.22 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 5124.08 examples/s]\n",
      "Map: 100%|██████████| 813/813 [00:00<00:00, 8084.61 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:03<00:00,  5.06it/s, loss=0.511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.3067\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:03<00:00,  5.11it/s, loss=0.59] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.4867\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:04<00:00,  4.76it/s, loss=0.435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.8337\n",
      "Test Metrics: Precision=0.8737, Recall=0.8737, F1=0.8737\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 155, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 155/155 [00:00<00:00, 7823.40 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 5798.15 examples/s]\n",
      "Map: 100%|██████████| 813/813 [00:00<00:00, 8718.87 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:11<00:00,  1.70it/s, loss=0.488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.2298\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:12<00:00,  1.66it/s, loss=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 10.9314\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:11<00:00,  1.80it/s, loss=0.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.7499\n",
      "Test Metrics: Precision=0.8700, Recall=0.8700, F1=0.8700\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 155, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 155/155 [00:00<00:00, 7697.25 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 5177.12 examples/s]\n",
      "Map: 100%|██████████| 813/813 [00:00<00:00, 8636.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:10<00:00,  1.91it/s, loss=0.286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.3489\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:10<00:00,  1.83it/s, loss=0.39] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.8853\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:10<00:00,  1.90it/s, loss=0.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.7836\n",
      "Test Metrics: Precision=0.9159, Recall=0.9159, F1=0.9159\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 155, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 155/155 [00:00<00:00, 8182.00 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 4419.41 examples/s]\n",
      "Map: 100%|██████████| 813/813 [00:00<00:00, 8480.59 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:03<00:00,  5.97it/s, loss=0.43] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.1319\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:03<00:00,  6.42it/s, loss=0.287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.1146\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:03<00:00,  6.22it/s, loss=0.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.3784\n",
      "Test Metrics: Precision=0.8687, Recall=0.8687, F1=0.8687\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 155, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 155/155 [00:00<00:00, 7043.14 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 3457.89 examples/s]\n",
      "Map: 100%|██████████| 813/813 [00:00<00:00, 8332.02 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:09<00:00,  2.05it/s, loss=0.368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.8960\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:09<00:00,  2.05it/s, loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.2457\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:09<00:00,  2.01it/s, loss=0.572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0591\n",
      "Test Metrics: Precision=0.8869, Recall=0.8869, F1=0.8869\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 155, Split 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 155/155 [00:00<00:00, 8005.78 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 4492.55 examples/s]\n",
      "Map: 100%|██████████| 813/813 [00:00<00:00, 8494.17 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:10<00:00,  1.88it/s, loss=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.3897\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:09<00:00,  2.11it/s, loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.4867\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:09<00:00,  2.22it/s, loss=0.215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.5644\n",
      "Test Metrics: Precision=0.9216, Recall=0.9216, F1=0.9216\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 155, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 155/155 [00:00<00:00, 7244.94 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 5016.53 examples/s]\n",
      "Map: 100%|██████████| 813/813 [00:00<00:00, 8919.08 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:03<00:00,  5.12it/s, loss=0.433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 18.3804\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:03<00:00,  5.23it/s, loss=0.619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.3295\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:03<00:00,  5.13it/s, loss=0.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.5456\n",
      "Test Metrics: Precision=0.8824, Recall=0.8824, F1=0.8824\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 155, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 155/155 [00:00<00:00, 6922.25 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 4782.03 examples/s]\n",
      "Map: 100%|██████████| 813/813 [00:00<00:00, 3729.48 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:12<00:00,  1.60it/s, loss=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.4001\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:11<00:00,  1.67it/s, loss=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.2017\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:11<00:00,  1.68it/s, loss=0.565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.5746\n",
      "Test Metrics: Precision=0.8837, Recall=0.8837, F1=0.8837\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 155, Split 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 155/155 [00:00<00:00, 7484.05 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 5233.38 examples/s]\n",
      "Map: 100%|██████████| 813/813 [00:00<00:00, 8337.78 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:11<00:00,  1.70it/s, loss=0.569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.3203\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:11<00:00,  1.75it/s, loss=0.193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.8423\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:11<00:00,  1.73it/s, loss=0.147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.0724\n",
      "Test Metrics: Precision=0.9144, Recall=0.9144, F1=0.9144\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 155, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 155/155 [00:00<00:00, 7704.64 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 5420.80 examples/s]\n",
      "Map: 100%|██████████| 813/813 [00:00<00:00, 8147.51 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:04<00:00,  4.83it/s, loss=0.313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.1423\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:03<00:00,  5.68it/s, loss=0.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.6897\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:03<00:00,  5.32it/s, loss=0.341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.7844\n",
      "Test Metrics: Precision=0.8760, Recall=0.8760, F1=0.8760\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 155, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 155/155 [00:00<00:00, 8115.91 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 4995.33 examples/s]\n",
      "Map: 100%|██████████| 813/813 [00:00<00:00, 8407.76 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:10<00:00,  1.98it/s, loss=0.149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.4731\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:10<00:00,  1.87it/s, loss=0.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.5205\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:10<00:00,  1.84it/s, loss=0.495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.0400\n",
      "Test Metrics: Precision=0.8837, Recall=0.8837, F1=0.8837\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 155, Split 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 155/155 [00:00<00:00, 8365.62 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 5565.12 examples/s]\n",
      "Map: 100%|██████████| 813/813 [00:00<00:00, 8701.43 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:10<00:00,  1.89it/s, loss=0.269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 16.4358\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:09<00:00,  2.01it/s, loss=0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.9864\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:09<00:00,  2.08it/s, loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.8898\n",
      "Test Metrics: Precision=0.9058, Recall=0.9058, F1=0.9058\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 155, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 155/155 [00:00<00:00, 7602.38 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 5348.56 examples/s]\n",
      "Map: 100%|██████████| 813/813 [00:00<00:00, 8052.58 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:04<00:00,  4.94it/s, loss=0.439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 19.4291\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:04<00:00,  4.83it/s, loss=0.526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 11.2765\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:03<00:00,  5.23it/s, loss=0.424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 8.7122\n",
      "Test Metrics: Precision=0.8750, Recall=0.8750, F1=0.8750\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 155, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 155/155 [00:00<00:00, 7943.47 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 4576.68 examples/s]\n",
      "Map: 100%|██████████| 813/813 [00:00<00:00, 8316.90 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:12<00:00,  1.60it/s, loss=0.431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.1858\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:12<00:00,  1.63it/s, loss=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.2281\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:12<00:00,  1.65it/s, loss=0.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.2638\n",
      "Test Metrics: Precision=0.8834, Recall=0.8834, F1=0.8834\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 155, Split 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 155/155 [00:00<00:00, 7951.92 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 4896.01 examples/s]\n",
      "Map: 100%|██████████| 813/813 [00:00<00:00, 8844.84 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:11<00:00,  1.78it/s, loss=0.724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.3841\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:11<00:00,  1.82it/s, loss=0.697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.7398\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:10<00:00,  1.91it/s, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.0012\n",
      "Test Metrics: Precision=0.9155, Recall=0.9155, F1=0.9155\n",
      "\n",
      "Fine-tuning bert-base-cased (small) with Train Size 160, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 160/160 [00:00<00:00, 8063.16 examples/s]\n",
      "Map: 100%|██████████| 32/32 [00:00<00:00, 5297.72 examples/s]\n",
      "Map: 100%|██████████| 807/807 [00:00<00:00, 8661.64 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:03<00:00,  5.61it/s, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 17.0603\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:03<00:00,  5.49it/s, loss=0.391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.9790\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:03<00:00,  5.52it/s, loss=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.1557\n",
      "Test Metrics: Precision=0.8783, Recall=0.8783, F1=0.8783\n",
      "\n",
      "Fine-tuning bert-large-cased (medium) with Train Size 160, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 160/160 [00:00<00:00, 7746.07 examples/s]\n",
      "Map: 100%|██████████| 32/32 [00:00<00:00, 4585.97 examples/s]\n",
      "Map: 100%|██████████| 807/807 [00:00<00:00, 8611.31 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:10<00:00,  1.85it/s, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 15.7648\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 20/20 [00:11<00:00,  1.80it/s, loss=0.433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.4328\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 20/20 [00:11<00:00,  1.71it/s, loss=0.325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.7237\n",
      "Test Metrics: Precision=0.8814, Recall=0.8814, F1=0.8814\n",
      "\n",
      "Fine-tuning roberta-large (large) with Train Size 160, Split 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 160/160 [00:00<00:00, 7921.91 examples/s]\n",
      "Map: 100%|██████████| 32/32 [00:00<00:00, 4918.92 examples/s]\n",
      "Map: 100%|██████████| 807/807 [00:00<00:00, 9030.36 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 20/20 [00:10<00:00,  1.86it/s, loss=0.34] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.8218\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  95%|█████████▌| 19/20 [00:08<00:00,  2.28it/s, loss=0.396]"
     ]
    }
   ],
   "source": [
    "# Define the models and their corresponding sizes\n",
    "models = {\n",
    "    \"small\": \"bert-base-cased\",\n",
    "    \"medium\": \"bert-large-cased\",\n",
    "    \"large\": \"roberta-large\"\n",
    "}\n",
    "\n",
    "iterate_and_finetune_with_torch_lip(dataset=dataset_lip, file_name='Experiments_lipitor.xlsx', models=models, start_size=5, end_size=500, step_size=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
