{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-21T20:36:46.449411Z",
     "start_time": "2024-12-21T20:36:46.445370Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, get_scheduler\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import Dataset, DatasetDict, Features, Sequence, Value, ClassLabel\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Combine all .txt and .ann files and combine them per medicine",
   "id": "6fcf0ba874e901ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T19:49:14.789323Z",
     "start_time": "2024-12-21T19:49:14.418762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Base folders containing annotation and text files\n",
    "annotations_folder = 'annotations/'\n",
    "original_texts_folder = 'originaltexts/'\n",
    "output_folder = 'output_datasets/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Group files by medicine\n",
    "file_groups = {}\n",
    "for file_name in os.listdir(annotations_folder):\n",
    "    if file_name.endswith('.ann'):\n",
    "        base_name = '.'.join(file_name.split('.')[:-1])\n",
    "        medicine = base_name.rsplit('.', 1)[0]\n",
    "        file_groups.setdefault(medicine, []).append(file_name)\n",
    "\n",
    "# Process each group\n",
    "for medicine, ann_files in file_groups.items():\n",
    "    combined_output = []\n",
    "\n",
    "    for ann_file in ann_files:\n",
    "        txt_file = ann_file.replace('.ann', '.txt')\n",
    "        txt_path = os.path.join(original_texts_folder, txt_file)\n",
    "        ann_path = os.path.join(annotations_folder, ann_file)\n",
    "\n",
    "        # Ensure the corresponding .txt file exists\n",
    "        if not os.path.exists(txt_path):\n",
    "            raise FileNotFoundError(f\"Text file not found for annotation file {ann_file}\")\n",
    "\n",
    "        # Read the content of the .ann and .txt files\n",
    "        with open(ann_path, 'r') as ann_f:\n",
    "            ann_lines = ann_f.readlines()\n",
    "\n",
    "        with open(txt_path, 'r') as txt_f:\n",
    "            txt_content = txt_f.read()\n",
    "\n",
    "        # Parse annotations and filter out AnnotatorNotes\n",
    "        annotations = []\n",
    "        for line in ann_lines:\n",
    "            if line.startswith('T'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 3:\n",
    "                    tag_info, word = parts[1], parts[2]\n",
    "                    tag_parts = tag_info.split()\n",
    "                    if len(tag_parts) >= 3:\n",
    "                        tag = tag_parts[0]\n",
    "                        try:\n",
    "                            start_idx = int(tag_parts[1])\n",
    "                            end_idx = int(tag_parts[2])\n",
    "                        except ValueError:\n",
    "                            if ';' in tag_parts[2]:  # Handle ranges like '742;763'\n",
    "                                start_idx = int(tag_parts[1])\n",
    "                                end_idx = int(tag_parts[2].split(';')[-1])\n",
    "                            else:\n",
    "                                raise ValueError(f\"Unexpected annotation format: {tag_parts}\")\n",
    "                        annotations.append((start_idx, end_idx, tag, word))\n",
    "\n",
    "        # Sort annotations by start index\n",
    "        annotations.sort(key=lambda x: x[0])\n",
    "\n",
    "        # Generate output format\n",
    "        output = []\n",
    "        current_idx = 0\n",
    "        for start_idx, end_idx, tag, word in annotations:\n",
    "            # Add text between the last annotation and the current annotation as \"O\"\n",
    "            if current_idx < start_idx:\n",
    "                intervening_text = txt_content[current_idx:start_idx]\n",
    "                for token in re.findall(r\"\\w+(?:'\\w+)?|[.,!?]\", intervening_text):\n",
    "                    output.append(f\"{token} O\")\n",
    "\n",
    "            # Add the annotated word with its tag\n",
    "            for i, token in enumerate(word.split()):\n",
    "                tag_prefix = 'B-' if i == 0 else 'I-'\n",
    "                output.append(f\"{token} {tag_prefix}{tag}\")\n",
    "\n",
    "            current_idx = end_idx\n",
    "\n",
    "        # Add remaining text as \"O\"\n",
    "        if current_idx < len(txt_content):\n",
    "            remaining_text = txt_content[current_idx:]\n",
    "            for token in re.findall(r\"\\w+(?:'\\w+)?|[.,!?]\", remaining_text):\n",
    "                output.append(f\"{token} O\")\n",
    "\n",
    "        # Add to combined output with a newline separator\n",
    "        combined_output.extend(output)\n",
    "        combined_output.append('')  # Empty line between posts\n",
    "\n",
    "    # Write combined output to file\n",
    "    combined_output_text = '\\n'.join(combined_output).strip()\n",
    "    output_file = os.path.join(output_folder, f\"{medicine}_combined_output.txt\")\n",
    "    with open(output_file, 'w') as out_f:\n",
    "        out_f.write(combined_output_text)\n",
    "\n",
    "    print(f\"Combined output saved for {medicine} in {output_file}\")"
   ],
   "id": "aa71104cf543fea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined output saved for ARTHROTEC in output_datasets/ARTHROTEC_combined_output.txt\n",
      "Combined output saved for CAMBIA in output_datasets/CAMBIA_combined_output.txt\n",
      "Combined output saved for CATAFLAM in output_datasets/CATAFLAM_combined_output.txt\n",
      "Combined output saved for DICLOFENAC-POTASSIUM in output_datasets/DICLOFENAC-POTASSIUM_combined_output.txt\n",
      "Combined output saved for DICLOFENAC-SODIUM in output_datasets/DICLOFENAC-SODIUM_combined_output.txt\n",
      "Combined output saved for FLECTOR in output_datasets/FLECTOR_combined_output.txt\n",
      "Combined output saved for LIPITOR in output_datasets/LIPITOR_combined_output.txt\n",
      "Combined output saved for PENNSAID in output_datasets/PENNSAID_combined_output.txt\n",
      "Combined output saved for SOLARAZE in output_datasets/SOLARAZE_combined_output.txt\n",
      "Combined output saved for VOLTAREN-XR in output_datasets/VOLTAREN-XR_combined_output.txt\n",
      "Combined output saved for VOLTAREN in output_datasets/VOLTAREN_combined_output.txt\n",
      "Combined output saved for ZIPSOR in output_datasets/ZIPSOR_combined_output.txt\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Combine all the medicine files into one dataset",
   "id": "806f757861e7d38d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T19:49:18.385903Z",
     "start_time": "2024-12-21T19:49:18.367834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Folder containing all combined output files\n",
    "output_datasets_folder = 'output_datasets/'\n",
    "final_output_file = 'final_dataset.txt'\n",
    "\n",
    "# Ensure the folder exists\n",
    "if not os.path.exists(output_datasets_folder):\n",
    "    raise FileNotFoundError(f\"The folder {output_datasets_folder} does not exist.\")\n",
    "\n",
    "# List all files in the folder\n",
    "output_files = [f for f in os.listdir(output_datasets_folder) if f.endswith('_combined_output.txt')]\n",
    "\n",
    "# Combine all files into a single final dataset\n",
    "final_dataset = []\n",
    "for file_name in output_files:\n",
    "    file_path = os.path.join(output_datasets_folder, file_name)\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read().strip()  # Read and strip any trailing spaces or newlines\n",
    "        final_dataset.append(content)\n",
    "\n",
    "    # Add an empty line to separate posts from different files\n",
    "    final_dataset.append('')\n",
    "\n",
    "# Write the combined dataset to the final output file\n",
    "with open(final_output_file, 'w') as f:\n",
    "    f.write('\\n'.join(final_dataset).strip())  # Ensure no extra trailing newline\n",
    "\n",
    "print(f\"Final dataset saved to {final_output_file}\")"
   ],
   "id": "f940e15d9430fc7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset saved to final_dataset.txt\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Read the final dataset into the Iob dataset format",
   "id": "3269e1b5930cd4c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:08:41.120713Z",
     "start_time": "2024-12-21T20:08:41.114177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_iob_file(file_path):\n",
    "    \"\"\"Reads an IOB file from filepath and returns sentences with tokens and tags.\"\"\"\n",
    "    sentences = []\n",
    "    sentence_tokens = []\n",
    "    sentence_labels = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # If line is not empty\n",
    "                token, tag = line.split()\n",
    "                sentence_tokens.append(token)\n",
    "                sentence_labels.append(tag)\n",
    "\n",
    "            else:\n",
    "                # End of a sentence\n",
    "                if sentence_tokens:\n",
    "                    sentences.append({\"tokens\": sentence_tokens, \"ner_tags\": sentence_labels})\n",
    "                    sentence_tokens = []\n",
    "                    sentence_labels = []\n",
    "        # Add the last sentence if file doesn't end with a newline\n",
    "        if sentence_tokens:\n",
    "            sentences.append({\"tokens\": sentence_tokens, \"ner_tags\": sentence_labels})\n",
    "    return sentences\n",
    "\n",
    "def create_dataset_from_final_file(final_file_path):\n",
    "    \"\"\"Create a dataset from a single IOB file and return it as a DatasetDict.\"\"\"\n",
    "\n",
    "    if not os.path.exists(final_file_path):\n",
    "        raise FileNotFoundError(f\"The file {final_file_path} does not exist.\")\n",
    "\n",
    "    # Parse the file\n",
    "    data = read_iob_file(final_file_path)\n",
    "\n",
    "    # Define the label names and ClassLabel feature\n",
    "    unique_labels = sorted(set(tag for d in data for tag in d[\"ner_tags\"]))\n",
    "    label_feature = ClassLabel(names=unique_labels)\n",
    "\n",
    "    # Define the Features schema for Hugging Face datasets\n",
    "    features = Features({\n",
    "        'tokens': Sequence(Value(\"string\")),\n",
    "        'ner_tags': Sequence(label_feature)\n",
    "    })\n",
    "\n",
    "    # Convert data into a Dataset\n",
    "    dataset = Dataset.from_list(data).cast(features)\n",
    "\n",
    "    # Create a DatasetDict\n",
    "    dataset_dict = DatasetDict({\"full_data\": dataset})\n",
    "\n",
    "    return dataset_dict\n"
   ],
   "id": "435d59ec1801f10d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:08:43.130990Z",
     "start_time": "2024-12-21T20:08:42.868065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_dataset_path = \"final_dataset.txt\"\n",
    "dataset_dict = create_dataset_from_final_file(final_dataset_path)\n",
    "dataset = dataset_dict['full_data']"
   ],
   "id": "35b8e6d4d262ac49",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 1248/1248 [00:00<00:00, 6546.79 examples/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenize and align labels, also add datacollator",
   "id": "a85774695bc51613"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:23:18.587790Z",
     "start_time": "2024-12-21T20:23:18.582422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ],
   "id": "5b8c9593f60bdb7e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example usage",
   "id": "468a7a1e0d4a46a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:14:00.022551Z",
     "start_time": "2024-12-21T20:13:59.829771Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True, remove_columns=dataset.column_names)",
   "id": "7926aee2e986a053",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbert-base-cased\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m tokenized_dataset \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mmap(\u001B[43mtokenize_and_align_labels\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m)\u001B[49m, batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, remove_columns\u001B[38;5;241m=\u001B[39mdataset\u001B[38;5;241m.\u001B[39mcolumn_names)\n",
      "Cell \u001B[1;32mIn[8], line 29\u001B[0m, in \u001B[0;36mtokenize_and_align_labels\u001B[1;34m(examples, tokenizer_)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize_and_align_labels\u001B[39m(examples, tokenizer_):\n\u001B[0;32m     28\u001B[0m     tokenized_inputs \u001B[38;5;241m=\u001B[39m tokenizer_(\n\u001B[1;32m---> 29\u001B[0m         \u001B[43mexamples\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     30\u001B[0m         is_split_into_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     31\u001B[0m     )\n\u001B[0;32m     32\u001B[0m     all_labels \u001B[38;5;241m=\u001B[39m examples[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mner_tags\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     33\u001B[0m     new_labels \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[1;31mTypeError\u001B[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset generators",
   "id": "1798cebc48c3299c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:15:21.261040Z",
     "start_time": "2024-12-21T20:15:21.255888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_train_datasets(dataset, number_of_samples, number_of_splits):\n",
    "    \"\"\"\n",
    "    Generates train datasets by sampling from the given dataset based on the number of samples and splits.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The base dataset to sample from.\n",
    "        number_of_samples (int): Number of samples per dataset.\n",
    "        number_of_splits (int): Number of datasets to generate (different seeds).\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, Dataset, List[int]]]: List of generated datasets with their names and indices.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "\n",
    "    for seed in range(number_of_splits):\n",
    "        # Set the random seed for reproducibility\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Shuffle and sample from the dataset\n",
    "        indices = list(range(len(dataset)))\n",
    "        random.shuffle(indices)\n",
    "        sampled_indices = indices[:number_of_samples]\n",
    "\n",
    "        sampled_dataset = dataset.select(sampled_indices)\n",
    "\n",
    "        # Add the dataset with its name and indices\n",
    "        datasets.append((f\"train_dataset_{number_of_samples}_{seed}\", sampled_dataset, sampled_indices))\n",
    "\n",
    "    return datasets"
   ],
   "id": "b4b79e14d8ca0efa",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:15:23.921472Z",
     "start_time": "2024-12-21T20:15:23.916446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_validation_datasets(dataset, train_indices, number_of_samples, number_of_splits):\n",
    "    \"\"\"\n",
    "    Generates validation datasets by sampling from the given dataset, ensuring no overlap with training data.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The base dataset to sample from.\n",
    "        train_indices (List[int]): Indices of the training dataset to exclude from sampling.\n",
    "        number_of_samples (int): Number of samples per validation dataset.\n",
    "        number_of_splits (int): Number of validation datasets to generate (different seeds).\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, Dataset, List[int]]]: List of generated validation datasets with names and indices.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    all_indices = set(range(len(dataset)))\n",
    "    available_indices = list(all_indices - set(train_indices))  # Exclude training indices\n",
    "\n",
    "    for seed in range(number_of_splits):\n",
    "        # Set the random seed for reproducibility\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Shuffle and sample from the remaining indices\n",
    "        random.shuffle(available_indices)\n",
    "        sampled_indices = available_indices[:number_of_samples]\n",
    "\n",
    "        sampled_dataset = dataset.select(sampled_indices)\n",
    "\n",
    "        # Add the dataset with its name and indices\n",
    "        datasets.append((f\"val_dataset_{number_of_samples}_{seed}\", sampled_dataset, sampled_indices))\n",
    "\n",
    "    return datasets"
   ],
   "id": "513adc9d9f4ee109",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:15:26.499820Z",
     "start_time": "2024-12-21T20:15:26.495146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_test_datasets(dataset, train_indices, val_indices, number_of_samples, number_of_splits):\n",
    "    \"\"\"\n",
    "    Generates test datasets by sampling from the given dataset, ensuring no overlap with training or validation data.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The base dataset to sample from.\n",
    "        train_indices (List[int]): Indices of the training dataset to exclude from sampling.\n",
    "        val_indices (List[int]): Indices of the validation dataset to exclude from sampling.\n",
    "        number_of_samples (int): Number of samples per test dataset.\n",
    "        number_of_splits (int): Number of test datasets to generate (different seeds).\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, Dataset]]: List of generated test datasets with names.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    all_indices = set(range(len(dataset)))\n",
    "    available_indices = list(all_indices - set(train_indices) - set(val_indices))  # Exclude train and val indices\n",
    "\n",
    "    for seed in range(number_of_splits):\n",
    "        # Set the random seed for reproducibility\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Shuffle and sample from the remaining indices\n",
    "        random.shuffle(available_indices)\n",
    "        sampled_indices = available_indices[:number_of_samples]\n",
    "\n",
    "        sampled_dataset = dataset.select(sampled_indices)\n",
    "\n",
    "        # Add the dataset with its name\n",
    "        datasets.append((f\"test_dataset_{number_of_samples}_{seed}\", sampled_dataset))\n",
    "\n",
    "    return datasets"
   ],
   "id": "58e5dae8d7e7bcf8",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example usage",
   "id": "87bfb3346f6372cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:06:41.997338Z",
     "start_time": "2024-12-21T20:06:41.987743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Generate Train Dataset\n",
    "# train_datasets = generate_train_datasets(dataset, number_of_samples=30, number_of_splits=1)\n",
    "# train_name, train_dataset, train_indices = train_datasets[0]\n",
    "# print(f\"{train_name}: {len(train_dataset)} samples\")\n",
    "#\n",
    "# # Step 2: Generate Validation Dataset\n",
    "# val_datasets = generate_validation_datasets(dataset, train_indices, number_of_samples=30, number_of_splits=1)\n",
    "# val_name, val_dataset, val_indices = val_datasets[0]\n",
    "# print(f\"{val_name}: {len(val_dataset)} samples\")\n",
    "#\n",
    "# # Step 3: Generate Test Dataset\n",
    "# test_datasets = generate_test_datasets(dataset, train_indices, val_indices, number_of_samples=30, number_of_splits=1)\n",
    "# test_name, test_dataset = test_datasets[0]\n",
    "# print(f\"{test_name}: {len(test_dataset)} samples\")"
   ],
   "id": "6f8557b1a1dc6dd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepping for training",
   "id": "b7084bbcd75432d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:24:47.698840Z",
     "start_time": "2024-12-21T20:24:46.963059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the evaluation metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "label_names = dataset.features[\"ner_tags\"].feature.names\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ],
   "id": "bc367cf3648c3146",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:21:16.377126Z",
     "start_time": "2024-12-21T20:21:16.372177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataset_given_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    data_collator_ = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "    # Tokenize and align labels\n",
    "    tokenized_dataset_ = dataset.map(tokenize_and_align_labels, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    # Step 1: Generate Train Dataset\n",
    "    train_datasets = generate_train_datasets(tokenized_dataset_, number_of_samples=30, number_of_splits=1)\n",
    "    train_name, train_dataset, train_indices = train_datasets[0]\n",
    "    print(f\"{train_name}: {len(train_dataset)} samples\")\n",
    "\n",
    "    # Step 2: Generate Validation Dataset\n",
    "    val_datasets = generate_validation_datasets(tokenized_dataset_, train_indices, number_of_samples=30, number_of_splits=1)\n",
    "    val_name, val_dataset, val_indices = val_datasets[0]\n",
    "    print(f\"{val_name}: {len(val_dataset)} samples\")\n",
    "\n",
    "    # Step 3: Generate Test Dataset\n",
    "    test_datasets = generate_test_datasets(tokenized_dataset_, train_indices, val_indices, number_of_samples=30, number_of_splits=1)\n",
    "    test_name, test_dataset = test_datasets[0]\n",
    "    print(f\"{test_name}: {len(test_dataset)} samples\")\n",
    "\n",
    "    return data_collator_, train_dataset, val_dataset, test_dataset\n"
   ],
   "id": "1f56f43a627b4076",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:21:18.109997Z",
     "start_time": "2024-12-21T20:21:18.105784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Flatten predictions and labels, removing ignored indices\n",
    "    true_labels = [label for label_seq in labels for label in label_seq if label != -100]\n",
    "    true_predictions = [pred for pred_seq, label_seq in zip(predictions, labels)\n",
    "                        for pred, label in zip(pred_seq, label_seq) if label != -100]\n",
    "    return true_labels, true_predictions\n"
   ],
   "id": "2a875af61f42f00a",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training and evaluation",
   "id": "e30995779e545bcf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T22:49:07.413263Z",
     "start_time": "2024-12-21T21:26:30.001608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define the models and their corresponding sizes\n",
    "models = {\n",
    "    \"small\": \"bert-base-cased\",\n",
    "    \"medium\": \"bert-large-cased\",\n",
    "    \"large\": \"roberta-large\"\n",
    "}\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    \"learning_rate\": [5e-6, 2e-5, 5e-5],\n",
    "    \"batch_size\": [8, 16],\n",
    "    \"weight_decay\": [0.0, 0.01]\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loop over models\n",
    "for size, model_name in models.items():\n",
    "    print(f\"\\nTuning and evaluating {model_name} ({size})...\")\n",
    "    best_f1 = 0.0\n",
    "    best_hyperparameters = None\n",
    "    model_save_path = f\"saved_models/{model_name}\"\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "    # Hyperparameter tuning loop\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(f\"\\nHyperparameters: {params}\")\n",
    "\n",
    "        # Create datasets and data collator for the current model\n",
    "        data_collator, tokenized_train, tokenized_val, tokenized_test = create_dataset_given_model(model_name)\n",
    "\n",
    "        # Create DataLoaders\n",
    "        train_dataloader = DataLoader(tokenized_train, batch_size=params[\"batch_size\"], shuffle=True, collate_fn=data_collator)\n",
    "        val_dataloader = DataLoader(tokenized_val, batch_size=params[\"batch_size\"], collate_fn=data_collator)\n",
    "        test_dataloader = DataLoader(tokenized_test, batch_size=params[\"batch_size\"], collate_fn=data_collator)\n",
    "\n",
    "        # Initialize the model for token classification\n",
    "        model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name, id2label=id2label, label2id=label2id\n",
    "        )\n",
    "\n",
    "        # Set up optimizer and learning rate scheduler\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"]\n",
    "        )\n",
    "        num_train_epochs = 3\n",
    "        num_update_steps_per_epoch = len(train_dataloader)\n",
    "        num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "        lr_scheduler = get_scheduler(\n",
    "            \"linear\",\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "\n",
    "        # Use the accelerator for distributed training\n",
    "        accelerator = Accelerator()\n",
    "        model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "            model, optimizer, train_dataloader, val_dataloader\n",
    "        )\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_train_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_train_epochs}\")\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\")\n",
    "            for batch in progress_bar:\n",
    "                # Move batch data to the same device as the model\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} Loss: {total_loss:.4f}\")\n",
    "\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            val_predictions, val_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_dataloader:\n",
    "                    # Move batch data to the same device as the model\n",
    "                    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                    outputs = model(**batch)\n",
    "                    logits = outputs.logits\n",
    "                    predictions = logits.argmax(dim=-1)\n",
    "                    labels = batch[\"labels\"]\n",
    "\n",
    "                    # Handle padding across processes for multi-GPU\n",
    "                    predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "                    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "                    predictions_gathered = accelerator.gather(predictions)\n",
    "                    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "                    # Postprocess to get flattened labels and predictions\n",
    "                    flat_labels, flat_predictions = postprocess(predictions_gathered, labels_gathered)\n",
    "                    val_labels.extend(flat_labels)\n",
    "                    val_predictions.extend(flat_predictions)\n",
    "\n",
    "            # Calculate validation metrics\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                val_labels, val_predictions, average=\"micro\"  # 'micro' aggregates across all classes\n",
    "            )\n",
    "            print(f\"Validation Metrics (Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f})\")\n",
    "\n",
    "        # Test loop for evaluation after training\n",
    "        model.eval()\n",
    "        test_predictions, test_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                # Move batch data to the same device as the model\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                logits = outputs.logits\n",
    "                predictions = logits.argmax(dim=-1)\n",
    "                labels = batch[\"labels\"]\n",
    "\n",
    "                # Handle padding across processes for multi-GPU\n",
    "                predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "                labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "                predictions_gathered = accelerator.gather(predictions)\n",
    "                labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "                # Postprocess to get flattened labels and predictions\n",
    "                flat_labels, flat_predictions = postprocess(predictions_gathered, labels_gathered)\n",
    "                test_labels.extend(flat_labels)\n",
    "                test_predictions.extend(flat_predictions)\n",
    "\n",
    "        # Calculate test metrics\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            test_labels, test_predictions, average=\"micro\"  # 'micro' aggregates across all classes\n",
    "        )\n",
    "        print(f\"Test Metrics (Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f})\")\n",
    "\n",
    "        # Save the best model based on test F1-score\n",
    "        if f1 > best_f1:\n",
    "            print(f\"New best model found for {model_name} with F1: {f1:.4f}\")\n",
    "            best_f1 = f1\n",
    "            best_hyperparameters = params\n",
    "\n",
    "            # Overwrite the saved model\n",
    "            accelerator.unwrap_model(model).save_pretrained(model_save_path)\n",
    "            tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "            # Save the best hyperparameters to a JSON file in the model's folder\n",
    "            best_config_path = os.path.join(model_save_path, \"best_config.json\")\n",
    "            with open(best_config_path, \"w\") as f:\n",
    "                json.dump({\n",
    "                    \"best_hyperparameters\": best_hyperparameters,\n",
    "                    \"best_f1\": best_f1\n",
    "                }, f, indent=4)\n",
    "\n",
    "    print(f\"\\nBest Model for {model_name} ({size}): {model_save_path}\")\n",
    "    print(f\"Best Hyperparameters: {best_hyperparameters}\")\n",
    "    print(f\"Best F1-Score: {best_f1:.4f}\") \n"
   ],
   "id": "fa33f20959c42e46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning and evaluating bert-base-cased (small)...\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 5e-06, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 5360.95 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:11<00:00,  2.78s/it, loss=2.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 10.3012\n",
      "Validation Metrics (Precision: 0.0757, Recall: 0.0757, F1: 0.0757)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:10<00:00,  2.69s/it, loss=2.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.4678\n",
      "Validation Metrics (Precision: 0.1252, Recall: 0.1252, F1: 0.1252)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:10<00:00,  2.73s/it, loss=2.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 9.0833\n",
      "Validation Metrics (Precision: 0.1642, Recall: 0.1642, F1: 0.1642)\n",
      "Test Metrics (Precision: 0.1584, Recall: 0.1584, F1: 0.1584)\n",
      "New best model found for bert-base-cased with F1: 0.1584\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 5e-06, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4521.32 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:07<00:00,  1.95s/it, loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.8535\n",
      "Validation Metrics (Precision: 0.4916, Recall: 0.4916, F1: 0.4916)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:07<00:00,  1.84s/it, loss=2.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 8.1376\n",
      "Validation Metrics (Precision: 0.6289, Recall: 0.6289, F1: 0.6289)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:07<00:00,  1.90s/it, loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 7.7468\n",
      "Validation Metrics (Precision: 0.6726, Recall: 0.6726, F1: 0.6726)\n",
      "Test Metrics (Precision: 0.6875, Recall: 0.6875, F1: 0.6875)\n",
      "New best model found for bert-base-cased with F1: 0.6875\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 2e-05, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4956.15 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it, loss=1.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.1044\n",
      "Validation Metrics (Precision: 0.7419, Recall: 0.7419, F1: 0.7419)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it, loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.4879\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it, loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.3846\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Test Metrics (Precision: 0.7664, Recall: 0.7664, F1: 0.7664)\n",
      "New best model found for bert-base-cased with F1: 0.7664\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 2e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 3776.43 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:08<00:00,  2.02s/it, loss=1.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.9551\n",
      "Validation Metrics (Precision: 0.7254, Recall: 0.7254, F1: 0.7254)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:07<00:00,  1.85s/it, loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.7971\n",
      "Validation Metrics (Precision: 0.7456, Recall: 0.7456, F1: 0.7456)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:07<00:00,  1.85s/it, loss=1.08] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4185\n",
      "Validation Metrics (Precision: 0.7476, Recall: 0.7476, F1: 0.7476)\n",
      "Test Metrics (Precision: 0.7644, Recall: 0.7644, F1: 0.7644)\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 3831.43 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:09<00:00,  2.44s/it, loss=1.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.0176\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:09<00:00,  2.38s/it, loss=1.09] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.9114\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:09<00:00,  2.39s/it, loss=0.883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.2800\n",
      "Validation Metrics (Precision: 0.7534, Recall: 0.7534, F1: 0.7534)\n",
      "Test Metrics (Precision: 0.7704, Recall: 0.7704, F1: 0.7704)\n",
      "New best model found for bert-base-cased with F1: 0.7704\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4916.34 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:07<00:00,  1.99s/it, loss=1.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.2509\n",
      "Validation Metrics (Precision: 0.7487, Recall: 0.7487, F1: 0.7487)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:07<00:00,  1.85s/it, loss=0.612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.3492\n",
      "Validation Metrics (Precision: 0.7537, Recall: 0.7537, F1: 0.7537)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:07<00:00,  1.87s/it, loss=0.643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.8711\n",
      "Validation Metrics (Precision: 0.7537, Recall: 0.7537, F1: 0.7537)\n",
      "Test Metrics (Precision: 0.7724, Recall: 0.7724, F1: 0.7724)\n",
      "New best model found for bert-base-cased with F1: 0.7724\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 5e-06, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 5111.38 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:13<00:00,  6.67s/it, loss=2.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.8278\n",
      "Validation Metrics (Precision: 0.1339, Recall: 0.1339, F1: 0.1339)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:13<00:00,  6.74s/it, loss=2.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.6048\n",
      "Validation Metrics (Precision: 0.1985, Recall: 0.1985, F1: 0.1985)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:12<00:00,  6.07s/it, loss=2.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4536\n",
      "Validation Metrics (Precision: 0.2281, Recall: 0.2281, F1: 0.2281)\n",
      "Test Metrics (Precision: 0.2346, Recall: 0.2346, F1: 0.2346)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 5e-06, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4924.15 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:10<00:00,  5.01s/it, loss=2.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.6608\n",
      "Validation Metrics (Precision: 0.1659, Recall: 0.1659, F1: 0.1659)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:10<00:00,  5.24s/it, loss=2.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.4301\n",
      "Validation Metrics (Precision: 0.2692, Recall: 0.2692, F1: 0.2692)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:10<00:00,  5.25s/it, loss=2.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.3035\n",
      "Validation Metrics (Precision: 0.3250, Recall: 0.3250, F1: 0.3250)\n",
      "Test Metrics (Precision: 0.3352, Recall: 0.3352, F1: 0.3352)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 2e-05, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 5204.58 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:13<00:00,  6.82s/it, loss=2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.6484\n",
      "Validation Metrics (Precision: 0.5848, Recall: 0.5848, F1: 0.5848)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:13<00:00,  6.73s/it, loss=1.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.5673\n",
      "Validation Metrics (Precision: 0.7476, Recall: 0.7476, F1: 0.7476)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:14<00:00,  7.07s/it, loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.9820\n",
      "Validation Metrics (Precision: 0.7487, Recall: 0.7487, F1: 0.7487)\n",
      "Test Metrics (Precision: 0.7654, Recall: 0.7654, F1: 0.7654)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 2e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4815.68 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:10<00:00,  5.23s/it, loss=2.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.6797\n",
      "Validation Metrics (Precision: 0.5744, Recall: 0.5744, F1: 0.5744)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:10<00:00,  5.27s/it, loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.8137\n",
      "Validation Metrics (Precision: 0.7342, Recall: 0.7342, F1: 0.7342)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:10<00:00,  5.03s/it, loss=1.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.2827\n",
      "Validation Metrics (Precision: 0.7413, Recall: 0.7413, F1: 0.7413)\n",
      "Test Metrics (Precision: 0.7594, Recall: 0.7594, F1: 0.7594)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 5e-05, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4969.43 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:13<00:00,  6.65s/it, loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.1419\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:12<00:00,  6.39s/it, loss=0.982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.2646\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:14<00:00,  7.07s/it, loss=1.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.8821\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Test Metrics (Precision: 0.7664, Recall: 0.7664, F1: 0.7664)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 5e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4872.38 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:10<00:00,  5.31s/it, loss=1.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.1059\n",
      "Validation Metrics (Precision: 0.7480, Recall: 0.7480, F1: 0.7480)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:09<00:00,  4.97s/it, loss=0.996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.2363\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:10<00:00,  5.03s/it, loss=0.858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.9306\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Test Metrics (Precision: 0.7664, Recall: 0.7664, F1: 0.7664)\n",
      "\n",
      "Best Model for bert-base-cased (small): saved_models/bert-base-cased\n",
      "Best Hyperparameters: {'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.01}\n",
      "Best F1-Score: 0.7724\n",
      "\n",
      "Tuning and evaluating bert-large-cased (medium)...\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 5e-06, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4682.45 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:42<00:00, 10.52s/it, loss=1.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.1968\n",
      "Validation Metrics (Precision: 0.6746, Recall: 0.6746, F1: 0.6746)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:48<00:00, 12.09s/it, loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 6.5508\n",
      "Validation Metrics (Precision: 0.7379, Recall: 0.7379, F1: 0.7379)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:46<00:00, 11.56s/it, loss=1.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.8392\n",
      "Validation Metrics (Precision: 0.7466, Recall: 0.7466, F1: 0.7466)\n",
      "Test Metrics (Precision: 0.7617, Recall: 0.7617, F1: 0.7617)\n",
      "New best model found for bert-large-cased with F1: 0.7617\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 5e-06, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4803.88 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:35<00:00,  8.95s/it, loss=2.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 9.2280\n",
      "Validation Metrics (Precision: 0.3745, Recall: 0.3745, F1: 0.3745)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:37<00:00,  9.42s/it, loss=1.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.5880\n",
      "Validation Metrics (Precision: 0.6171, Recall: 0.6171, F1: 0.6171)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:36<00:00,  9.16s/it, loss=1.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.7410\n",
      "Validation Metrics (Precision: 0.6585, Recall: 0.6585, F1: 0.6585)\n",
      "Test Metrics (Precision: 0.6798, Recall: 0.6798, F1: 0.6798)\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 2e-05, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4803.36 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:35<00:00,  8.97s/it, loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.2831\n",
      "Validation Metrics (Precision: 0.7487, Recall: 0.7487, F1: 0.7487)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:46<00:00, 11.67s/it, loss=0.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.1554\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:44<00:00, 11.13s/it, loss=0.721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.7644\n",
      "Validation Metrics (Precision: 0.7487, Recall: 0.7487, F1: 0.7487)\n",
      "Test Metrics (Precision: 0.7637, Recall: 0.7637, F1: 0.7637)\n",
      "New best model found for bert-large-cased with F1: 0.7637\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 2e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4700.79 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:33<00:00,  8.37s/it, loss=1.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.0064\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:37<00:00,  9.40s/it, loss=0.748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.8730\n",
      "Validation Metrics (Precision: 0.7493, Recall: 0.7493, F1: 0.7493)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:38<00:00,  9.73s/it, loss=0.816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.4634\n",
      "Validation Metrics (Precision: 0.7497, Recall: 0.7497, F1: 0.7497)\n",
      "Test Metrics (Precision: 0.7664, Recall: 0.7664, F1: 0.7664)\n",
      "New best model found for bert-large-cased with F1: 0.7664\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4739.79 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:37<00:00,  9.26s/it, loss=0.983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.7285\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:46<00:00, 11.59s/it, loss=0.806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.1179\n",
      "Validation Metrics (Precision: 0.7513, Recall: 0.7513, F1: 0.7513)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:49<00:00, 12.31s/it, loss=0.708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.6751\n",
      "Validation Metrics (Precision: 0.7584, Recall: 0.7584, F1: 0.7584)\n",
      "Test Metrics (Precision: 0.7754, Recall: 0.7754, F1: 0.7754)\n",
      "New best model found for bert-large-cased with F1: 0.7754\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4675.47 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:37<00:00,  9.28s/it, loss=0.837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.8950\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:35<00:00,  8.77s/it, loss=0.807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.1555\n",
      "Validation Metrics (Precision: 0.7534, Recall: 0.7534, F1: 0.7534)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:38<00:00,  9.73s/it, loss=0.602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.5244\n",
      "Validation Metrics (Precision: 0.7732, Recall: 0.7732, F1: 0.7732)\n",
      "Test Metrics (Precision: 0.7951, Recall: 0.7951, F1: 0.7951)\n",
      "New best model found for bert-large-cased with F1: 0.7951\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 5e-06, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4887.98 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [01:01<00:00, 30.98s/it, loss=2.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.7983\n",
      "Validation Metrics (Precision: 0.2338, Recall: 0.2338, F1: 0.2338)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [01:03<00:00, 31.70s/it, loss=2.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.3216\n",
      "Validation Metrics (Precision: 0.3631, Recall: 0.3631, F1: 0.3631)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [01:07<00:00, 33.72s/it, loss=2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.0289\n",
      "Validation Metrics (Precision: 0.4337, Recall: 0.4337, F1: 0.4337)\n",
      "Test Metrics (Precision: 0.4378, Recall: 0.4378, F1: 0.4378)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 5e-06, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4863.79 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:42<00:00, 21.43s/it, loss=2.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.7058\n",
      "Validation Metrics (Precision: 0.2638, Recall: 0.2638, F1: 0.2638)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:46<00:00, 23.41s/it, loss=2.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.2283\n",
      "Validation Metrics (Precision: 0.3991, Recall: 0.3991, F1: 0.3991)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:48<00:00, 24.40s/it, loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9542\n",
      "Validation Metrics (Precision: 0.4505, Recall: 0.4505, F1: 0.4505)\n",
      "Test Metrics (Precision: 0.4726, Recall: 0.4726, F1: 0.4726)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 2e-05, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 5186.31 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:59<00:00, 29.90s/it, loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.4273\n",
      "Validation Metrics (Precision: 0.7234, Recall: 0.7234, F1: 0.7234)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [01:02<00:00, 31.32s/it, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.7470\n",
      "Validation Metrics (Precision: 0.7470, Recall: 0.7470, F1: 0.7470)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [01:00<00:00, 30.39s/it, loss=1.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.1331\n",
      "Validation Metrics (Precision: 0.7476, Recall: 0.7476, F1: 0.7476)\n",
      "Test Metrics (Precision: 0.7647, Recall: 0.7647, F1: 0.7647)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 2e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 3213.85 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:39<00:00, 19.96s/it, loss=2.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.6286\n",
      "Validation Metrics (Precision: 0.7342, Recall: 0.7342, F1: 0.7342)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:48<00:00, 24.08s/it, loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.9810\n",
      "Validation Metrics (Precision: 0.7453, Recall: 0.7453, F1: 0.7453)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:44<00:00, 22.26s/it, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.2452\n",
      "Validation Metrics (Precision: 0.7460, Recall: 0.7460, F1: 0.7460)\n",
      "Test Metrics (Precision: 0.7640, Recall: 0.7640, F1: 0.7640)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 5e-05, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4850.36 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:59<00:00, 29.61s/it, loss=1.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.5859\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:58<00:00, 29.06s/it, loss=0.833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.8553\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [01:02<00:00, 31.21s/it, loss=0.97] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.7428\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Test Metrics (Precision: 0.7660, Recall: 0.7660, F1: 0.7660)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 5e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4776.92 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:41<00:00, 20.63s/it, loss=1.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.5614\n",
      "Validation Metrics (Precision: 0.7460, Recall: 0.7460, F1: 0.7460)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:37<00:00, 18.62s/it, loss=0.761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.7911\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:48<00:00, 24.21s/it, loss=0.76] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.5784\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Test Metrics (Precision: 0.7660, Recall: 0.7660, F1: 0.7660)\n",
      "\n",
      "Best Model for bert-large-cased (medium): saved_models/bert-large-cased\n",
      "Best Hyperparameters: {'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.01}\n",
      "Best F1-Score: 0.7951\n",
      "\n",
      "Tuning and evaluating roberta-large (large)...\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 5e-06, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4852.51 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:43<00:00, 10.81s/it, loss=1.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 7.7519\n",
      "Validation Metrics (Precision: 0.7413, Recall: 0.7413, F1: 0.7413)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:40<00:00, 10.13s/it, loss=0.997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.0145\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:35<00:00,  8.82s/it, loss=0.899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4752\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Test Metrics (Precision: 0.7664, Recall: 0.7664, F1: 0.7664)\n",
      "New best model found for roberta-large with F1: 0.7664\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 5e-06, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4863.14 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:34<00:00,  8.65s/it, loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.1065\n",
      "Validation Metrics (Precision: 0.7137, Recall: 0.7137, F1: 0.7137)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:31<00:00,  7.88s/it, loss=1.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.3463\n",
      "Validation Metrics (Precision: 0.7473, Recall: 0.7473, F1: 0.7473)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:30<00:00,  7.60s/it, loss=1.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.4653\n",
      "Validation Metrics (Precision: 0.7476, Recall: 0.7476, F1: 0.7476)\n",
      "Test Metrics (Precision: 0.7660, Recall: 0.7660, F1: 0.7660)\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 2e-05, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4790.23 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:40<00:00, 10.02s/it, loss=1.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.0436\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:41<00:00, 10.32s/it, loss=0.955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.2734\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:37<00:00,  9.31s/it, loss=0.995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.0045\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Test Metrics (Precision: 0.7664, Recall: 0.7664, F1: 0.7664)\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 2e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 5000.32 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:32<00:00,  8.15s/it, loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.9525\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:30<00:00,  7.71s/it, loss=1.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.3242\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:30<00:00,  7.65s/it, loss=1.08] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.9590\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Test Metrics (Precision: 0.7664, Recall: 0.7664, F1: 0.7664)\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4905.73 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:39<00:00,  9.93s/it, loss=1.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.5838\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:39<00:00,  9.97s/it, loss=0.911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.9640\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:36<00:00,  9.10s/it, loss=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.7695\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Test Metrics (Precision: 0.7664, Recall: 0.7664, F1: 0.7664)\n",
      "\n",
      "Hyperparameters: {'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4746.59 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:33<00:00,  8.38s/it, loss=1.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.8411\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:33<00:00,  8.47s/it, loss=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.9956\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:34<00:00,  8.51s/it, loss=0.879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.7734\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Test Metrics (Precision: 0.7664, Recall: 0.7664, F1: 0.7664)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 5e-06, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4730.51 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:47<00:00, 23.87s/it, loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.2364\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:54<00:00, 27.35s/it, loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.7224\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [01:04<00:00, 32.32s/it, loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.4379\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Test Metrics (Precision: 0.7664, Recall: 0.7664, F1: 0.7664)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 5e-06, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 3123.63 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:39<00:00, 19.97s/it, loss=1.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.2153\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:48<00:00, 24.35s/it, loss=1.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.7249\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:45<00:00, 22.63s/it, loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.4922\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Test Metrics (Precision: 0.7664, Recall: 0.7664, F1: 0.7664)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 2e-05, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 5259.90 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:55<00:00, 27.58s/it, loss=2.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.0290\n",
      "Validation Metrics (Precision: 0.7288, Recall: 0.7288, F1: 0.7288)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [01:02<00:00, 31.43s/it, loss=1.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.5600\n",
      "Validation Metrics (Precision: 0.7473, Recall: 0.7473, F1: 0.7473)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:50<00:00, 25.35s/it, loss=1.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.2364\n",
      "Validation Metrics (Precision: 0.7480, Recall: 0.7480, F1: 0.7480)\n",
      "Test Metrics (Precision: 0.7660, Recall: 0.7660, F1: 0.7660)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 2e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4685.89 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:43<00:00, 21.90s/it, loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.8894\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:48<00:00, 24.38s/it, loss=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.3433\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:38<00:00, 19.29s/it, loss=0.987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.1074\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Test Metrics (Precision: 0.7664, Recall: 0.7664, F1: 0.7664)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 5e-05, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4478.71 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:54<00:00, 27.27s/it, loss=1.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.8790\n",
      "Validation Metrics (Precision: 0.7480, Recall: 0.7480, F1: 0.7480)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [01:02<00:00, 31.10s/it, loss=1.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.3610\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [01:09<00:00, 35.00s/it, loss=0.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.9819\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Test Metrics (Precision: 0.7664, Recall: 0.7664, F1: 0.7664)\n",
      "\n",
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 5e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1248/1248 [00:00<00:00, 4827.13 examples/s]\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_30_0: 30 samples\n",
      "val_dataset_30_0: 30 samples\n",
      "test_dataset_30_0: 30 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [01:05<00:00, 32.92s/it, loss=1.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.7813\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:51<00:00, 25.84s/it, loss=0.995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.7757\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:50<00:00, 25.32s/it, loss=0.989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.1307\n",
      "Validation Metrics (Precision: 0.7483, Recall: 0.7483, F1: 0.7483)\n",
      "Test Metrics (Precision: 0.7664, Recall: 0.7664, F1: 0.7664)\n",
      "\n",
      "Best Model for roberta-large (large): saved_models/roberta-large\n",
      "Best Hyperparameters: {'batch_size': 8, 'learning_rate': 5e-06, 'weight_decay': 0.0}\n",
      "Best F1-Score: 0.7664\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
