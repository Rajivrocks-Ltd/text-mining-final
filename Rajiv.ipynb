{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-08T21:06:37.256989Z",
     "start_time": "2024-12-08T21:06:37.252108Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import os\n",
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict, ClassLabel, Sequence, Value, Features, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, get_scheduler\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Load toy dataset for testing\n",
    "Will be replaced with Remco's code for dataset creation, token alignment, tokenization and dataset split creation  \n",
    "This is code from assignment 2"
   ],
   "id": "41cf0f11977b00c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:06:41.715185Z",
     "start_time": "2024-12-08T21:06:41.288686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Step 1: Read and parse IOB files\n",
    "def read_iob_file(file_path):\n",
    "    \"\"\"Reads an IOB file from filepath and returns sentences with tokens and tags.\"\"\"\n",
    "    sentences = []\n",
    "    sentence_tokens = []\n",
    "    sentence_labels = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # If line is not empty\n",
    "                token, _, tag = line.split()\n",
    "                sentence_tokens.append(token)\n",
    "                sentence_labels.append(tag)\n",
    "                \n",
    "            else:\n",
    "                # End of a sentence\n",
    "                if sentence_tokens:\n",
    "                    sentences.append({\"tokens\": sentence_tokens, \"ner_tags\": sentence_labels})\n",
    "                    sentence_tokens = []\n",
    "                    sentence_labels = []\n",
    "        # Add the last sentence if file doesn't end with a newline\n",
    "        if sentence_tokens:\n",
    "            sentences.append({\"tokens\": sentence_tokens, \"ner_tags\": sentence_labels})\n",
    "    return sentences\n",
    "\n",
    "# Step 2: Convert IOB data to Hugging Face dataset format\n",
    "def create_dataset_from_files(data_dir):\n",
    "    \"\"\"Create dataset from train and test files, generates the ClassLabel from unique values in train, val\n",
    "       and test and returns the DatasetDict\n",
    "\n",
    "    Args:\n",
    "        data_dir (Directory): The directory having the train, val and test.txt files. \n",
    "\n",
    "    Returns:\n",
    "        dataset_dict (DatasetDict): The DatasetDict necessary for further training and classification purposes\n",
    "    \"\"\"\n",
    "\n",
    "    # Define paths for train, validation, and test files\n",
    "    file_paths = {\n",
    "        \"train\": os.path.join(data_dir, \"train.txt\"),\n",
    "        \"validation\": os.path.join(data_dir, \"val.txt\"),\n",
    "        \"test\": os.path.join(data_dir, \"test.txt\"),\n",
    "    }\n",
    "    \n",
    "    # Parse the files\n",
    "    data = {split: read_iob_file(path) for split, path in file_paths.items()}\n",
    "\n",
    "    # Define the label names and ClassLabel feature\n",
    "    unique_labels = sorted(set(tag for split_data in data.values() for d in split_data for tag in d[\"ner_tags\"]))\n",
    "    label_feature = ClassLabel(names=unique_labels)\n",
    "\n",
    "    # Define the Features schema for Hugging Face datasets\n",
    "    features = Features({\n",
    "        'tokens': Sequence(Value(\"string\")),\n",
    "        'ner_tags': Sequence(label_feature)\n",
    "    })\n",
    "\n",
    "    # Convert data into DatasetDict\n",
    "    dataset_dict = DatasetDict({\n",
    "        split: Dataset.from_list(split_data).cast(features)\n",
    "        for split, split_data in data.items()\n",
    "    })\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "# Step 3: Create the dataset\n",
    "data_dir = \"example_data\"  # Adjust this path to your data directory\n",
    "dataset = create_dataset_from_files(data_dir)\n",
    "\n",
    "# Assign the datasets to the correct variables\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ],
   "id": "855d691a563c6f9d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1992 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8cf0cf85f454e0cab1448863989e0d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bea8c091b1d34bd5b0a897562af75f17"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/864 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c636b756929144e0af234552c7cee4d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:06:52.900936Z",
     "start_time": "2024-12-08T21:06:52.894549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, \n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"] \n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=train_dataset.column_names)\n",
    "# tokenized_val_dataset = val_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=val_dataset.column_names)\n",
    "# tokenized_test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=test_dataset.column_names)\n"
   ],
   "id": "30a204b77a572491",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:49:38.763029Z",
     "start_time": "2024-12-08T20:49:38.755958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "# tokenized_train_dataset[0]"
   ],
   "id": "cac5837afb25fe0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 15982, 1407, 119, 102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1],\n",
       " 'labels': [-100, 12, 12, 12, -100]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:07:04.964251Z",
     "start_time": "2024-12-08T21:07:04.119056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the evaluation metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "label_names = train_dataset.features[\"ner_tags\"].feature.names\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", id2label=id2label, label2id=label2id)"
   ],
   "id": "24e67e25743687ad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepping the data for training",
   "id": "2f66f39b77bcbd7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:07:10.199238Z",
     "start_time": "2024-12-08T21:07:10.193827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataset_given_model(train, val, test, model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "    # Tokenize and align labels\n",
    "    tokenized_train = train_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=train_dataset.column_names)\n",
    "    tokenized_val = val_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=val_dataset.column_names)\n",
    "    tokenized_test = test_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=test_dataset.column_names)\n",
    "\n",
    "    return data_collator, tokenized_train, tokenized_val, tokenized_test"
   ],
   "id": "ecab2acb99334850",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:07:16.688870Z",
     "start_time": "2024-12-08T21:07:16.684099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ],
   "id": "5c88082577aa4b1e",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training and evaluation",
   "id": "d0a776f838636d52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T22:14:31.539636Z",
     "start_time": "2024-12-08T21:08:16.114345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the models and their corresponding sizes\n",
    "models = {\n",
    "    \"small\": \"bert-base-cased\",\n",
    "    \"medium\": \"bert-large-cased\",\n",
    "    \"large\": \"roberta-large\"\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "for size, model_name in models.items():\n",
    "    print(f\"\\nTraining and evaluating {model_name} ({size})...\")\n",
    "\n",
    "    # Create datasets and data collator for the current model\n",
    "    data_collator, tokenized_train, tokenized_val, tokenized_test = create_dataset_given_model(\n",
    "        train_dataset, val_dataset, test_dataset, model_name\n",
    "    )\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(tokenized_train, batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "    val_dataloader = DataLoader(tokenized_val, batch_size=8, collate_fn=data_collator)\n",
    "    test_dataloader = DataLoader(tokenized_test, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "    # Initialize the model for token classification\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "\n",
    "    # Set up optimizer and learning rate scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    num_train_epochs = 3\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    # Use the accelerator for distributed training\n",
    "    accelerator = Accelerator()\n",
    "    model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, val_dataloader\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_train_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_train_epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\")\n",
    "        for batch in progress_bar:\n",
    "            # Move batch data to the same device as the model\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "        print(f\"Epoch {epoch + 1} Loss: {total_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_predictions, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                # Move batch data to the same device as the model\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                logits = outputs.logits\n",
    "                predictions = logits.argmax(dim=-1)\n",
    "                labels = batch[\"labels\"]\n",
    "        \n",
    "                # Handle padding across processes for multi-GPU\n",
    "                predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "                labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "        \n",
    "                predictions_gathered = accelerator.gather(predictions)\n",
    "                labels_gathered = accelerator.gather(labels)\n",
    "        \n",
    "                true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "                metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "        \n",
    "        val_results = metric.compute()\n",
    "        print(f\"Validation Metrics (Epoch {epoch + 1}): {val_results}\")\n",
    "\n",
    "    # Test loop\n",
    "    model.eval()\n",
    "    test_predictions, test_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            # Move batch data to the same device as the model\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            labels = batch[\"labels\"]\n",
    "        \n",
    "            # Handle padding across processes for multi-GPU\n",
    "            predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "            labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "        \n",
    "            predictions_gathered = accelerator.gather(predictions)\n",
    "            labels_gathered = accelerator.gather(labels)\n",
    "        \n",
    "            true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "            metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    test_results = metric.compute()\n",
    "    print(f\"Test Metrics for {model_name}: {test_results}\")\n",
    "\n",
    "    # Store results for the current model\n",
    "    results[size] = test_results\n",
    "\n",
    "# Final results\n",
    "for size, metrics in results.items():\n",
    "    print(f\"\\nFinal Test Metrics for {models[size]} ({size}): {metrics}\")\n"
   ],
   "id": "e53e564e1925de8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating bert-base-cased (small)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1992 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "141f37b5f1754d11abf7b16922b3bc7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b986a2477ce41cf93eb1bf533a8209e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/864 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0b1da7a17bc4d5ba0fcf10d4fe52ae0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/249 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4c5f2930de9401199cca26f5400279a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 85.2722\n",
      "Validation Metrics (Epoch 1): {'ART': {'precision': np.float64(0.5465393794749404), 'recall': np.float64(0.324822695035461), 'f1': np.float64(0.40747330960854095), 'number': np.int64(705)}, 'CON': {'precision': np.float64(0.5058139534883721), 'recall': np.float64(0.3425196850393701), 'f1': np.float64(0.40845070422535207), 'number': np.int64(254)}, 'LOC': {'precision': np.float64(0.6115942028985507), 'recall': np.float64(0.5424164524421594), 'f1': np.float64(0.5749318801089919), 'number': np.int64(389)}, 'MAT': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(0)}, 'PER': {'precision': np.float64(0.7266949152542372), 'recall': np.float64(0.5813559322033899), 'f1': np.float64(0.6459510357815442), 'number': np.int64(590)}, 'SPE': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(0)}, 'overall_precision': np.float64(0.5807743658210948), 'overall_recall': np.float64(0.44891640866873067), 'overall_f1': np.float64(0.5064027939464494), 'overall_accuracy': 0.9333653133312982}\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/249 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8697267c3164b478f854181b436a244"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 28.4917\n",
      "Validation Metrics (Epoch 2): {'ART': {'precision': np.float64(0.6062052505966588), 'recall': np.float64(0.37797619047619047), 'f1': np.float64(0.465627864344638), 'number': np.int64(672)}, 'CON': {'precision': np.float64(0.5872093023255814), 'recall': np.float64(0.38113207547169814), 'f1': np.float64(0.4622425629290618), 'number': np.int64(265)}, 'LOC': {'precision': np.float64(0.5971014492753624), 'recall': np.float64(0.544973544973545), 'f1': np.float64(0.5698478561549102), 'number': np.int64(378)}, 'MAT': {'precision': np.float64(0.08888888888888889), 'recall': np.float64(0.3076923076923077), 'f1': np.float64(0.13793103448275862), 'number': np.int64(13)}, 'PER': {'precision': np.float64(0.7415254237288136), 'recall': np.float64(0.587248322147651), 'f1': np.float64(0.6554307116104869), 'number': np.int64(596)}, 'SPE': {'precision': np.float64(0.3333333333333333), 'recall': np.float64(0.5357142857142857), 'f1': np.float64(0.41095890410958896), 'number': np.int64(28)}, 'overall_precision': np.float64(0.6208277703604806), 'overall_recall': np.float64(0.4764344262295082), 'overall_f1': np.float64(0.5391304347826087), 'overall_accuracy': 0.9358946404430684}\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/249 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba25c01625fb4c6a911eef7c033c47e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 19.9413\n",
      "Validation Metrics (Epoch 3): {'ART': {'precision': np.float64(0.594272076372315), 'recall': np.float64(0.40161290322580645), 'f1': np.float64(0.47930702598652547), 'number': np.int64(620)}, 'CON': {'precision': np.float64(0.5872093023255814), 'recall': np.float64(0.3494809688581315), 'f1': np.float64(0.438177874186551), 'number': np.int64(289)}, 'LOC': {'precision': np.float64(0.663768115942029), 'recall': np.float64(0.5518072289156627), 'f1': np.float64(0.6026315789473685), 'number': np.int64(415)}, 'MAT': {'precision': np.float64(0.13333333333333333), 'recall': np.float64(0.375), 'f1': np.float64(0.19672131147540986), 'number': np.int64(16)}, 'PER': {'precision': np.float64(0.7521186440677966), 'recall': np.float64(0.6089193825042881), 'f1': np.float64(0.6729857819905213), 'number': np.int64(583)}, 'SPE': {'precision': np.float64(0.4), 'recall': np.float64(0.5142857142857142), 'f1': np.float64(0.45), 'number': np.int64(35)}, 'overall_precision': np.float64(0.6395193591455274), 'overall_recall': np.float64(0.48927477017364657), 'overall_f1': np.float64(0.5543981481481481), 'overall_accuracy': 0.9372901312633553}\n",
      "Test Metrics for bert-base-cased: {'ART': {'precision': np.float64(0.5983935742971888), 'recall': np.float64(0.37817258883248733), 'f1': np.float64(0.46345256609642305), 'number': np.int64(394)}, 'CON': {'precision': np.float64(0.5527950310559007), 'recall': np.float64(0.40271493212669685), 'f1': np.float64(0.4659685863874346), 'number': np.int64(221)}, 'LOC': {'precision': np.float64(0.6593406593406593), 'recall': np.float64(0.5405405405405406), 'f1': np.float64(0.5940594059405939), 'number': np.int64(444)}, 'MAT': {'precision': np.float64(0.140625), 'recall': np.float64(0.47368421052631576), 'f1': np.float64(0.21686746987951808), 'number': np.int64(19)}, 'PER': {'precision': np.float64(0.6916488222698073), 'recall': np.float64(0.6961206896551724), 'f1': np.float64(0.693877551020408), 'number': np.int64(464)}, 'SPE': {'precision': np.float64(0.5), 'recall': np.float64(0.16666666666666666), 'f1': np.float64(0.25), 'number': np.int64(6)}, 'overall_precision': np.float64(0.6205049732211171), 'overall_recall': np.float64(0.5239018087855297), 'overall_f1': np.float64(0.5681260945709282), 'overall_accuracy': 0.9518033687294032}\n",
      "\n",
      "Training and evaluating bert-large-cased (medium)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "949fe13f6b0c459abfb4f7b054742733"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Rajiv\\Desktop\\Leiden university\\Master\\Text mining\\Assignments\\1\\Text_mining_a1\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Rajiv\\.cache\\huggingface\\hub\\models--bert-large-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4017fd0c7fed43c68a1c2af9e327c6e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b432f59dae74811a562d491fa0c6319"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "121a6b848426479387f9d2ccb53811e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1992 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d83cbf094c964f0eb6bd62186b555cf0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5911259230648bea71ae3f9ad68ef34"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/864 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2a250b923444863b1ece81bf641bb8f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f71227ec978b49329ee45b2952eadf2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/249 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "697cc0ccc250446ebdb56669cfe89a92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 72.0516\n",
      "Validation Metrics (Epoch 1): {'ART': {'precision': np.float64(0.5751789976133651), 'recall': np.float64(0.3990066225165563), 'f1': np.float64(0.4711632453567937), 'number': np.int64(604)}, 'CON': {'precision': np.float64(0.5930232558139535), 'recall': np.float64(0.24817518248175183), 'f1': np.float64(0.34991423670668953), 'number': np.int64(411)}, 'LOC': {'precision': np.float64(0.672463768115942), 'recall': np.float64(0.48232848232848236), 'f1': np.float64(0.5617433414043583), 'number': np.int64(481)}, 'MAT': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(0)}, 'PER': {'precision': np.float64(0.7288135593220338), 'recall': np.float64(0.6220614828209765), 'f1': np.float64(0.671219512195122), 'number': np.int64(553)}, 'SPE': {'precision': np.float64(0.2222222222222222), 'recall': np.float64(0.5555555555555556), 'f1': np.float64(0.31746031746031744), 'number': np.int64(18)}, 'overall_precision': np.float64(0.6201602136181575), 'overall_recall': np.float64(0.4494436381228834), 'overall_f1': np.float64(0.5211781206171108), 'overall_accuracy': 0.932449522480485}\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/249 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d52be3cf6f7f4ec1bc09f5c107d74f0d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 26.2343\n",
      "Validation Metrics (Epoch 2): {'ART': {'precision': np.float64(0.5894988066825776), 'recall': np.float64(0.4395017793594306), 'f1': np.float64(0.5035677879714577), 'number': np.int64(562)}, 'CON': {'precision': np.float64(0.5988372093023255), 'recall': np.float64(0.3588850174216028), 'f1': np.float64(0.44880174291938996), 'number': np.int64(287)}, 'LOC': {'precision': np.float64(0.6811594202898551), 'recall': np.float64(0.5904522613065326), 'f1': np.float64(0.6325706594885598), 'number': np.int64(398)}, 'MAT': {'precision': np.float64(0.2), 'recall': np.float64(0.75), 'f1': np.float64(0.31578947368421056), 'number': np.int64(12)}, 'PER': {'precision': np.float64(0.7161016949152542), 'recall': np.float64(0.6167883211678832), 'f1': np.float64(0.6627450980392157), 'number': np.int64(548)}, 'SPE': {'precision': np.float64(0.6444444444444445), 'recall': np.float64(0.6041666666666666), 'f1': np.float64(0.6236559139784946), 'number': np.int64(48)}, 'overall_precision': np.float64(0.6415220293724967), 'overall_recall': np.float64(0.5180592991913746), 'overall_f1': np.float64(0.5732180137190575), 'overall_accuracy': 0.9414766037242162}\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/249 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ab12d36a5034168a7be1088ce7927e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 17.6319\n",
      "Validation Metrics (Epoch 3): {'ART': {'precision': np.float64(0.5847255369928401), 'recall': np.float64(0.4359430604982206), 'f1': np.float64(0.4994903160040775), 'number': np.int64(562)}, 'CON': {'precision': np.float64(0.5697674418604651), 'recall': np.float64(0.358974358974359), 'f1': np.float64(0.4404494382022472), 'number': np.int64(273)}, 'LOC': {'precision': np.float64(0.6782608695652174), 'recall': np.float64(0.6015424164524421), 'f1': np.float64(0.6376021798365121), 'number': np.int64(389)}, 'MAT': {'precision': np.float64(0.37777777777777777), 'recall': np.float64(0.6296296296296297), 'f1': np.float64(0.47222222222222215), 'number': np.int64(27)}, 'PER': {'precision': np.float64(0.7245762711864406), 'recall': np.float64(0.6107142857142858), 'f1': np.float64(0.6627906976744186), 'number': np.int64(560)}, 'SPE': {'precision': np.float64(0.6666666666666666), 'recall': np.float64(0.5882352941176471), 'f1': np.float64(0.625), 'number': np.int64(51)}, 'overall_precision': np.float64(0.6448598130841121), 'overall_recall': np.float64(0.518796992481203), 'overall_f1': np.float64(0.575), 'overall_accuracy': 0.9419563036936898}\n",
      "Test Metrics for bert-large-cased: {'ART': {'precision': np.float64(0.5742971887550201), 'recall': np.float64(0.4525316455696203), 'f1': np.float64(0.5061946902654868), 'number': np.int64(316)}, 'CON': {'precision': np.float64(0.5279503105590062), 'recall': np.float64(0.4187192118226601), 'f1': np.float64(0.467032967032967), 'number': np.int64(203)}, 'LOC': {'precision': np.float64(0.6456043956043956), 'recall': np.float64(0.6151832460732984), 'f1': np.float64(0.6300268096514745), 'number': np.int64(382)}, 'MAT': {'precision': np.float64(0.1875), 'recall': np.float64(0.4), 'f1': np.float64(0.25531914893617025), 'number': np.int64(30)}, 'PER': {'precision': np.float64(0.6059957173447538), 'recall': np.float64(0.6505747126436782), 'f1': np.float64(0.6274944567627494), 'number': np.int64(435)}, 'SPE': {'precision': np.float64(0.5), 'recall': np.float64(0.2), 'f1': np.float64(0.28571428571428575), 'number': np.int64(5)}, 'overall_precision': np.float64(0.5807192042846213), 'overall_recall': np.float64(0.5536105032822757), 'overall_f1': np.float64(0.566840926064227), 'overall_accuracy': 0.9526272427682168}\n",
      "\n",
      "Training and evaluating roberta-large (large)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0282fe8dedd48ad8b2f7b9fbffe93ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Rajiv\\Desktop\\Leiden university\\Master\\Text mining\\Assignments\\1\\Text_mining_a1\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Rajiv\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99572d0b69f94cc997c8d9d7050bc1a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "626b6add22b94cf1b773bce31847c3f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e535a681f420405492865e252b105f67"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50aec0068e85475aa2e716c71546b44c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1992 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3eefe15c24c42298ae17563529fc8d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f61b3f46f4024d4489a8748deb26ca2f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/864 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e80cd656a022407c8cab6fb93fdec8bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55d101452d7a4cf1b3a838bc36e5ad2c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/249 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aadd55efac4a49a78d5750b68be67bb0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 152.5536\n",
      "Validation Metrics (Epoch 1): {'ART': {'precision': np.float64(0.03341288782816229), 'recall': np.float64(0.7777777777777778), 'f1': np.float64(0.06407322654462243), 'number': np.int64(18)}, 'CON': {'precision': np.float64(0.11627906976744186), 'recall': np.float64(0.4166666666666667), 'f1': np.float64(0.1818181818181818), 'number': np.int64(48)}, 'LOC': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(0)}, 'MAT': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(0)}, 'PER': {'precision': np.float64(0.1228813559322034), 'recall': np.float64(0.4603174603174603), 'f1': np.float64(0.19397993311036787), 'number': np.int64(126)}, 'SPE': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(0)}, 'overall_precision': np.float64(0.06141522029372497), 'overall_recall': np.float64(0.4791666666666667), 'overall_f1': np.float64(0.10887573964497041), 'overall_accuracy': 0.9064585059526405}\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/249 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf23ce5c050641bf917c88449ca81976"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 98.0196\n",
      "Validation Metrics (Epoch 2): {'ART': {'precision': np.float64(0.2911694510739857), 'recall': np.float64(0.4), 'f1': np.float64(0.3370165745856354), 'number': np.int64(305)}, 'CON': {'precision': np.float64(0.3081395348837209), 'recall': np.float64(0.29608938547486036), 'f1': np.float64(0.301994301994302), 'number': np.int64(179)}, 'LOC': {'precision': np.float64(0.028985507246376812), 'recall': np.float64(0.22727272727272727), 'f1': np.float64(0.05141388174807198), 'number': np.int64(44)}, 'MAT': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(0)}, 'PER': {'precision': np.float64(0.4004237288135593), 'recall': np.float64(0.5220994475138122), 'f1': np.float64(0.45323741007194246), 'number': np.int64(362)}, 'SPE': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(0)}, 'overall_precision': np.float64(0.24966622162883845), 'overall_recall': np.float64(0.4202247191011236), 'overall_f1': np.float64(0.3132328308207705), 'overall_accuracy': 0.9148750599624962}\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/249 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "179d8509e62b4ed98d1c8137a37bbb7a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 69.9804\n",
      "Validation Metrics (Epoch 3): {'ART': {'precision': np.float64(0.31742243436754175), 'recall': np.float64(0.4539249146757679), 'f1': np.float64(0.3735955056179775), 'number': np.int64(293)}, 'CON': {'precision': np.float64(0.3372093023255814), 'recall': np.float64(0.2871287128712871), 'f1': np.float64(0.3101604278074866), 'number': np.int64(202)}, 'LOC': {'precision': np.float64(0.07246376811594203), 'recall': np.float64(0.2976190476190476), 'f1': np.float64(0.11655011655011656), 'number': np.int64(84)}, 'MAT': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(0)}, 'PER': {'precision': np.float64(0.4258474576271186), 'recall': np.float64(0.5037593984962406), 'f1': np.float64(0.4615384615384615), 'number': np.int64(399)}, 'SPE': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(3)}, 'overall_precision': np.float64(0.27837116154873165), 'overall_recall': np.float64(0.42507645259938837), 'overall_f1': np.float64(0.33642597821702297), 'overall_accuracy': 0.9154855871963717}\n",
      "Test Metrics for roberta-large: {'ART': {'precision': np.float64(0.3855421686746988), 'recall': np.float64(0.34782608695652173), 'f1': np.float64(0.3657142857142857), 'number': np.int64(276)}, 'CON': {'precision': np.float64(0.43478260869565216), 'recall': np.float64(0.44025157232704404), 'f1': np.float64(0.43749999999999994), 'number': np.int64(159)}, 'LOC': {'precision': np.float64(0.09065934065934066), 'recall': np.float64(0.4177215189873418), 'f1': np.float64(0.1489841986455982), 'number': np.int64(79)}, 'MAT': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(0)}, 'PER': {'precision': np.float64(0.3811563169164882), 'recall': np.float64(0.5760517799352751), 'f1': np.float64(0.4587628865979381), 'number': np.int64(309)}, 'SPE': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(0)}, 'overall_precision': np.float64(0.2884468247895945), 'overall_recall': np.float64(0.4580801944106926), 'overall_f1': np.float64(0.3539906103286385), 'overall_accuracy': 0.9300164774807763}\n",
      "\n",
      "Final Test Metrics for bert-base-cased (small): {'ART': {'precision': np.float64(0.5983935742971888), 'recall': np.float64(0.37817258883248733), 'f1': np.float64(0.46345256609642305), 'number': np.int64(394)}, 'CON': {'precision': np.float64(0.5527950310559007), 'recall': np.float64(0.40271493212669685), 'f1': np.float64(0.4659685863874346), 'number': np.int64(221)}, 'LOC': {'precision': np.float64(0.6593406593406593), 'recall': np.float64(0.5405405405405406), 'f1': np.float64(0.5940594059405939), 'number': np.int64(444)}, 'MAT': {'precision': np.float64(0.140625), 'recall': np.float64(0.47368421052631576), 'f1': np.float64(0.21686746987951808), 'number': np.int64(19)}, 'PER': {'precision': np.float64(0.6916488222698073), 'recall': np.float64(0.6961206896551724), 'f1': np.float64(0.693877551020408), 'number': np.int64(464)}, 'SPE': {'precision': np.float64(0.5), 'recall': np.float64(0.16666666666666666), 'f1': np.float64(0.25), 'number': np.int64(6)}, 'overall_precision': np.float64(0.6205049732211171), 'overall_recall': np.float64(0.5239018087855297), 'overall_f1': np.float64(0.5681260945709282), 'overall_accuracy': 0.9518033687294032}\n",
      "\n",
      "Final Test Metrics for bert-large-cased (medium): {'ART': {'precision': np.float64(0.5742971887550201), 'recall': np.float64(0.4525316455696203), 'f1': np.float64(0.5061946902654868), 'number': np.int64(316)}, 'CON': {'precision': np.float64(0.5279503105590062), 'recall': np.float64(0.4187192118226601), 'f1': np.float64(0.467032967032967), 'number': np.int64(203)}, 'LOC': {'precision': np.float64(0.6456043956043956), 'recall': np.float64(0.6151832460732984), 'f1': np.float64(0.6300268096514745), 'number': np.int64(382)}, 'MAT': {'precision': np.float64(0.1875), 'recall': np.float64(0.4), 'f1': np.float64(0.25531914893617025), 'number': np.int64(30)}, 'PER': {'precision': np.float64(0.6059957173447538), 'recall': np.float64(0.6505747126436782), 'f1': np.float64(0.6274944567627494), 'number': np.int64(435)}, 'SPE': {'precision': np.float64(0.5), 'recall': np.float64(0.2), 'f1': np.float64(0.28571428571428575), 'number': np.int64(5)}, 'overall_precision': np.float64(0.5807192042846213), 'overall_recall': np.float64(0.5536105032822757), 'overall_f1': np.float64(0.566840926064227), 'overall_accuracy': 0.9526272427682168}\n",
      "\n",
      "Final Test Metrics for roberta-large (large): {'ART': {'precision': np.float64(0.3855421686746988), 'recall': np.float64(0.34782608695652173), 'f1': np.float64(0.3657142857142857), 'number': np.int64(276)}, 'CON': {'precision': np.float64(0.43478260869565216), 'recall': np.float64(0.44025157232704404), 'f1': np.float64(0.43749999999999994), 'number': np.int64(159)}, 'LOC': {'precision': np.float64(0.09065934065934066), 'recall': np.float64(0.4177215189873418), 'f1': np.float64(0.1489841986455982), 'number': np.int64(79)}, 'MAT': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(0)}, 'PER': {'precision': np.float64(0.3811563169164882), 'recall': np.float64(0.5760517799352751), 'f1': np.float64(0.4587628865979381), 'number': np.int64(309)}, 'SPE': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(0)}, 'overall_precision': np.float64(0.2884468247895945), 'overall_recall': np.float64(0.4580801944106926), 'overall_f1': np.float64(0.3539906103286385), 'overall_accuracy': 0.9300164774807763}\n"
     ]
    }
   ],
   "execution_count": 65
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
